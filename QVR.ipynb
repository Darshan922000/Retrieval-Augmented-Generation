{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "#OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Ingestion Techniques\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': './data/data.txt'}, page_content='What is Retrieval-Augmented Generation?\\nRetrieval-Augmented Generation (RAG) is the process of optimizing the output of a large language model, so it references an authoritative knowledge base outside of its training data sources before generating a response. Large Language Models (LLMs) are trained on vast volumes of data and use billions of parameters to generate original output for tasks like answering questions, translating languages, and completing sentences. RAG extends the already powerful capabilities of LLMs to specific domains or an organization\\'s internal knowledge base, all without the need to retrain the model. It is a cost-effective approach to improving LLM output so it remains relevant, accurate, and useful in various contexts.\\n\\nWhy is Retrieval-Augmented Generation important?\\nLLMs are a key artificial intelligence (AI) technology powering intelligent chatbots and other natural language processing (NLP) applications. The goal is to create bots that can answer user questions in various contexts by cross-referencing authoritative knowledge sources. Unfortunately, the nature of LLM technology introduces unpredictability in LLM responses. Additionally, LLM training data is static and introduces a cut-off date on the knowledge it has.\\n\\nKnown challenges of LLMs include:\\n\\nPresenting false information when it does not have the answer.\\nPresenting out-of-date or generic information when the user expects a specific, current response.\\nCreating a response from non-authoritative sources.\\nCreating inaccurate responses due to terminology confusion, wherein different training sources use the same terminology to talk about different things.\\nYou can think of the Large Language Model as an over-enthusiastic new employee who refuses to stay informed with current events but will always answer every question with absolute confidence. Unfortunately, such an attitude can negatively impact user trust and is not something you want your chatbots to emulate!\\n\\nRAG is one approach to solving some of these challenges. It redirects the LLM to retrieve relevant information from authoritative, pre-determined knowledge sources. Organizations have greater control over the generated text output, and users gain insights into how the LLM generates the response.\\n\\nWhat are the benefits of Retrieval-Augmented Generation?\\nRAG technology brings several benefits to an organization\\'s generative AI efforts.\\n\\nCost-effective implementation\\nChatbot development typically begins using a foundation model. Foundation models (FMs) are API-accessible LLMs trained on a broad spectrum of generalized and unlabeled data. The computational and financial costs of retraining FMs for organization or domain-specific information are high. RAG is a more cost-effective approach to introducing new data to the LLM. It makes generative artificial intelligence (generative AI) technology more broadly accessible and usable.\\n\\nCurrent information\\nEven if the original training data sources for an LLM are suitable for your needs, it is challenging to maintain relevancy. RAG allows developers to provide the latest research, statistics, or news to the generative models. They can use RAG to connect the LLM directly to live social media feeds, news sites, or other frequently-updated information sources. The LLM can then provide the latest information to the users.\\n\\nEnhanced user trust\\nRAG allows the LLM to present accurate information with source attribution. The output can include citations or references to sources. Users can also look up source documents themselves if they require further clarification or more detail. This can increase trust and confidence in your generative AI solution.\\n\\nMore developer control\\nWith RAG, developers can test and improve their chat applications more efficiently. They can control and change the LLM\\'s information sources to adapt to changing requirements or cross-functional usage. Developers can also restrict sensitive information retrieval to different authorization levels and ensure the LLM generates appropriate responses. In addition, they can also troubleshoot and make fixes if the LLM references incorrect information sources for specific questions. Organizations can implement generative AI technology more confidently for a broader range of applications.\\n\\nHow does Retrieval-Augmented Generation work?\\nWithout RAG, the LLM takes the user input and creates a response based on information it was trained on—or what it already knows. With RAG, an information retrieval component is introduced that utilizes the user input to first pull information from a new data source. The user query and the relevant information are both given to the LLM. The LLM uses the new knowledge and its training data to create better responses. The following sections provide an overview of the process.\\n\\nCreate external data\\nThe new data outside of the LLM\\'s original training data set is called external data. It can come from multiple data sources, such as a APIs, databases, or document repositories. The data may exist in various formats like files, database records, or long-form text. Another AI technique, called embedding language models, converts data into numerical representations and stores it in a vector database. This process creates a knowledge library that the generative AI models can understand.\\n\\nRetrieve relevant information\\nThe next step is to perform a relevancy search. The user query is converted to a vector representation and matched with the vector databases. For example, consider a smart chatbot that can answer human resource questions for an organization. If an employee searches, \"How much annual leave do I have?\" the system will retrieve annual leave policy documents alongside the individual employee\\'s past leave record. These specific documents will be returned because they are highly-relevant to what the employee has input. The relevancy was calculated and established using mathematical vector calculations and representations.\\n\\nAugment the LLM prompt\\nNext, the RAG model augments the user input (or prompts) by adding the relevant retrieved data in context. This step uses prompt engineering techniques to communicate effectively with the LLM. The augmented prompt allows the large language models to generate an accurate answer to user queries.\\n\\nUpdate external data\\nThe next question may be—what if the external data becomes stale? To maintain current information for retrieval, asynchronously update the documents and update embedding representation of the documents. You can do this through automated real-time processes or periodic batch processing. This is a common challenge in data analytics—different data-science approaches to change management can be used.\\n\\nWhat is the difference between Retrieval-Augmented Generation and semantic search?\\nSemantic search enhances RAG results for organizations wanting to add vast external knowledge sources to their LLM applications. Modern enterprises store vast amounts of information like manuals, FAQs, research reports, customer service guides, and human resource document repositories across various systems. Context retrieval is challenging at scale and consequently lowers generative output quality.\\n\\nSemantic search technologies can scan large databases of disparate information and retrieve data more accurately. For example, they can answer questions such as, \"How much was spent on machinery repairs last year?” by mapping the question to the relevant documents and returning specific text instead of search results. Developers can then use that answer to provide more context to the LLM.\\n\\nConventional or keyword search solutions in RAG produce limited results for knowledge-intensive tasks. Developers must also deal with word embeddings, document chunking, and other complexities as they manually prepare their data. In contrast, semantic search technologies do all the work of knowledge base preparation so developers don\\'t have to. They also generate semantically relevant passages and token words ordered by relevance to maximize the quality of the RAG payload.')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# txt loader...\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "reader = TextLoader(file_path=\"./data/data.txt\", encoding=\"utf-8\")\n",
    "text = reader.load()\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://python.langchain.com/docs/introduction/'}, page_content='Introduction\\nLangChain is a framework for developing applications powered by large language models (LLMs).\\nLangChain simplifies every stage of the LLM application lifecycle:\\n\\nDevelopment: Build your applications using LangChain\\'s open-source components and third-party integrations.\\nUse LangGraph to build stateful agents with first-class streaming and human-in-the-loop support.\\nProductionization: Use LangSmith to inspect, monitor and evaluate your applications, so that you can continuously optimize and deploy with confidence.\\nDeployment: Turn your LangGraph applications into production-ready APIs and Assistants with LangGraph Platform.\\n\\n\\n\\nLangChain implements a standard interface for large language models and related\\ntechnologies, such as embedding models and vector stores, and integrates with\\nhundreds of providers. See the integrations page for\\nmore.\\n\\nSelect chat model:OpenAI▾OpenAIAnthropicAzureGoogleAWSCohereNVIDIAFireworks AIGroqMistral AITogether AIDatabrickspip install -qU langchain-openaiimport getpassimport osif not os.environ.get(\"OPENAI_API_KEY\"):  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")from langchain_openai import ChatOpenAImodel = ChatOpenAI(model=\"gpt-4o-mini\")\\nmodel.invoke(\"Hello, world!\")\\nnoteThese docs focus on the Python LangChain library. Head here for docs on the JavaScript LangChain library.\\nArchitecture\\u200b\\nThe LangChain framework consists of multiple open-source libraries. Read more in the\\nArchitecture page.\\n\\nlangchain-core: Base abstractions for chat models and other components.\\nIntegration packages (e.g. langchain-openai, langchain-anthropic, etc.): Important integrations have been split into lightweight packages that are co-maintained by the LangChain team and the integration developers.\\nlangchain: Chains, agents, and retrieval strategies that make up an application\\'s cognitive architecture.\\nlangchain-community: Third-party integrations that are community maintained.\\nlanggraph: Orchestration framework for combining LangChain components into production-ready applications with persistence, streaming, and other key features. See LangGraph documentation.\\n\\nGuides\\u200b\\nTutorials\\u200b\\nIf you\\'re looking to build something specific or are more of a hands-on learner, check out our tutorials section.\\nThis is the best place to get started.\\nThese are the best ones to get started with:\\n\\nBuild a Simple LLM Application\\nBuild a Chatbot\\nBuild an Agent\\nIntroduction to LangGraph\\n\\nExplore the full list of LangChain tutorials here, and check out other LangGraph tutorials here. To learn more about LangGraph, check out our first LangChain Academy course, Introduction to LangGraph, available here.\\nHow-to guides\\u200b\\nHere you’ll find short answers to “How do I….?” types of questions.\\nThese how-to guides don’t cover topics in depth – you’ll find that material in the Tutorials and the API Reference.\\nHowever, these guides will help you quickly accomplish common tasks using chat models,\\nvector stores, and other common LangChain components.\\nCheck out LangGraph-specific how-tos here.\\nConceptual guide\\u200b\\nIntroductions to all the key parts of LangChain you’ll need to know! Here you\\'ll find high level explanations of all LangChain concepts.\\nFor a deeper dive into LangGraph concepts, check out this page.\\nIntegrations\\u200b\\nLangChain is part of a rich ecosystem of tools that integrate with our framework and build on top of it.\\nIf you\\'re looking to get up and running quickly with chat models, vector stores,\\nor other LangChain components from a specific provider, check out our growing list of integrations.\\nAPI reference\\u200b\\nHead to the reference section for full documentation of all classes and methods in the LangChain Python packages.\\nEcosystem\\u200b\\n🦜🛠️ LangSmith\\u200b\\nTrace and evaluate your language model applications and intelligent agents to help you move from prototype to production.\\n🦜🕸️ LangGraph\\u200b\\nBuild stateful, multi-actor applications with LLMs. Integrates smoothly with LangChain, but can be used without it. LangGraph powers production-grade agents, trusted by Linkedin, Uber, Klarna, GitLab, and many more.\\nAdditional resources\\u200b\\nVersions\\u200b\\nSee what changed in v0.3, learn how to migrate legacy code, read up on our versioning policies, and more.\\nSecurity\\u200b\\nRead up on security best practices to make sure you\\'re developing safely with LangChain.\\nContributing\\u200b\\nCheck out the developer\\'s guide for guidelines on contributing and help getting your dev environment set up.')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# web based loader...\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4 # Beautiful Soup is a library that makes it easy to scrape information from web pages.\n",
    "reader = WebBaseLoader(web_path=\"https://python.langchain.com/docs/introduction/\", bs_kwargs=dict(parse_only = bs4.SoupStrainer(class_=\"theme-doc-markdown markdown\")))\n",
    "text = reader.load()\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': './data/AI_Governance.pdf', 'page': 0, 'page_label': 'i'}, page_content='Research \\nPaper\\nAI governance \\nand\\xa0human\\xa0rights\\nResetting the relationship\\nKate Jones\\nInternational Law \\nProgramme  \\nJanuary 2023\\n'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 1, 'page_label': 'ii'}, page_content='Chatham House, the Royal Institute of International  \\nAffairs, is a world-leading policy institute based in\\xa0London. \\nOur mission is to help governments and societies build \\na\\xa0sustainably secure, prosperous and\\xa0just world.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 2, 'page_label': '1'}, page_content='1 Chatham House\\nContents\\n Summary 2\\n01 Introduction 3\\n02 What is AI? 5\\n03 Governing AI: why human rights? 9\\n04 Principles of AI governance: the contribution of human rights 21\\n05 Processes of AI governance: the contribution of human rights 34\\n06 Remedies in AI\\xa0governance: the\\xa0contribution of\\xa0human rights 44\\n07 Conclusion and recommendations 50\\n About the author 54\\n Acknowledgments 54'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 3, 'page_label': '2'}, page_content='2 Chatham House\\nSummary\\n — Artificial intelligence (AI) is redefining what it means to be human. Human rights \\nhave so far been largely overlooked in the governance of AI – particularly in the \\nUK and the US. This is an error and requires urgent correction.\\n — While human rights do not hold all the answers, they ought to be the baseline for \\nAI\\xa0governance. International human rights law is a crystallization of ethical principles \\ninto norms, their meanings and implications well-developed over the last 70 years. \\nThese norms command high international consensus, are relatively clear, and can \\nbe developed to account for new situations. They offer a well-calibrated method of \\nbalancing the rights of the individual against competing rights and interests using tests \\nof necessity and proportionality. Human rights provide processes of governance for \\nbusiness and governments, and an ecosystem for provision of remedy for breaches.\\n — The omission of human rights has arisen in part because those with human \\nrights\\xa0expertise are often not included in AI governance, both in companies \\nand\\xa0in governments. Various myths about human rights have also contributed \\nto\\xa0their being overlooked: human rights are wrongly perceived as adding little to \\nethics; as preventing innovation; as being overly complex, vague, old-fashioned \\nor\\xa0radical; or as only concerning governments.\\n — Companies, governments and civil society are retreading the territory of human \\nrights with a new proliferation of AI ethics principles and compliance assessment \\nmethods. As a result, businesses developing or purchasing AI do not know what \\nstandards they should meet, and may find it difficult to justify the costs of ethical \\nprocesses when competitors have no obligation to do the same. Meanwhile, \\nindividuals do not know what standards they can expect from AI affecting them \\nand often have no means of complaint. Consequently, many people do not trust \\nAI: they suspect that it may be biased or unfair, that it could be spying on them \\nor\\xa0manipulating their choices.\\n — The human rights to privacy and data protection, equality and non-discrimination \\nare key to the governance of AI, as are human rights’ protection of autonomy \\nand of economic, social and cultural rights in ensuring that AI will benefit \\neveryone. Human rights law imposes not only duties on governments to uphold, \\nbut also responsibilities on companies and organizations to comply, as well as \\nrequirements for legal remedies and reparation of harms.\\n — Companies and investors, governments, international organizations and civil \\nsociety should take steps to establish human rights as the foundation on which \\nAI\\xa0governance is built, including through inclusive discussion, championing \\nhuman rights and establishing standards and processes for implementation \\nof\\xa0human rights law and remedy in case of breach.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 4, 'page_label': '3'}, page_content='3 Chatham House\\n01 \\nIntroduction\\nAI is redefining what it means to be human. As existing \\ninternational norms designed to allow every human being a life \\nof liberty and dignity, human rights ought to be the foundation \\nfor AI governance.\\nHuman rights are central to what it means to be human. They were drafted \\nand\\xa0agreed internationally, with worldwide popular support, to define freedoms \\nand\\xa0entitlements that would allow every human being to live a life of liberty and \\ndignity. Those fundamental human rights have been interpreted and developed \\nover decades to delineate the parameters of fairness, equality and liberty for \\nevery individual.\\nNow, artificial intelligence (AI) is redefining what it means to be human. \\nIts systems\\xa0and processes have the potential to alter the human experience \\nfundamentally. AI will affect not only public policy areas such as road safety and \\nhealthcare, but also human autonomy, relationships and dignity. It will affect \\nlifestyles and professions, as well as the future course of human development \\nand the nature and scale of conflicts. It will change the relationships between \\ncommunities and those between the individual, the state and corporations. \\nAI offers tremendous benefits for all societies but also presents risks. These risks \\npotentially include further division between the privileged and the unprivileged; \\nthe erosion of individual freedoms through ubiquitous surveillance; and the \\nreplacement of independent thought and judgement with\\xa0automated control.\\nThis paper aims to explain why human rights ought to be the foundation for \\nAI governance, to explore the reasons why they are not\\xa0– except in the EU and \\nsome international organizations\\xa0– and to demonstrate how human rights can \\nbe\\xa0embedded from the beginning in future AI governance initiatives.\\nWhile AI is being implemented rapidly around the world, most governance \\ninitiatives to date have emerged from developed states. This paper therefore \\nfocuses on practice and process primarily in the EU, the UK and the US. However, \\nthe paper also acknowledges the significance of AI initiatives elsewhere in the \\nworld\\xa0– China in particular is a leading developer and exporter of AI technology.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 5, 'page_label': '4'}, page_content='AI governance and human rights\\nResetting the relationship\\n4 Chatham House\\nThe following chapter explains AI and the risks and benefits it presents \\nfor human\\xa0rights. Chapter\\xa03 aims to dispel myths and fears about human \\nrights, before discussing why human rights should provide the baseline for AI \\ngovernance. Chapters 4, 5 and 6 outline the principal import of human rights \\nfor AI governance principles, processes and remedies respectively. Finally, \\nChapter\\xa07 offers recommendations on actions that governments, organizations, \\ncompanies and individuals can take to ensure that human rights are embedded \\nin\\xa0AI\\xa0governance in future.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 6, 'page_label': '5'}, page_content='5 Chatham House\\n02 \\nWhat is AI?\\nAI has capacity to transform human life\\xa0– \\nboth\\xa0for\\xa0better and for worse.\\nAI is increasingly present in our lives, and its impact will expand significantly \\nin the\\xa0coming years. From predictive text, to social media news feeds, to virtual \\nhomes and mobile phone voice assistants, AI is already a part of everyday life. \\nAI\\xa0offers automated translation, assists shoppers buying online and recommends \\nthe fastest route on the drive home. It is also a key component of much-debated, \\nrapidly developing technologies such as facial recognition and self-driving vehicles.\\nThere is no single agreed definition of AI: it is a general term referring to \\nmachines’\\xa0evolving capacity to take on tasks requiring some form of intelligence. \\nThe tasks that AI performs can include generating predictions, making decisions \\nand providing recommendations.1 This means that AI may make decisions itself, \\nor\\xa0provide information for use in human decision-making.\\nAI systems are algorithmic\\xa0– the algorithm being the computational process or \\nset of rules that the computer follows to calculate a result. To learn, AI generally \\nrelies on synthesising and making inferences from large quantities of data. \\nIt\\xa0is\\xa0the machine’s capacity to learn by itself how to do tasks better, rather than \\nsimply following instructions, that distinguishes AI from traditional computer \\nprogrammes. Contrary to popular myth, self-improvement does not prevent \\nAI\\xa0from being constrained by rules.\\nGovernments are among the largest adopters of AI, deploying it to assist in \\nmaking\\xa0decisions that can have major consequences for the lives of individual \\ncitizens. For example, governments are using AI to assist with decisions on \\nentitlement to immigration status, welfare benefits, school entry and priority \\n1 The European Commission’s High-Level Expert Group on Artificial Intelligence offers a fuller definition: \\n‘Artificial intelligence (AI) systems are software (and possibly also hardware) systems designed by humans \\nthat, given a complex goal, act in the physical or digital dimension by perceiving their environment through \\ndata acquisition, interpreting the collected structured or unstructured data, reasoning on the knowledge, or \\nprocessing the information, derived from this data and deciding the best action(s) to take to achieve the given \\ngoal. AI systems can either use symbolic rules or learn a numeric model, and they can also adapt their behaviour \\nby analysing how the environment is affected by their previous actions.’ Independent High-Level Expert \\nGroup on Artificial Intelligence (2019), Ethics Guidelines for Trustworthy AI, Brussels: European Commission, \\nhttps://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 7, 'page_label': '6'}, page_content='AI governance and human rights\\nResetting the relationship\\n6 Chatham House\\nvaccinations. They are adopting it to assist with provision of justice, in both civil \\nand criminal processes. And they may be using AI to assist in delivery of critical \\ninfrastructure and national security.\\nAI is likely to pervade almost every domain of human activity, and to become \\nincreasingly important as technology evolves towards greater interoperability, \\nincluding through the development of the metaverse.2 This paper discusses general \\nfeatures of AI, but by no means diminishes the need for parallel sector-specific \\ndiscussion. The use of AI in the healthcare system, in social media or in the criminal \\njustice process, for instance, each raise specific human rights issues that need \\nto\\xa0be\\xa0addressed in context, alongside the overarching issues discussed here.\\n2.1 What potential does AI hold for human \\nrights\\xa0and the common good?\\nDue to its speed and its power of self-learning, AI has the capacity to transform \\nour\\xa0societies. It can operate faster\\xa0– and potentially better\\xa0– than any human. \\nIt\\xa0can\\xa0achieve scientific breakthroughs, calculate fair distributions and outcomes, \\nand\\xa0make more accurate predictions.\\nAI holds enormous potential to enable human development and flourishing. \\nFor\\xa0example, AI is accelerating the battle against disease3 and mitigating the \\nimpact of disability;4 it is helping to tackle climate change5 and optimize efficiency \\nin agriculture;6 it can assist distribution of humanitarian aid;7 it has enormous \\npotential for improving access to, and quality of, education globally;8 and it can \\ntransform public and private transport.9 AI could help to ensure that policing is \\nfair and respectful of human dignity. It may make workplaces more productive, \\nreduce the load of manual labour and help developed countries to manage the \\nchallenges of an ageing population. To give a specific example of the benefits, \\nthe AI programme AlphaFold is predicting the structures of both human and \\nanimal\\xa0proteins with tremendous speed and remarkable accuracy, with potentially \\ntransformative effects on medical treatments, crop science and plastic reduction.10\\n2 Moynihan, H., Buchser, M. and Wallace, J. (2022), What is the metaverse?, Explainer, London: Royal Institute \\nof\\xa0International Affairs, https://www.chathamhouse.org/2022/04/what-metaverse.\\n3 For example, as regards COVID-19: Soomro, T. A. et al. (2021), ‘Artificial intelligence (AI) for medical imaging \\nto combat coronavirus disease (COVID-19): a detailed review with direction for future research’, Artificial \\nIntelligence Review, 55(2), pp. 1409–39, https://doi.org/10.1007%2Fs10462-021-09985-z.\\n4 For example: Microsoft (undated), ‘AI for Accessibility’, https://www.microsoft.com/en-us/ai/\\nai-for-accessibility.\\n5 Rolnick, D. et al. (2019), ‘Tackling Climate Change with Machine Learning’, arXiv, 1906.05433v2 [cs.CY], \\nhttps://arxiv.org/pdf/1906.05433.pdf.\\n6 Cline, T. (2019), ‘Digital agriculture: making the most of machine learning on farm’, Spore, https://spore.cta.\\nint/en/dossiers/article/digital-agriculture-making-the-most-of-machine-learning-on-farm-sid0dbfbb123-30b2-\\n48fd-830e-71312f66af04?msclkid=d9322204a57311ecb7a36f2895e35dd1.\\n7 For example: UN Global Pulse (2022), ‘Innovating Together for our Common Future’, www.unglobalpulse.org.\\n8 For example: UNESCO (2022), ‘Artificial Intelligence and the Futures of Learning’, https://en.unesco.org/\\nthemes/ict-education/ai-futures-learning.\\n9 For example, European Parliament Briefing (2019), ‘Artificial Intelligence in Transport: Current and \\nFuture Developments, Opportunities and Challenges’, https://www.europarl.europa.eu/RegData/etudes/\\nBRIE/2019/635609/EPRS_BRI(2019)635609_EN.pdf?msclkid=cd1a70d2aaa011ec9f9ff79af4f9d88d.\\n10 Tunyasuvunakool, K. et al. (2021), ‘Highly accurate protein structure prediction for the human proteome’, \\nNature, 596, 21 July 2021, pp. 590–96, https://doi.org/10.1038/s41586-021-03828-1.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 8, 'page_label': '7'}, page_content='AI governance and human rights\\nResetting the relationship\\n7 Chatham House\\nIn short, when properly managed, AI can enable delivery of the UN’s Sustainable \\nDevelopment Goals (SDGs) by the 2030 deadline,11 boost the implementation \\nof economic, social and cultural rights worldwide, and support improvements \\nin\\xa0many areas of life.\\nTo achieve these aims, AI must be harnessed for the good of all societies. Doing \\nso\\xa0involves not only goodwill, but also ensuring that commercial considerations \\ndo not dictate the development of AI entirely. Provision of funding for AI research \\nand development outside the commercial sector will be invaluable, as will access \\nto\\xa0data for AI developers such that they may generate applications of AI that benefit \\npeople in all communities.\\nJust as the industrial revolution brought progress at the expense of upheaval \\nin\\xa0traditional ways of living, so will AI bring change to our societies. Work must \\nbe\\xa0done now to mitigate the risk of negative impacts. Governments must anticipate \\nand manage the changes that widespread use of AI will herald. They must consider \\nboth the implications of AI for their own public policymaking, which may be \\nsubject to judicial review, and how to govern a society in which AI is increasingly \\nbeing developed by the private sector and becoming a feature of life for the world’s \\npopulation. This includes governance not only of AI itself but of its implications \\nfor current ways of life. For example, governments should address the risk that \\nAI will upend current practices and norms in the workplace, through mass \\nunemployment and an undermining of bargaining power between employers and \\nemployees. Governments should be taking active steps to ensure the benefits of AI \\nare distributed equitably, avoiding the division of society into ‘winners’ and ‘losers’ \\nfrom emerging technology. To preserve and promote public interest, governments \\nmust not allow companies to develop AI in a policy and regulatory vacuum.\\n2.2 What are the key human rights \\nand\\xa0ethical\\xa0challenges posed by AI?\\nEvidence abounds of problematic uses of AI. At one end of the spectrum, AI is being \\ndeliberately used as a tool of suppression: for example, the Chinese government’s \\nuse of AI to conduct mass surveillance of its Uyghur minority.12 Some types of AI \\ncould be used deliberately to limit people’s freedom to express themselves and to \\nmeet with others, to monitor the general public for compliance with behavioural \\nrules,13 to detect ‘suspicious behaviour’14 or to restrict access to society’s benefits \\nto\\xa0a privileged few.\\n11 Vinuesa, R. et al. (2020), ‘The role of artificial intelligence in achieving the Sustainable Development \\nGoals’, Nature Communications, 11(233), https://doi.org/10.1038/s41467-019-14108-y; Chui, M. et al. \\n(2019), ‘Using AI to help achieve Sustainable Development Goals’, New York: UN Development Programme, \\nhttps://www.undp.org/blog/using-ai-help-achieve-sustainable-development-goals.\\n12  Mozur, P. (2019), ‘One Month, 500,000 Face Scans: How China is using AI to profile a minority’, New York \\nTimes, 14 April 2019, https://www.nytimes.com/2019/04/14/technology/china-surveillance-artificial-\\nintelligence-racial-profiling.html.\\n13  Heikkila, M. (2021), ‘The rise of AI surveillance’, Politico, 26 May 2021, https://www.politico.eu/article/\\nthe-rise-of-ai-surveillance-coronavirus-data-collection-tracking-facial-recognition-monitoring.\\n14 Vinocur, N. (2020), ‘French politicians urge deployment of surveillance technology after series of attacks’, \\nPolitico, 30 October 2020, https://www.politico.eu/article/french-politicians-urge-deployment-of-surveillance-\\ntechnology-after-series-of-attacks.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 9, 'page_label': '8'}, page_content='AI governance and human rights\\nResetting the relationship\\n8 Chatham House\\nMany AI tools abuse human rights as a collateral consequence of their operation. \\nAI risks embedding and exaggerating bias and discrimination, invading privacy, \\nreducing personal autonomy and making society more, rather than less, unequal. \\nFor example, AI sentencing tools may discriminate against minorities, potentially \\nturning back decades of progress towards equality. AI in healthcare may harm \\nhuman health if algorithms are incorrect or biased,15 while AI in welfare-provision \\nor migration may make unfair decisions on eligibility. AI tools may infer sensitive \\ninformation about individuals in violation of their privacy.\\nEven an AI tool designed with the intention of implementing scrupulous standards \\nof fairness will fail if it does not replicate the complex range of factors and subtle, \\ncontext-specific decision-making processes of humans. Unchecked, AI systems \\ntend to exacerbate structural imbalances of power and to disadvantage the most \\nmarginalized in society.16\\nFurther, some AI tools may have outputs detrimental to humanity through their \\npotential to shape human experience of the world. For example, AI algorithms \\nin social media may, by distorting the availability of information, manipulate \\naudience views in violation of the rights to freedom of thought and opinion,17 or \\nprioritize content that incites hatred and violence between social groups.18 AI used \\nto detect aptitudes or to select people for jobs, while intended to broaden human \\nhorizons and ambition, risks doing the opposite. Without safeguards, AI is likely \\nto entrench and exaggerate social divides and divisions, distort our impressions \\nof\\xa0the world and thus have negative consequences on aspects of human life. These \\nrisks are amplified by the difficulty of identifying when AI fails, for example when \\nit is malfunctioning, manipulative, acting illegally or making unfair decisions. \\nAt\\xa0present, companies rarely make public their identification of mistakes or errors \\nin their AI. Consumers cannot therefore see which standards have been met.\\nFinally, AI may entrench and even exacerbate social divides between rich \\nand poor, worsening the situation of the most vulnerable. As AI development \\nand implementation is largely driven by the commercial sector, it risks being \\nharnessed for the benefit of those who can pay rather than to resolve the world’s \\nmost significant challenges, and risks being deployed in ways that further \\ndispossess\\xa0vulnerable communities around the world.19\\n15 Park, Y. et al. (2020), ‘Evaluating artificial intelligence in medicine: phases of clinical research’, \\nJAMIA\\xa0Open,\\xa03(3), October 2020, pp. 326–31, https://doi.org/10.1093/jamiaopen/ooaa033.\\n16 European Digital Rights (EDRi) et al. (2021), Civil Society Statement on an EU Artificial Intelligence Act for \\nFundamental Rights, 30 November 2021, https://edri.org/wp-content/uploads/2021/12/Political-statement-on-\\nAI-Act.pdf; Kalluri, P. (2020), ‘Don’t ask if artificial intelligence is good or fair, ask how it shifts power’, Nature, \\n7\\xa0July 2020, https://www.nature.com/articles/d41586-020-02003-2.\\n17 Jones, K. (2019), Online Disinformation and Political Discourse: Applying a Human Rights Framework, \\nResearch\\xa0Paper, London: Royal Institute of International Affairs, https://www.chathamhouse.org/2019/11/\\nonline-disinformation-and-political-discourse-applying-human-rights-framework.\\n18 Kornbluh, K. (2022), ‘Disinformation, Radicalization, and Algorithmic Amplification: What Steps Can Congress \\nTake?’, Just Security blog, 7 February 2022, https://www.justsecurity.org/79995/disinformation-radicalization-\\nand-algorithmic-amplification-what-steps-can-congress-take.\\n19 Hao, K. (2022), ‘AI Colonialism’, MIT Technology Review, 19 April 2022, \\nhttps://www.technologyreview.com/2022/04/19/1049592/artificial-intelligence-colonialism.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 10, 'page_label': '9'}, page_content='9 Chatham House\\n03 \\nGoverning AI:  \\nwhy human rights?\\nHuman rights have been wrongly overlooked in AI \\ngovernance discussions. They\\xa0offer clarity and specificity, \\ninternational acceptance and legitimacy, and mechanisms \\nfor\\xa0implementation, oversight and accountability.\\nIn the 1940s, there was fervent belief that human rights would be central to \\nworld peace and to human flourishing, key not only to safeguarding humanity \\nfrom catastrophe but to the enjoyment of everyday life.20 Supporters of the ‘vast \\nmovement of public opinion’21 in favour of human rights at that time would be \\namazed at their relative absence from today’s debate on AI.\\n3.1 Human rights overlooked\\nAI governance has much to gain from a multidisciplinary (and potentially \\ninterdisciplinary) approach, drawing from, among others, philosophy, human rights \\nlaw, science and technology studies, sociology, statistics, diverse impact assessment \\nand audit practices and stakeholder theory. However, with some exceptions,22 \\n20 See David Maxwell Fyfe’s closing speech for the UK prosecution at Nuremberg, available at \\n‘The\\xa0Human’s\\xa0In\\xa0the\\xa0Telling’, https://thehumansinthetelling.wordpress.com.\\n21 René Brunet, former delegate to the League of Nations, quoted in Lauren, P.G. (2011), The Evolution \\nof\\xa0International Human Rights: Visions Seen, 3rd edn, Philadelphia: University of Pennsylvania Press, p. 153.\\n22 Exceptions include the EU’s Artificial Intelligence Act (discussed below) and academic texts including: \\nMcGregor,\\xa0L., Murray, D. and Ng, V. (2019), ‘International Human Rights Law as a Framework for \\nAlgorithmic Accountability’, International & Comparative Law Quarterly, 68(2), April 2019, pp. 309–43, \\nhttps://doi.org/10.1017/S0020589319000046; and Yeung, K., Howes, A. and Pogrebna, G. (2019), ‘AI\\xa0Governance \\nby Human Rights-Centred Design, Deliberation and Oversight: an end to Ethics Washing’, in\\xa0Dubber, M. and \\nPasquale, F. (eds) (2019), The Oxford Handbook of AI Ethics, Oxford: Oxford University Press. The White House’s \\nrecent ‘Blueprint for an AI Bill of Rights’ helpfully introduces the language of rights into mainstream AI governance \\nin the US, albeit without focusing directly on the existing international human rights framework. See The White \\nHouse Office of Science and Technology Policy (2022), ’Blueprint for an AI\\xa0Bill of Rights: Making Automated \\nSystems Work For The American People’, https://www.whitehouse.gov/ostp/ai-bill-of-rights.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 11, 'page_label': '10'}, page_content='AI governance and human rights\\nResetting the relationship\\n10 Chatham House\\nthe\\xa0human rights framework has been overlooked as an existing and flexible \\nbaseline\\xa0for AI governance.\\nAI governance initiatives are often branded as ‘AI ethics’, ‘responsible AI’ or ‘value \\nsensitive design’. Some of these initiatives, such as the Asilomar AI Principles,23 \\nare\\xa0statements drawn primarily from the philosophical discipline of ethics. Many are \\nmultidisciplinary statements of principle, and so may include human rights law as an \\naspect of ‘ethics’. For example, the UNESCO Recommendation on the Ethics of Artificial \\nIntelligence lists ‘[r]espect, protection and promotion of human rights and fundamental \\nfreedoms and human dignity’ as the first of its ‘values’ to be respected by all actors in the \\nAI system life cycle.24 And the Institute of Electrical and Electronics Engineers (IEEE)’s \\nStandard Model Process for Addressing Ethical Concerns during System Design lists \\nas\\xa0its first ‘ethical principle’ that ‘[h]uman rights are to be protected’.25\\nMany sets of AI governance principles produced by companies, governments, \\ncivil\\xa0society and international organizations fail to mention human rights \\nat all. Of\\xa0those that do, only a small proportion (around 15\\xa0per\\xa0cent) take \\nhuman rights as a\\xa0framework.26 Most national AI strategies do not engage \\nwith\\xa0human\\xa0rights in depth.27\\nSo why, then, are human rights not central to AI governance?\\nFirst, in many arenas, human rights are simply omitted from discussions on AI \\ngovernance. Software developers and others in the AI industry generally do not \\ninvolve anyone from the human rights community in discussions on responsible \\nAI. There is a marked lack of human rights-focused papers or panels at the largest \\n23  Future of Life Institute (2017), Asilomar AI Principles, https://futureoflife.org/2017/08/11/ai-principles.\\n24  UN Educational, Scientific and Cultural Organization (2021), Recommendation on the Ethics of Artificial \\nIntelligence, Paris: UNESCO, https://unesdoc.unesco.org/ark:/48223/pf0000381137, section III.1.\\n25  Institute of Electrical and Electronics Engineers (IEEE), Standard Model Process for Addressing Ethical \\nConcerns during System Design, IEEE Std 7000-2021, Annex H.\\n26 A 2020 review of 36 prominent sets of AI principles from around the world, authored by a diverse range \\nof\\xa0governmental and non-governmental bodies, found that only 23 referred to international human rights. \\nOnly one-half of the government documents reviewed include any reference to human rights. Five of the 36\\xa0sets \\nof AI\\xa0principles used international human rights as a framework for their work. See Fjeld, J. et al. (2020), \\nPrincipled Artificial Intelligence: Mapping Consensus in Ethical and Rights-based Approaches to Principles for\\xa0AI, \\nBerkman Klein Center for Internet & Society, Research Publication No. 2020-1, http://dx.doi.org/10.2139/\\nssrn.3518482. A separate evaluation of 22 sets of guidelines makes no reference to human rights: Hagendorff,\\xa0T. \\n(2020), ‘The\\xa0Ethics of AI Ethics: An Evaluation of Guidelines’, Minds and Machines, 30, pp. 99–120, \\nhttps://link.springer.com/article/10.1007/s11023-020-09517-8.\\n27 A review of all national AI strategies published by 1 January 2020 found that while a majority referred to human \\nrights, most only mentioned them in passing rather than engaging with them in depth. Only European states and \\nIndia referred to human rights; many East and South East Asian states had developed a\\xa0strategy, but these did \\nnot refer to human rights. None of the strategies reviewed came from Africa, and only one from Latin America \\n(Colombia). Global Partners Digital and Stanford Global Digital Policy Incubator (2020), National Artificial \\nIntelligence Strategies and Human Rights: A Review, https://www.gp-digital.org/wp-content/uploads/2020/04/\\nNational-Artifical-Intelligence-Strategies-and-Human-Rights%E2%80%94A-Review_April2020.pdf.\\nMany sets of AI governance principles produced \\nby\\xa0companies, governments, civil\\xa0society\\xa0and \\ninternational organizations fail\\xa0to\\xa0mention human \\nrights at all.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 12, 'page_label': '11'}, page_content='AI governance and human rights\\nResetting the relationship\\n11 Chatham House\\ninternational conferences on responsible AI.28 Corporate-level discussion of AI \\nethics\\xa0and their implementation often fails to refer to, or engage with, human rights. \\nJob advertisements for corporate AI ethics specialists usually make no reference \\nto human rights. Governments focused on AI ethics may not involve human rights \\nlawyers in policy development until a late stage, if at all. In contrast, human rights \\nare often the focus of civil society and academic discussions in different venues\\xa0– \\nand with different participants\\xa0– to those where corporate and public sector AI \\ngovernance decisions are made.29 Notable exceptions are discussions hosted by \\ninternational organizations such as the UN and the Council of Europe, where human \\nrights law forms a well-established shared lexicon; and the European Union, which \\nhas placed human rights at the core of the draft\\xa0Artificial Intelligence Act.30\\nSecond, certain myths about human rights can too often lead to them being \\ndisregarded by those involved in AI governance discussions. The following \\nare\\xa0some of the most common.\\n3.2 Myths about human rights\\nMyth 1. ‘Ethics holds all the answers’\\nEthics and human rights are distinct disciplines with valuable, complementary \\nroles to play in AI governance. Both ethics and human rights share the rationale \\nof curbing state and corporate power by acting as a bulwark of the interests of the \\nindividual. But they offer different, complementary means for reaching this end. \\nOne cannot substitute for the other or be considered at the exclusion of the other. \\nBoth disciplines must be considered together.\\nEthics plays an important role in preceding and supplementing regulation. \\nIt has been the subject of much pioneering research and implementation in \\nthe field of AI governance. However, ethics is a branch of philosophy, not \\na\\xa0system of norms: multiple versions are possible, and\\xa0– despite, or perhaps \\nexacerbated by, the efforts to draft so many sets of AI ethics principles\\xa0– there is \\ncurrently a lack of international consensus as to what precisely AI ethics entails. \\nSignificant differences of both substance and terminology between these sets of \\nprinciples make it difficult for companies and public bodies to understand their \\nresponsibilities, and for individuals to know what standards to expect.\\n28 See the analysis of research contributions and shortcomings, including significant influence of industry, at \\nthe ACM Conference on Fairness, Accountability and Transparency: Laufer, B. et al. (2022), ‘Four years of FAccT: \\nA\\xa0Reflexive, Mixed-Methods Analysis of Research Contributions, Shortcomings, and Future Prospects’, ACM \\nDigital Library, https://doi.org/10.1145/3531146.3533107.\\n29 Of over 180 papers accepted for the ACM Conference on Fairness, Accountability and Transparency 2022\\xa0– \\n‘a\\xa0computer science conference with a cross-disciplinary focus’\\xa0– only three refer to human rights in their abstract. ACM \\nFAccT (2022), ‘Accepted Papers’, https://facctconference.org/2022/acceptedpapers.html. In contrast, Access Now’s \\nRightsCon Conference 2022, on technology and human rights, included Artificial Intelligence as one of its programme \\ntracks; but only 11\\xa0per\\xa0cent of its attendees came from the private sector and 4\\xa0per\\xa0cent from government. RightsCon \\n(2022), Outcomes Report, https://www.rightscon.org/cms/assets/uploads/2022/09/Outcomes-Report-2022-v10.pdf.\\n30 Proposal for a Regulation of the European Parliament and of the Council laying down harmonised rules on \\nartificial intelligence (Artificial Intelligence Act) and amending certain Union legislative acts, COM/2021/206 \\nhttps://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX:52021PC0206.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 13, 'page_label': '12'}, page_content='AI governance and human rights\\nResetting the relationship\\n12 Chatham House\\nThe malleability of ethics means that it is difficult for civil society to hold \\nother\\xa0actors to account. Some technology companies face criticism for so-called \\n‘ethics-washing’ undertaken for reputational purposes,31 and for exerting undue \\ninfluence on some ethics researchers through funding.32 Courts and tribunals do \\nnot allocate remedies for compliance with ethics. Moreover, while ethical principles \\nare intended to ensure that technology reflects moral values, a focus on ethics \\nmay\\xa0minimize the appetite for legal regulation.33\\nAlthough in some environments, the branding of ‘ethics’ may be more palatable than \\nthat of human rights for political reasons, it is of primary importance that human \\nrights are considered at all – whatever the branding. To avoid conceptual confusion, \\nhuman rights ought to be regarded as parallel to ethics rather than as a mere element \\nof it. Any principles and processes of ethics should complement, rather than compete \\nwith, the existing human rights legal system. Conflicts between norms are damaging \\nas they undermine the legal certainty and predictability of regulatory behaviour \\non\\xa0which states, businesses and individuals rely.\\nCurrent popular support for AI ethics in principle, without a shared understanding \\nof what AI ethics means in practice, has similarities with support for human rights \\nin\\xa0the 1940s. There was widespread support then for the concept of human rights, to \\nprevent a repetition of the atrocities of the Second World War and to end domination \\nand repression. However, there was no specific understanding or consensus on what \\nexactly ‘human rights’ meant. Establishing agreement on the content of the Universal \\nDeclaration of Human Rights\\xa0– and later the International Covenant on Civil and \\nPolitical Rights and International Covenant on Economic, Social and Cultural \\nRights\\xa0– required worldwide canvassing, expert input, negotiation and political \\ncompromise.34 There is no evidence that reaching universal agreement on AI ethics \\nwithout reference to the already-agreed human rights framework would be easier, \\nor\\xa0less politically charged, than those 20th century debates.\\nMyth 2. ‘Human rights prevent innovation’\\nHuman rights do not prevent innovation or undermine a ‘move fast and break \\nthings’ ethos, save that they entail compliance with minimum standards and \\ntherefore forbid certain egregious activities. Most innovators want a level playing \\nfield, and to avoid being undercut by actors with lower standards or being caught \\nin a ‘race to the bottom’ with unscrupulous competitors. Innovators want to know \\nhow they can meet shared standards and inspire trust in their products. Human \\nrights provide an appropriate basis for standards and processes internationally. \\nFor businesses, considering human rights from the outset of AI development \\nand\\xa0deployment may help to foster customer trust and minimize potential \\ncosts\\xa0and\\xa0time expended in litigation at a later stage.\\n31 Carnegie Council for Ethics in International Affairs (undated), ‘Ethics Washing’, \\nhttps://www.carnegiecouncil.org/explore-engage/key-terms/ethics-washing (accessed 12 Sep. 2022).\\n32  Williams, O. (2019), ‘How Big Tech funds the debate on AI ethics’, New Statesman, 6 June 2019, \\nhttps://www.newstatesman.com/science-tech/2019/06/how-big-tech-funds-debate-ai-ethics.\\n33  Wagner, B., (2018), ‘Ethics as an escape from regulation. From “ethics-washing” to ethics-shopping?’ \\nin\\xa0Bayamlioglu, E. et al. (eds) (2018), Being Profiled:Cogitas Ergo Sum: 10 Years of Profiling the European Citizen, \\npp.\\xa084–8, Amsterdam: Amsterdam University Press, https://doi.org/10.1515/9789048550180-016.\\n34 Lauren (2011), The Evolution of International Human Rights.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 14, 'page_label': '13'}, page_content='AI governance and human rights\\nResetting the relationship\\n13 Chatham House\\nMyth 3. ‘Human rights are complex and entail \\nexpensive legal advice’\\nWhile human rights can appear complex to non-specialists, initiatives such \\nas\\xa0the UN’s B-Tech project show how the technology industry and investors can \\nimplement their human rights responsibilities.35 Routine inclusion of human rights \\nin computer science and coding training could reduce the perception of complexity. \\nIn reality, human rights are no more complex than any equivalent system of rules \\nor principles: they consist of clear rules, with steps to be followed in implementing \\nthem. While novel situations will still pose challenges, human rights have been \\ndeveloped over many years and are inherently flexible to adapt to such challenges. \\nIn this way, human rights have answers for many situations, in terms of steps to \\nfollow or outcomes to\\xa0reach.\\nA business trying to establish ethical credentials needs advice in order to do so \\neffectively\\xa0– this is the case whatever the source of the rules followed. Following \\nhuman rights standards means following relatively clear, existing rules and \\nminimizing the chances of public censure or litigation for failure to comply.\\nMyth 4. ‘Human rights are about governments’\\nHuman rights are not commonly part of the lexicon of AI developers and corporate \\nethics advisers – particularly outside the EU\\xa0– because they are seen as regulating \\ngovernment, rather than corporate, activity.\\nWhile states are the primary bearer of duties under international human rights \\nlaw, all companies have responsibilities to respect human rights. The Office of \\nthe UN High Commissioner on Human Rights (OHCHR)’s Guiding Principles \\non\\xa0Business and Human Rights, unanimously endorsed by the UN Human Rights \\nCouncil (HRC) and General Assembly (UNGA) in 2011, state that governments \\nare obliged to take reasonable steps to ensure that companies and other non-state \\nactors respect human rights, and that companies have a responsibility to respect \\nhuman rights in their activities worldwide, including through due diligence and \\nimpact assessment.36 Consideration of human rights impacts ought therefore \\nto\\xa0be\\xa0a\\xa0standard part of corporate practice.\\nHowever, the extent of corporate responsibilities is only patchily understood. \\nThis\\xa0situation is changing, slowly and gradually, as businesses find it in their \\ninterests to take account of human rights impacts.37 Increasingly, both national \\nlaws and investors’ environmental, social and governance (ESG) or equivalent \\nframeworks, plus civil society and public pressure, are obliging companies to \\ngive due regard to human rights. The European Commission’s proposed directive \\n35 UN Office of the High Commissioner on Human Rights (undated), ‘B-Tech Project’, https://www.ohchr.org/en/\\nbusiness-and-human-rights/b-tech-project (accessed 12 Sep. 2022).\\n36 UN Office of the High Commissioner on Human Rights (2011), Guiding Principles on Business and Human  \\nRights, https://www.ohchr.org/sites/default/files/Documents/Publications/GuidingPrinciplesBusiness \\nHR_EN.pdf.\\n37 Moynihan, H. and Alves Pinto, T. (2021), The Role of the Private Sector in Protecting Civic Space, Synthesis Paper, \\nLondon: Royal Institute of International Affairs, https://www.chathamhouse.org/2021/02/role-private-sector-\\nprotecting-civic-space.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 15, 'page_label': '14'}, page_content='AI governance and human rights\\nResetting the relationship\\n14 Chatham House\\nrequiring mandatory human rights and environmental due diligence by larger \\ncompanies based or active in the EU would be transformative and should herald \\na\\xa0consistency of approach within the EU.38\\nMyth 5. ‘Human rights are radical’\\nThere are two dimensions to this particular myth: first, that\\xa0– in line with \\npopular\\xa0news coverage\\xa0– human rights are only relevant to extreme situations, \\nsuch as the treatment of criminals, immigrants or terrorists. This view is plain \\nwrong: human rights are about everyday protection from harm and discrimination \\nfor every adult and child, living free from state interference, and being provided \\nwith basic entitlements. In democracies, most people have a general, unspoken \\nassumption that their human rights will be respected: for example, if arrested \\nthey will be treated with dignity; if prosecuted they will be granted a fair trial \\nin a language they understand; or if voting their vote will be secret and will be \\ncounted fairly. Human rights routinely inform new legislation and policies, from \\ndata protection to\\xa0social housing and social security. They are not often politically \\ncontroversial. They are only newsworthy on the rare occasions when they are \\ndenied, or when they are portrayed as an obstacle to popular policies. The human \\nrights law framework is not a radical philosophy, but a check and balance against \\ndiscrimination or indignity in policy development.\\nThe second dimension to this myth is a misconception that human rights \\nare\\xa0absolutist in nature: that, for example, they prohibit developments such as \\nfacial recognition technology. The desire for quick political soundbites in today’s \\nworld encourages absolutist positions that can do human rights a disservice. \\nThe reality of human rights is more nuanced. For example, many civil society \\norganizations currently assert that all facial recognition technology is contrary \\nto human rights law.39 But this is a shorthand for asserting that facial recognition \\nas commonly configured (i.e.\\xa0involving mass capturing and retention of personal \\ndata and potentially discriminatory judgements without regard to human rights \\nconsiderations) is contrary to human rights law. In fact, human rights law does not \\nlead to a conclusion that facial recognition, properly configured and constrained, \\nshould be banned where there are good reasons of safety or security for using it.40 \\nRather, in this case as elsewhere, human rights law balances rights and interests \\nto\\xa0reach nuanced, subtle judgements.\\n38  European Commission (2022), Proposal for a Directive of the European Parliament and of the Council on \\nCorporate Sustainability Due Diligence, COM(2022) 71 final (23.02.22), https://ec.europa.eu/info/sites/default/\\nfiles/1_1_183885_prop_dir_susta_en.pdf.\\n39  For example, Liberty’s website states that: ‘Facial recognition technology…breaches everyone’s human rights, \\ndiscriminates against people of colour and is unlawful. It’s time to ban it.’ See Liberty (2022), ‘Facial Recognition’, \\nhttps://www.libertyhumanrights.org.uk/fundamental/facial-recognition.\\n40  See, for example, Information Commissioner’s Office (2021), Information Commissioner’s Opinion: The use \\nof\\xa0live facial recognition technology in public places, https://ico.org.uk/media/for-organisations/documents/ \\n2619985/ico-opinion-the-use-of-lfr-in-public-places-20210618.pdf.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 16, 'page_label': '15'}, page_content='AI governance and human rights\\nResetting the relationship\\n15 Chatham House\\nMyth 6. ‘Human rights are vague’\\nThere is a perception that human rights norms are too vague to guide AI. For \\nexample, some advocates of ethics argue that human rights are unable to provide \\nguidance when values conflict.41 These objections are largely unfounded. One \\nstrength of human rights law is its system for weighing competing rights and \\ninterests, whether the balance is to be struck between competing individual \\nrights\\xa0or with other collective or societal interests.42\\nMany human rights are framed in terms that make this balancing explicit. \\nFor\\xa0example, Article 21 of the International Covenant on Civil and Political Rights \\nstates that the right of peaceful assembly shall be subject to no restrictions, ‘other \\nthan those imposed in\\xa0conformity with the law and… necessary in a democratic \\nsociety in the interests of national security or public safety, public order (ordre \\npublic), the protection of public health or morals or the protection of the rights \\nand freedoms of others’. In\\xa0considering whether this right has been violated, \\nthe UN Human Rights Committee will consider first whether there has been an \\ninterference, then if so, whether that interference is lawful and both ‘necessary for \\nand proportionate to’ one or more of the legitimate grounds for restriction listed in \\nthe article.43 UN human rights bodies, national and regional courts have developed \\nextensive jurisprudence on the appropriate balancing of rights and interests, \\nbalancing flexibility with predictability. These well-established, well-understood \\nsystems have repeatedly proven themselves capable of adaptation in the face of new \\npolicy tools and novel situations. For example, the European Court of Human Rights \\n(ECtHR) recently developed new tests by which to assess bulk interception of\\xa0online \\ncommunications for intelligence purposes.44\\nThe impact of AI is a novel but not insurmountable challenge, as emerging \\njurisprudence is already demonstrating. Indeed, one strength of international human \\nrights law is its capacity to develop incrementally both as societal standards progress \\nand in the face of new factual situations.45\\nMyth 7. ‘Human rights get it wrong’\\nSome may consider that human rights protect the wrong values, apply protection \\nin the wrong ways or are too rigid to apply to technological or social developments. \\n41  Canca, C. (2019), ‘Why Ethics cannot be replaced by the Universal Declaration of Human Rights’, UN University \\nOur World, 15 August 2019, https://ourworld.unu.edu/en/why-ethics-cannot-be-replaced-by-the-universal-\\ndeclaration-of-human-rights.\\n42  Yeung, Howes and Pogrebna (2019), ‘AI Governance by Human Rights-Centred Design, Deliberation \\nand Oversight’.\\n43  UN Human Rights Committee (2020), General Comment No. 37 on the right of peaceful assembly \\n(Article 21), para. 36.\\n44  Big Brother Watch and others v UK (ECtHR App no 58170/13).\\n45  See section\\xa03.3 below.\\nOne strength of human rights law is its system \\nfor weighing competing rights and interests.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 17, 'page_label': '16'}, page_content='AI governance and human rights\\nResetting the relationship\\n16 Chatham House\\nFor example, it has been suggested by some policymakers and academics that \\nthe individual right to privacy should be replaced or augmented by a concept of \\ncollective interest in appropriate handling of data that is sensitive to the interests \\nof minority groups.46 Group privacy may be a useful political concept in assessing \\nappropriate limits of state or corporate power resulting from mass collection \\nand\\xa0processing of data.47 But it cannot substitute for human rights law. Such \\nclaims\\xa0underestimate the flexibility of human rights and its processes, including due \\ndiligence and human rights impact assessment, to secure the protection of human \\nrights for all rather than just for those who claim infringement. The right to privacy \\nis capable of evolution in light of competing interests, and enables a\\xa0balance to be \\nstruck between privacy and the public interest in data-sharing and accessibility, \\nwhile safeguarding the interests of groups categorized as such by AI by insistence on \\nboth freedom from discrimination and fairness and due process in decision-making. \\nThere may be scope for considering greater empowerment of\\xa0data subjects48 and/\\nor group enforcement of rights; but it would be a rash move to abandon many \\nyears of\\xa0judicial interpretation and scholarship, including concerns about the \\ndisplacement of individual rights by group rights, by adding, or\\xa0replacing them \\nwith, new legal constructs.\\nMyth 8. ‘Human rights are organized around national models’\\nHuman rights obligations are primarily owed by a state’s government to people \\nwithin that state’s territory or jurisdiction. These jurisdictional limitations are \\nunder pressure: for example, UNGA has stressed that arbitrary surveillance and \\ncollection of personal data can violate various human rights, including when \\nundertaken extraterritorially.49 Regarding businesses, the corporate responsibility \\nto respect human rights applies in respect of all individuals affected by a company’s \\noperations, regardless of location.50 In practical terms, businesses should consider \\ntheir human rights responsibilities towards everyone impacted by their work, \\nin any country.\\nMyth 9. ‘Human rights entail greater legal risk’\\nHuman rights are legal rules, and so do entail accountability through courts and \\ntribunals. But this accountability does not hinge on whether an organization pays \\nattention to human rights, but on whether it is liable by reference to a rule of law. \\n46 For example, Mantelero (2016), ‘Personal data for decisional purposes in the age of analytics: From an \\nindividual to a collective dimension of data protection’, Computer Law & Security Review, February 2016, 32(2), \\npp. 238–55, https://doi.org/10.1016/j.clsr.2016.01.014.\\n47  van der Sloot, B. (2017), ‘Do groups have a right to protect their group interest in privacy and should they? \\nPeeling the onion of rights and interests protected under Article 8 ECHR’, in Taylor, L., Floridi, L. and van der \\nSloot, B. (2017), Group Privacy: New Challenges of Data Technologies, Cham: Springer, p. 223.\\n48  Wong, J., Henderson, T. and Ball, K. (2022), ‘Data protection for the common good: Developing a framework \\nfor a data protection-focused data commons’, Data & Policy, 4(e3), https://doi.org/10.1017/dap.2021.40.\\n49 UN General Assembly Resolution (2020), The right to privacy in the digital age, A/RES/75/176 \\n(28\\xa0December\\xa02020), preambular para. 24.\\n50 UN Office of the High Commissioner for Human Rights (2011), Guiding Principles on Business and Human \\nRights, principle 11.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 18, 'page_label': '17'}, page_content='AI governance and human rights\\nResetting the relationship\\n17 Chatham House\\nConsidering human rights will not place a company or government at greater risk \\nfrom human rights claims. On the contrary, addressing human rights issues should \\nhelp to protect against potential claims.\\n3.3 What human rights have to offer\\nHuman rights law provides a means to define the harm that AI should avoid.51 \\nIt\\xa0places its focus on the interests of each individual and addresses the most \\npressing societal concerns of AI, including non-discrimination, fairness and privacy. \\nIt\\xa0provides an excellent starting point by which to assess whether and to what extent \\nAI is ‘for good’. Economic and social rights offer a basis for considering societal \\ndistribution of AI’s potential benefits.\\nHuman rights offer a framework for regulating AI that is an existing system of \\ninternational, regional and domestic law, commanding international legitimacy \\nand a shared language across the world. This framework should be adopted \\nin respect of AI, not\\xa0only because of its intrinsic merit but because the current \\ngeopolitical stasis is likely to prevent effective multilateral cooperation on new \\nnormative frameworks. The focus of discussion should not be on whether human \\nrights can or should be applied to AI, nor on potential alternatives, but on how the \\nexisting framework of human rights does apply in the field of AI. This is already \\nthe\\xa0focus of international organizations at both regional and global level.52\\nHuman rights crystallize a set of ethical values into international norms.53 \\nThe\\xa0system is not perfect, and was not created with AI in mind, but is a universally \\nagreed blueprint for the protection of human values and the common good \\nthat has proven itself capable of adaptation to new circumstances. It\\xa0avoids the \\nneed for\\xa0fresh theoretical debates on the relative merits of different approaches. \\nAs\\xa0a\\xa0set\\xa0of norms, human rights avoid the allegation\\xa0– often levelled at\\xa0ethics\\xa0– \\nof\\xa0being vague and malleable enough to suit corporate interests.\\nHuman rights are relatively clear. It is possible to list comprehensively the legally \\nbinding international, regional and domestic human rights obligations that apply \\nin each country in the world. The meaning of those obligations is reasonably \\nwell-understood.54\\nThe human rights approach has proved relatively successful over more than \\n70\\xa0years, developing incrementally with the benefit of several generations \\nof academic input, governmental negotiation, civil society input and court \\n51  McGregor, L., Murray, D. and Ng, V. (2019), ‘International Human Rights Law as a Framework for Algorithmic \\nAccountability’, International & Comparative Law Quarterly, 68(2), April 2019, https://doi.org/10.1017/\\nS0020589319000046, pp. 324–27.\\n52 For example, the UN B-Tech Project, https://www.ohchr.org/en/business-and-human-rights/b-tech-\\nproject; and Council of Europe’s Committee on Artificial Intelligence, https://www.coe.int/en/web/artificial-\\nintelligence/cai.\\n53  ‘There is no conflict between ethical values and human rights, but the latter represent a specific crystallisation \\nof these values that are circumscribed and contextualised by legal provision and judicial decisions’. Mantelero,\\xa0A. \\nand Esposito, S. (2021), ‘An evidence-based methodology for human rights impact assessment (HRIA) in \\nthe development of AI data-intensive systems’, Computer & Security Law Review, 2021, https://ssrn.com/\\nabstract=3829759, p. 6.\\n54  UN Special Rapporteur on Extreme Poverty (2019), Report on use of digital technologies in the welfare state, \\nA/74/493, https://digitallibrary.un.org/record/3834146?ln=en#record-files-collapse-header.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 19, 'page_label': '18'}, page_content='AI governance and human rights\\nResetting the relationship\\n18 Chatham House\\nrulings from many parts of the world. It has evolved in tandem with societal \\ndevelopment, its impact gradually increasing without meeting widespread \\ncalls\\xa0for\\xa0abandonment\\xa0or radical change.\\nHuman rights provide processes and accountability \\nas\\xa0well as principles\\nHuman rights law is accompanied by a vast range of practical tools for \\nimplementation, political oversight and legal accountability that are absent \\nfrom\\xa0ethics.55 Breaches of human rights entail legal as well as political avenues \\nof\\xa0redress. The international human rights framework includes a range of \\nremedial mechanisms with practical effect, ranging from civil society advocacy \\nthrough domestic and international courts, to scrutiny by UN bodies and other \\nstates. In\\xa0many parts of the world, violations of rights by government may \\nbe challenged in\\xa0court with legally binding effect\\xa0– acting as an important \\nconstraint\\xa0on state power.\\nAs companies and governments already have human rights commitments, their \\nuse of AI will be scrutinized by human rights mechanisms in any case, including \\nthrough claims made to domestic courts in the event of alleged breach. Human \\nrights have already formed the basis for high-profile rulings on, for example, \\nimage\\xa0databases56 and uses of facial recognition technology.57\\nHuman rights have international acceptance and legitimacy\\nInternational human rights law benefits from a higher degree of international \\nacceptance and legitimacy than any other system of values. Governments in every \\ncontinent know and understand the core human rights treaties. Every state is party \\nto some of them, while some treaties have near-universal ratification. This remains \\nthe case, despite an apparently waning commitment to the universality of human \\nrights in the rhetoric of certain countries.58 Human rights have played a role, to \\na greater or lesser extent, in shaping the policies and activities of governments \\naround the world.59\\n55 van Veen, C. and Cath, C., (2018), ‘Artificial Intelligence: What’s Human Rights Got to Do with It?’, Data & \\nSociety: Points blog, 14 May 2018, https://points.datasociety.net/artificial-intelligence-whats-human-rights-got-\\nto-do-with-it-4622ec1566d5; Latonero, M. (2020), AI Principle Proliferation as a Crisis of Legitimacy, Carr Center \\nDiscussion Paper Series, Issue 2020-011, https://carrcenter.hks.harvard.edu/files/cchr/files/mark_latonero_ai_\\nprinciples_6.pdf?m=1601910899.\\n56 For example, by the Canadian Office of the Privacy Commissioner (2021), ‘Clearview AI’s unlawful \\npractices represented mass surveillance of Canadians, commissioners say’, news release, 2 February 2021, \\nhttps://www.priv.gc.ca/en/opc-news/news-and-announcements/2021/nr-c_210203/?=february-2-2021.\\n57  For example, the Marseille Administrative Tribunal ruled against the use of facial recognition technology at the \\nentrances to French high schools in the La Quadrature du Net case, https://www.laquadrature.net/wp-content/\\nuploads/sites/8/2020/02/1090394890_1901249.pdf.\\n58  For example, Position Paper of the People’s Republic of China for the 77th Session of the United Nations General \\nAssembly, 20 September 2022, http://geneva.china-mission.gov.cn/eng/dbdt/202209/t20220921_10768735.htm,  \\nsection IV.\\n59  Latonero, M. (2020), AI Principle Proliferation as a Crisis of Legitimacy, Carr Center Discussion Paper \\nSeries, Issue 2020-011, https://carrcenter.hks.harvard.edu/files/cchr/files/mark_latonero_ai_principles_6.\\npdf?m=1601910899, p. 6.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 20, 'page_label': '19'}, page_content='AI governance and human rights\\nResetting the relationship\\n19 Chatham House\\nUN processes affecting all states, such as the HRC’s Universal Periodic Review \\nand\\xa0the UN treaty bodies’ periodic examinations of states’ compliance, entail that \\nevery UN member state engages with the international human rights architecture. \\nRegional treaties that have strong local support reinforce these UN instruments \\nin\\xa0some parts of the world.60 International human rights law has constitutional \\nor\\xa0quasi-constitutional status in many countries, notably in Europe, embedding \\nit\\xa0deep into systems of governance.61 Civil society uses the human rights law \\nframework as a basis for monitoring state and corporate activities worldwide.\\nThis international legitimacy has given human rights a significant role in the \\nproduction of internationally negotiated sets of AI governance principles. For \\nexample, the OECD AI Principles call on all actors to respect the rule of law, human \\nrights and democratic values throughout the AI system life cycle.62 As discussed \\npreviously, UNESCO’s Recommendation on the Ethics of Artificial Intelligence \\nnames human rights and fundamental freedoms as the first of the ‘values’ around \\nwhich it is crafted.63 The\\xa0Council of Europe’s Committee on Artificial Intelligence \\n(CAI) is working on a potential legal framework for the development, design and \\napplication of AI, based on the Council’s standards on human rights, democracy \\nand the rule of law.64 Although the universality of human rights is increasingly \\ncontested, there is still, to a large degree, a global consensus on the continued \\nrelevance of long-agreed human rights commitments.\\nHuman rights achieve a balance between universality \\nand\\xa0sensitivity to national contexts\\nInternational human rights law offers a degree of discretion to governments as to \\nhow they implement each right, within certain parameters. This flexibility is known \\nas the ‘margin of appreciation’ in Europe, now enshrined in the preamble to the \\nECHR,65 and has similar effect in the UN human rights system.66 It varies according \\nto the specific right in question and the impact of any interference: for example, \\nhuman rights law offers governments no discretion in implementing bans on \\ntorture or slavery, but European human rights law permits governments a narrow \\n60 For example, ECHR; Inter-American Charter on Human Rights; African Charter on Human and People’s Rights.\\n61 Yeung, Howes and Pogrebna (2019), ‘AI Governance by Human Rights-Centred Design, Deliberation \\nand Oversight’.\\n62 Organisation for Economic Co-operation and Development (2019), Recommendation of the Council on Artificial \\nIntelligence, https://legalinstruments.oecd.org/en/instruments/OECD-LEGAL-0449, Article 1.2(a).\\n63  UN Educational, Scientific and Cultural Organization (2021), Recommendation on the Ethics of \\nArtificial Intelligence.\\n64 Council of Europe (2022), ‘Inaugural Meeting of the Committee on Artificial Intelligence (CAI)’.\\n65 Protocol 15 to the ECHR, Article 1.\\n66 Shany, Y. (2018), ‘All Roads Lead to Strasbourg?: Application of the Margin of Appreciation Doctrine \\nby\\xa0the\\xa0European Court of Human Rights and the UN Human Rights Committee’,\\xa0Journal of International \\nDispute\\xa0Settlement, 9(2), May 2018, pp. 180–98,\\xa0https://doi.org/10.1093/jnlids/idx011.\\nInternational human rights law offers a\\xa0degree \\nof\\xa0discretion to governments as to\\xa0how they implement \\neach right, within certain parameters.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 21, 'page_label': '20'}, page_content='AI governance and human rights\\nResetting the relationship\\n20 Chatham House\\nmargin of appreciation concerning general bans on protest, and a wider margin \\nconcerning whether governments choose to sanction protestors who intentionally \\ndisrupt ordinary life.67\\nHuman rights are necessary but not sufficient for AI governance\\nInternational human rights law may not currently address all the potential harms \\nto people caused by AI. But it is adaptable to new circumstances and changing \\nsocial norms: the ECHR, for example, is ‘a living instrument which… must be \\ninterpreted in light of present-day conditions.’68 The UN secretary-general’s High \\nLevel Panel on Digital Cooperation has called for an urgent examination of how \\nhuman rights frameworks apply in the digital age.69\\nHuman rights law may develop through new attention to existing rights. For \\nexample, the rights to freedom of thought and opinion are absolute. However, \\ntheir parameters remain relatively unclear because they were largely taken for \\ngranted until challenged by the emergence of a technologically enabled industry \\nof influence.70 Further, new contexts may lead to new understandings and \\nformulations of rights. For example, explainability and human involvement\\xa0– \\ncommonly discussed elements of AI ethics\\xa0– are not usually considered as elements \\nof human rights, but might be found in existing requirements that individuals be \\nprovided with reasons for decisions made concerning them, and of the possibility \\nof contesting those decisions and securing adequate remedies. The Council of \\nEurope’s work on a potential convention is likely to clarify the application of \\nhuman\\xa0rights to\\xa0AI,71 as human rights litigation is already beginning to do.72\\nThe development of human rights law and its subsequent interpretation take time, \\nyet technology moves quickly. Human rights in their current form, while essential, \\nare not sufficient to act as an entire system for the ethical management of AI. \\nHuman rights should rather be the starting point for normative constraints on AI, \\nthe baseline to which new rights or further ethical guardrails might appropriately \\nbe added, including any ethical principles that businesses or other entities may \\nchoose to adopt.\\nThe second half of this paper explores the contributions of human rights in detail \\nand concludes by recommending practical actions to place human rights at the \\nheart of AI governance.\\n67  European Court of Human Rights (2021), Guide on Article 11 of the European Convention on Human Rights, \\nhttps://echr.coe.int/Documents/Guide_Art_11_ENG.pdf.\\n68 Tyrer v United Kingdom, ECtHR App No 5856/72, judgment of 25 April 1978, Series A No 26, para. 31.\\n69 UN Secretary-General’s High-Level Panel on Digital Cooperation (2019), The Age of Digital Interdependence \\nhttps://www.un.org/en/pdfs/DigitalCooperation-report-for%20web.pdf.\\n70  Jones (2019), Online Disinformation and Political Discourse: Applying a Human Rights Framework.\\n71 Council of Europe (2022), ‘Inaugural Meeting of the Committee on Artificial Intelligence (CAI)’.\\n72 See Chapter\\xa06.1 below.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 22, 'page_label': '21'}, page_content='21 Chatham House\\n04 \\nPrinciples of AI \\ngovernance: the \\ncontribution of \\nhuman rights\\nKey principles of human rights law have an important \\nrole\\xa0to\\xa0play in\\xa0determining AI governance standards.\\nThere are three dimensions to AI governance: (i) the substantive standards, \\nor\\xa0principles, that the developers and implementers of AI should meet; (ii) the \\nprocesses to ensure that substantive standards are met; and (iii) accountability \\nand\\xa0remedies for any breach of those standards.\\nIn each of these dimensions, AI governance is immature because technology \\nand\\xa0its\\xa0uses have developed much more rapidly than the rules constraining them. \\nHuman rights law offers baseline standards for all three dimensions.\\n4.1 Principles: the landscape\\nAI ethical principles from companies, civil society and intergovernmental \\norganizations have proliferated in recent years,73 causing more confusion \\nthan clarity through their overlapping nature, number and diversity.74 There \\n73 The AI Ethics Guidelines Global Inventory lists over 165 sets of guidelines. AlgorithmWatch (2022), ‘AI Ethics \\nGuidelines Global Inventory’, https://inventory.algorithmwatch.org.\\n74 Floridi, L. and Cowls, J. (2019), ‘A Unified Framework of Five Principles for AI in Society’, Harvard Data Science \\nReview, 1.1, https://doi.org/10.1162/99608f92.8cd550d1.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 23, 'page_label': '22'}, page_content='AI governance and human rights\\nResetting the relationship\\n22 Chatham House\\nare common themes such as data protection, understandability, transparency \\nfor accountability and tackling bias. But the precise meaning of each of these \\nterms varies.75 Some ethics principles identified\\xa0– such as beneficence and \\nnon-maleficence76\\xa0– are so abstract that they are not easily translatable for practical \\nuse in governance. There is no unifying theme between rival sets of ethical principles \\nand there are debates on the representativeness of those principles, as\\xa0most stem \\nlargely from Europe and North America, from separate corporate and\\xa0national \\ncontexts, and from men.77\\nSome assert that, without unanimity as to what it entails, ethics offers a\\xa0lexicon \\nthat can be used to give a veneer of respectability to any corporate activity. In the \\nwords of Philip Alston, ‘as long as you are focused on ethics, it’s mine\\xa0against yours. \\nI will define fairness, what is transparency, what is accountability. There are no \\nuniversal standards.’78\\n4.2 Principles: Human rights law\\nTo date, there are no international human rights treaties that specifically address \\nthe impact of AI,79 but existing human rights laws apply to applications of AI. \\nThe\\xa0former UN high commissioner for human rights, Michelle Bachelet, clarified \\nthat AI can have significant impacts on the implementation of many human rights, \\nincluding privacy, health, education, freedom of movement, freedom of assembly \\nand association, and freedom of expression.80 Bachelet noted that inferences and \\npredictions about individuals made by AI may profoundly affect not only those \\nindividuals’ privacy but also their autonomy, and may raise issues regarding \\nfreedom of thought and opinion, freedom of expression, the right to a fair trial \\nand\\xa0other related rights.81 Uses of faulty data may result in bias or discrimination,82 \\nas may faulty AI tools. Uses of AI in the criminal justice process may lead to \\nviolations of the rights to privacy, fair trial, freedom from arbitrary arrest and \\ndetention and even the right to life.83\\nWhile all rights are relevant, this section provides an overview of key rights that \\nshould form the basis of any safeguards for AI development.\\n75  Fjeld, J. et al. (2020), Principled Artificial Intelligence; Hagendorff (2020), ‘The Ethics of AI Ethics’; Floridi \\nand\\xa0Cowls (2019), ‘A Unified Framework of Five Principles for AI in Society’.\\n76  Floridi and Cowls (2019), ‘A Unified Framework of Five Principles for AI in Society’.\\n77  Hagendorff (2020), ‘The Ethics of AI Ethics’; Montreal AI Ethics Institute (2021), ‘The Proliferation of AI \\nEthics\\xa0Principles: What’s Next?’, https://montrealethics.ai/the-proliferation-of-ai-ethics-principles-whats-next.\\n78  UN Special Rapporteur on Extreme Poverty (2019), Report on use of digital technologies in the welfare state, \\nA/74/493, https://digitallibrary.un.org/record/3834146?ln=en#record-files-collapse-header. See also Yeung, \\nHowes and Pogrebna (2019), ‘AI Governance by Human Rights-Centred Design, Deliberation and Oversight’, p.\\xa03: \\n‘Yet the vagueness and elasticity of the scope and content of “AI ethics” has meant that it currently operates as \\nan empty vessel into which anyone (including the tech industry, and the so-called Digital Titans) can pour their \\npreferred “ethical” content.’\\n79  Work is under way at the Council of Europe for a legal instrument on AI, by reference to the Council of Europe’s \\nstandards on human rights, democracy and the rule of law. See Council of Europe (2022), ‘Inaugural Meeting \\nof\\xa0the Committee on Artificial Intelligence (CAI)’.\\n80 United Nations High Commissioner for Human Rights (2021), The Right to Privacy in the Digital Age, \\nA/HRC/48/31, https://documents-dds-ny.un.org/doc/UNDOC/GEN/G21/249/21/PDF/G2124921.pdf.\\n81 Ibid., para. 17.\\n82  Ibid., para. 19.\\n83  Ibid., para. 24.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 24, 'page_label': '23'}, page_content='AI governance and human rights\\nResetting the relationship\\n23 Chatham House\\n4.2.1 Privacy\\nThe challenges presented by AI\\nAI is having a huge impact on privacy and data protection. Far more information \\nabout individuals is collated now than ever before, increasing the potential \\nfor exploitation. A new equilibrium is needed between the value of personal \\ndata for AI on the one hand and personal privacy on the other. There are two \\nparallel challenges to overcome: (i) AI is causing, and contributing to, significant \\nbreaches of privacy and data protection; and (ii) use of extensive personal data \\nin AI decision-making and influencing is contributing to an accretion of state \\nand\\xa0corporate power.\\nExamples of breaches of privacy and data protection include:\\n — AI’s requirement for data sets may create an incentive for companies and \\npublic institutions to share personal data in breach of privacy requirements. \\nFor example, in 2017, a UK health trust was found to have shared the data of \\n1.6\\xa0million patients with Google’s DeepMind, without adequate consent from \\nthe patients concerned.84\\n — AI may facilitate the harvesting of personal data without adequate consent. \\nBetween 2013 and 2018, Cambridge Analytica collated personal data of up \\nto 87 million Facebook users without their knowledge or consent for use \\nin\\xa0political advertising.85\\n — The practice of using publicly available images to create AI facial recognition \\ndatabases raises major privacy concerns. Projects such as Exposing.ai aim to \\nhighlight the privacy implications of extant large facial recognition datasets.86 \\nSome large companies, including Microsoft and Facebook, have closed their \\nfacial recognition operations.87 Clearview AI’s provision of facial recognition \\ntechnology for law enforcement purposes\\xa0– via a database of 10 billion images \\ngleaned from the internet\\xa0– has been found in breach of privacy laws in several \\ncountries, including Australia, Canada, France and the UK.88\\n — AI lends itself to bulk interception and assessment of online communications. \\nIn\\xa02021, the ECtHR found that the UK’s former regime for bulk interception, \\nusing digital and automated methods, lacked necessary end-to-end safeguards \\nfor compliance with privacy rights.89\\n84 BBC News (2017), ‘Google DeepMind NHS app test broke UK privacy law’, 3 July 2017, https://www.bbc.co.uk/ \\nnews/technology-40483202.\\n85 Information Commissioner’s Office (2018), Investigation into the use of data analytics in political campaigns, \\nhttps://ico.org.uk/media/action-weve-taken/2260271/investigation-into-the-use-of-data-analytics-in-political-\\ncampaigns-final-20181105.pdf.\\n86 Harvey, A. and LaPlace, J. (2021), ‘Exposing.ai’, 1 January 2021, https://exposing.ai (accessed 12 Sep. 2022).\\n87  Microsoft withdrew its MS Celeb database in 2019: Computing News (2019), ‘Microsoft withdraws facial \\nrecognition database of 100,000 people’, 6 June 2019, https://www.computing.co.uk/news/3076968/microsoft-\\nwithdraws-facial-recognition-database-of-100-000-people. Meta announced in November 2021 that it was \\nshutting down Facebook’s facial recognition system: Meta (2021), ‘An update on our use of face recognition’, \\n2\\xa0November 2021, https://about.fb.com/news/2021/11/update-on-use-of-face-recognition.\\n88 Lomas, N. (2021), ‘France latest to slap Clearview AI with order to delete data’, TechCrunch, 16\\xa0December\\xa02021, \\nhttps://techcrunch.com/2021/12/16/clearview-gdpr-breaches-france.\\n89  Big Brother Watch and others v UK (ECtHR App no 58170/13).'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 25, 'page_label': '24'}, page_content='AI governance and human rights\\nResetting the relationship\\n24  Chatham House\\n — ‘Smart’ devices, such as fridges and vehicles, may not only collate data on users \\nto improve performance, but also to sell to third parties. If not properly secured, \\nsuch devices may also expose users to surveillance by hackers. In 2017, for \\nexample, the German authorities withdrew the ‘My Friend Cayla’ doll from sale \\nover fears that children’s conversations could be listened to via Bluetooth.90\\nAI impacts privacy in several ways. First, its thirst for data creates compelling \\nreasons for increased collection and sharing of data, including personal data, \\nwith the aim of improving the technology’s operation. Second, AI may be used \\nto collate data, including that of a sensitive, personal nature, for purposes of \\nsurveillance. Third, AI may be used to develop profiles of individuals that are then \\nthe basis of decisions on matters fundamental to their lives\\xa0– from healthcare to \\nsocial benefits, to employment to insurance provision. As part of this profiling, \\nAI may infer further, potentially sensitive information about individuals without \\ntheir knowledge or consent, such as conclusions on their sexual orientation, \\nrelationship status or health conditions. Finally, AI may make use of personal data \\nto micro-target advertising and political messaging, to manipulate and exploit \\nindividual vulnerabilities, or even to facilitate crimes such as identity theft.\\nInternational human rights law\\nThe human right to privacy currently entails that any processing of personal data \\nshould be fair, lawful and transparent, based on free consent or another legitimate \\nbasis laid down in law. Data should only be held for a limited period and for \\nspecific purposes, with those purposes not to be lightly changed. Data should \\nbe\\xa0held securely, and sensitive personal data should enjoy heightened protection. \\nPrivacy entails that individuals should know that their personal data has been \\nretained and processed, and that they have a right both to rectify or erase their \\npersonal data and to limit how it is used. Privacy further entails that individuals \\nmust not\\xa0be\\xa0exposed to mass surveillance or unlimited profiling. Personal data \\nshould\\xa0not be\\xa0transferred, particularly overseas, unless similar standards will \\nbe\\xa0upheld by the recipient of that data.91\\nHuman rights law is already the widely accepted basis for most legislation \\nprotecting privacy. The EU’s General Data Protection Regulation (GDPR) is founded \\non the right to protection of personal data in Article 8(1) of the EU Charter of \\nFundamental Rights\\xa0– this is an aspect of the right to privacy in earlier human\\xa0rights \\ntreaties. Privacy and data protection is one of the European Commission’s Seven \\nPrinciples for Trustworthy AI, while most statements of AI principles include \\na\\xa0commitment to privacy.92\\nApplication of human rights law to the challenges of AI\\nWith the development of AI, it is becoming apparent that changes need \\nto\\xa0be\\xa0made\\xa0to the contours of the right to privacy.\\n90 BBC News (2017), ‘German parents told to destroy Cayla toys over hacking fears’, 17 February 2017, \\nhttps://www.bbc.co.uk/news/world-europe-39002142.\\n91 United Nations High Commissioner for Human Rights (2018), The Right to Privacy in the Digital Age, \\nA/HRC/39/29, https://www.ohchr.org/en/documents/reports/ahrc3929-right-privacy-digital-age-report-\\nunited-nations-high-commissioner-human.\\n92  35 of the 36 statements of AI principles reviewed by Fjeld et al. included this commitment: Fjeld et al. (2020), \\nPrincipled Artificial Intelligence.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 26, 'page_label': '25'}, page_content='AI governance and human rights\\nResetting the relationship\\n25 Chatham House\\nThere is growing awareness of the tension between privacy’s requirement to restrict \\nflows of personal data on the one hand, and economic and commercial arguments \\nin favour of free flow on the other. There are many sound reasons for improved \\ndata accessibility: fostering developments in AI innovation; facilitating increased \\nuse of AI; and preventing data restrictions from distorting markets or acting \\nas\\xa0a\\xa0barrier to competition and innovation.\\nPrivacy should not be viewed as static: it is flexible enough to adapt and develop, \\nthrough new legislation or through judicial interpretation, in light of rapidly \\nchanging technological and social conditions. Individual privacy remains \\nvital to ensuring that individuals do not live in a surveillance state, and that \\nindividuals retain control over their own data and by whom and how it is seen \\nand used. This is critical at a time when the value of privacy is being steadily \\nand\\xa0unconsciously diluted.\\nThe human right to privacy should be used to resolve competing interests in \\nan\\xa0AI-dominated world\\xa0– whether those interests are commercial, individual or \\ntechnical. For example, rather than privacy impeding the transfer of anonymized \\ndata for use in AI data sets, the balancing between rights and interests allowed \\nby\\xa0the human right to privacy could be used to set appropriate limits on \\ndata-profiling and\\xa0micro-targeting.\\n4.2.2 Equality: discrimination and bias\\nThe challenges presented by AI\\nBecause AI generally operates by applying rules to the treatment of people, rather \\nthan by assessing each individual on their merits, it carries significant risks of \\nembedding discrimination, as the rules that it applies may distinguish between \\npeople, directly or indirectly, by reference to protected characteristics. Indeed, \\nexamples of such bias and discrimination in the use of AI abound:\\n — In 2015, researchers found that female job seekers were much less likely than \\nmales to be shown adverts for highly paid jobs on Google.93\\n — In 2016, researchers found that an algorithm used to determine offenders’ risk \\nof recidivism often overstated the risk that black defendants would re-offend, \\nand understated the risk of reoffending by white defendants.94\\n93 Gibbs, S. (2015), ‘Women less likely to be shown ads for high-paid jobs on Google, study shows’, Guardian, \\n8 July 2015, https://www.theguardian.com/technology/2015/jul/08/women-less-likely-ads-high-paid-\\njobs-google-study.\\n94 Larson, J. et al. (2016), ‘How We Analyzed the COMPAS Recidivism Algorithm’, ProPublica, 23 May 2016, \\nhttps://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm; see also State of \\nWisconsin v Eric L Loomis (2016) WI 68, 881 N.W.2d 749.\\nPrivacy should not be viewed as static: it is\\xa0flexible \\nenough to adapt and develop […] in light of rapidly \\nchanging technological and\\xa0social conditions.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 27, 'page_label': '26'}, page_content='AI governance and human rights\\nResetting the relationship\\n26 Chatham House\\n — In 2017, Amazon abandoned its automated recruitment platform, built on \\nobserving patterns in applicant CVs over the previous years, having been unable \\nto prevent it from discriminating on the basis of gender or from making other \\ninappropriate recommendations.95\\n — In 2018, Immigration New Zealand suspended its use of data-profiling, which \\nhad been predicting likely healthcare costs and criminality of immigrants on the \\nbasis of demographics including age, gender and ethnicity.96\\n — In 2019, researchers found that AI widely used to allocate healthcare in US \\nhospitals was systematically discriminating against black people, by referring \\nthem on to specialized care programmes less frequently than white people. \\nThe\\xa0algorithm was predicting future healthcare costs as a proxy for illness, using \\npast costs for individuals in similar situations. This failed to take account of the \\nfact that less money had been spent historically on caring for black patients.97\\n — In 2020, the Austrian public employment service (AMS) began using an \\nalgorithm that enabled it to classify jobseekers according to their likelihood of \\nsuccessful re-employment. The algorithm has been criticized for discriminating \\non the basis of gender, disability and other factors, and for intersectional \\ndiscrimination.98 AMS has suspended use of the algorithm pending the \\noutcome\\xa0of legal challenges.99\\nAI makes it difficult to assess whether discrimination has occurred. An individual \\nusually becomes aware of discrimination by comparing their treatment, or its \\noutcome, with that of other people. But when complex AI is used to make each \\nindividual a personalized offer (for example, on social security payments) or \\ndecision (for example, on school or college entry), that individual may have no \\nmeans of knowing what criteria were used, nor how their result differs from others. \\nConsequently, individuals may not know, or have any accessible way of finding out, \\nwhether they have been disadvantaged or how.100\\nAI developers have learned from past problems and gone to considerable lengths \\nto devise systems that promote equality as much, or more, than human decision-\\nmaking.101 Nonetheless, several features of AI systems may cause them to make \\nbiased decisions. First, AI systems rely on training data to train the decision-making \\nalgorithm. Any imbalance or bias in that training data is likely then to be replicated \\nand become exaggerated in the AI system. If the training data is taken from the real \\nworld, rather than artificially generated, AI is likely to replicate and exaggerate any \\n95 Dastin, J. (2018), ‘Amazon scraps secret AI recruiting tool that showed bias against women’, Reuters, \\n11\\xa0October 2018, https://www.reuters.com/article/us-amazon-com-jobs-automation-insight-idUSKCN1MK08G.\\n96 Bonnett, G. (2018), ‘Immigration NZ using data system to predict likely troublemakers’, RNZ News, \\n5\\xa0April\\xa02018, https://www.rnz.co.nz/news/national/354135/immigration-nz-using-data-system-to-predict-\\nlikely-troublemakers.\\n97  Obermeyer, Z. et al. (2019), ‘Dissecting racial bias in an algorithm used to manage the health of populations’, \\nScience, 25 October 2019, 366(6464), pp. 447–553, https://doi.org/10.1126/science.aax2342.\\n98 Allhutter, D. et al. (2020), ‘Algorithmic profiling of job seekers in Austria: how austerity politics are made \\neffective’, Frontiers in Big Data, 21 February 2020, https://doi.org/10.3389/fdata.2020.00005.\\n99 Der Standard (2022), ‘“Zum In-die-Tonne-Treten”: Neue Kritik am AMS-Algorithmus’ [“To Be Thrown In The \\nBin”: New Criticism of the AMS Algorithm’], 28 April 2022, https://www.derstandard.at/story/2000135277980/\\nneuerliche-kritik-am-ams-algorithmus-zum-in-die-tonne-treten.\\n100 Obermeyer et al. (2019), ‘Dissecting racial bias in an algorithm used to manage the health of populations’.\\n101 For example, HireVue, an AI recruitment tool used by some large companies, claims to ‘[i]ncrease diversity \\nand mitigate bias’ by finding a wider candidate pool, evaluating objectively and consistently, and helping to avoid \\nunconscious bias. See HireVue (2022), https://www.hirevue.com/employment-diversity-bias.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 28, 'page_label': '27'}, page_content='AI governance and human rights\\nResetting the relationship\\n27 Chatham House\\nbias already present in society. Second, AI systems rely on the instructions given \\nto them, as well as their own self-learning. Any discrimination or bias deployed \\nby the designer risks being replicated and exaggerated in the AI system. Third, \\nAI\\xa0systems operate within a context: an AI system will lead to bias if it is deployed \\nwithin the context of social conditions that undermine enjoyment of rights by \\ncertain groups.102 Without human involvement, AI is currently unable to replicate \\ncontextual notions of fairness.\\nInternational human rights law\\nHuman rights law provides standards of equality and non-discrimination by which \\nto assess AI. It requires that all individuals’ rights be respected and ensured ‘without \\ndistinction of any kind, such as race, colour, sex, language, religion, political or other \\nopinion, national or social origin, property, birth or other status’.103 The law entails \\nprohibitions against not just direct discrimination (i.e.\\xa0treating people differently \\non prohibited grounds), but indirect discrimination (i.e.\\xa0treating people the same, \\nbut in a way that puts people from a protected group at a disadvantage without \\nan objective justification) and structural discrimination (i.e.\\xa0creating structural \\nconditions in society that prevent all groups from accessing the same opportunities). \\nAcknowledging that equality does not always mean treating everyone the \\nsame, discrimination law provides structured tests for assessing and preventing \\nunlawful treatment.\\nThis ban on discrimination has formed the basis for well-developed understandings \\nof, and jurisprudence on, non-discrimination in both the public and private sectors. \\nHuman rights law obliges governments both to ensure there is no discrimination \\nin public sector decision-making and to protect individuals against discrimination \\nin the private sector. Human rights law does not forbid differential treatment that \\nstems from factors other than protected characteristics, but such treatment must \\nmeet standards of fairness and due process in decision-making (see below).\\nApplication of human rights law to the challenges presented by AI\\nHuman rights practitioners are accustomed to considering the prohibition of \\ndiscrimination by reference to well-established tests, and to resolving tensions \\nbetween non-discrimination and other rights like freedom of speech. Adopting the \\nstandards that are well established and internationally accepted in human rights \\nlaw minimizes the need for fresh debates on highly contested concepts in ethics \\n(what is ‘justice’? what is ‘fairness’?).104 Further, it avoids the risk of confusion \\nfrom the imposition of parallel, non-human rights standards of discrimination \\nspecifically in the field of AI.\\n102  Wachter, S., Mittelstadt, B. and Russell, C. (2020), ‘Why Fairness Cannot be Automated: Bridging the \\nGap between EU Non-Discrimination Law and AI’, Computer Law & Security Review, 41(2021): 105567, \\nhttps://ssrn.com/abstract=3547922.\\n103 International Covenant on Civil and Political Rights, Article 2(1). Some non-discrimination laws forbid \\ndiscrimination in all circumstances, rather than merely in the implementation of rights: see Protocol 12 to the \\nEuropean Convention on Human Rights and Articles 20 and 21 of the European Charter of Fundamental Rights.\\n104 For example on justice, Floridi, L. et al. (2018), ‘AI4People\\xa0– An Ethical Framework for a Good AI Society’, \\nMinds and Machines, 28, pp. 689–707, https://doi.org/10.1007/s11023-018-9482-5; Bartneck, C. et\\xa0al. (2021), \\nAn Introduction to Ethics in Robotics and AI, Springer Briefs in Ethics, p. 33.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 29, 'page_label': '28'}, page_content='AI governance and human rights\\nResetting the relationship\\n28 Chatham House\\nInternational human rights law does not simply require governments to ban \\ndiscrimination in AI. As the UN special rapporteur on contemporary forms \\nof\\xa0racism has observed, human rights law also requires governments to deploy \\na\\xa0structural understanding of discrimination risks from AI. To combat the potential \\nfor bias, the tech sector would benefit from more diversity among AI developers, \\nmore guidance on bias detection and mitigation and the collection and use of data \\nto monitor for bias, and more leadership by example from the public sector.105 \\nAI\\xa0developers and implementers must consider holistically the impact of all \\nalgorithms on individuals and groups, rather than merely the impact of each \\nalgorithm on each right separately.106 Algorithms should be reviewed regularly \\nto\\xa0ensure that their results are not discriminatory, even though obtaining data \\nfor\\xa0comparison purposes may be challenging.107 Vigilance is needed to ensure \\nthat\\xa0other factors are not used as proxies for protected characteristics\\xa0– for \\nexample, that postcode is not used as a proxy for ethnic origin.\\nLegislators, regulators (such as the UK’s Equality and Human Rights \\nCommission)\\xa0and courts need to consider the methodology for ensuring and \\noverseeing compliance with the right to non-discrimination with regard to AI. \\nNew tools may be necessary to detect discrimination, as AI systems operate \\ndifferently and are generally more opaque than non-AI decision-making processes. \\nTo be able to review the operation of AI effectively, the law and the courts may \\nhave to take more account of statistical method as well as context, while also \\nadopting more standardized thresholds where possible and appropriate.108 \\nIn parallel, AI developers need to ensure that automated decision-making \\nmatches its human equivalent by developing capacity to take account of a rich \\ncomplexity of factors relevant to the circumstances of the individual. Legal and \\ntechnical communities should work together to find adequate ways of reducing \\ndiscrimination in algorithmic systems, including by embedding transparency \\nand\\xa0contextual approaches.\\n105 Centre for Data Ethics and Innovation (2020), Review into Bias in Algorithmic Decision-Making, \\nhttps://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/957259/\\nReview_into_bias_in_algorithmic_decision-making.pdf, pp. 9–10.\\n106 McGregor, L., Murray, D. and Ng, V. (2019), ‘International Human Rights Law as a Framework for Algorithmic \\nAccountability’, International & Comparative Law Quarterly, 68(2), April 2019, https://doi.org/10.1017/\\nS0020589319000046, p. 326.\\n107  Centre for Data Ethics and Innovation (2020), Review into Bias in Algorithmic Decision-Making, pp. 9–10.\\n108 Wachter, S., Mittelstadt, B. and Russell, C. (2020), ‘Why Fairness Cannot be Automated: Bridging the \\nGap between EU Non-Discrimination Law and AI’, Computer Law & Security Review, 41 (2021): 105567, \\nhttps://ssrn.com/abstract=3547922.\\nAdopting well-established and internationally \\naccepted standards in human rights law \\nminimizes the need for fresh debates \\non\\xa0highly\\xa0contested concepts in ethics.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 30, 'page_label': '29'}, page_content='AI governance and human rights\\nResetting the relationship\\n29 Chatham House\\n4.2.3 Autonomy\\nThe challenges presented by AI\\nAI poses two principal risks to autonomy. First, empathic AI109 is developing \\nthe capacity to recognize and measure human emotion as expressed through \\nbehaviour, expressions, body language, voice and so on.110 Second, it is increasingly \\nable to react to and simulate human emotion, with the aim of generating empathy \\nfrom its human users. Empathic AI is beginning to appear in a multitude of devices \\nand settings, from games and mobile phones, to cars, homes and toys, and across \\nindustries including education, insurance and retail. Research is ongoing as to how \\nAI can monitor the mental111 and physical health of employees.112\\nSome empathic AI has clear benefits. From 2022, EU law requires that new vehicles \\nincorporate telemetrics for the detection of drowsiness and distraction in drivers.113 \\nBesides the obvious safety benefits for drivers and operators of machinery, empathic \\nAI offers assistive potential (particularly for disabled people) and prospects for \\nimproving mental health. Other possible enhancements to daily lives range from \\nrecommendations for cures to ailments to\\xa0curated music-streaming.114\\nHowever, empathic AI also carries major risks. The science of emotion detection \\nand recognition is still in development, meaning that, at present, any chosen \\nlabelling or scoring of emotion is neither definitive nor necessarily accurate. Aside \\nfrom these concerns, empathic AI also raises significant risks of both surveillance \\nand manipulation. The use of emotion recognition technology for surveillance \\nis likely to breach the right to privacy and other rights\\xa0– for example, when used \\nto monitor employee or student engagement or to identify criminal suspects.115 \\nMore broadly, monitoring of emotion, as of all behaviour, is likely to influence how \\npeople behave\\xa0– potentially having a chilling effect on the freedoms of expression, \\nassociation and assembly, and even of thought.116 This is particularly the case \\nwhere access to rights and benefits is made contingent on an individual meeting \\nstandards of behaviour, as for instance in China’s ‘social credit’ system.117\\nRegarding manipulation, empathic AI blurs the line between recommendation \\nand\\xa0direction. Algorithms may influence individuals’ emotions and thoughts, \\n109 Also known as ‘emotion AI’, ‘emotional AI’, and ‘affective computing’ (a term coined by Rosalind Picard in\\xa0her \\n1995 book on the topic). One example is sentiment analysis, which entails the assessment of text (such as\\xa0customer \\nfeedback and comments) and, increasingly, of images (of people, objects or scenes) for\\xa0emotional tone.\\n110 For an overview and research in this field, see Emotional AI Lab (undated), www.emotionalai.org.\\n111 For example, Lewis, R. et al. (2022), ‘Can a Recommender System Support Treatment Personalisation \\nin\\xa0Digital Mental Health Therapy?’, MIT Media Lab, 21 April 2022, https://www.media.mit.edu/publications/\\nrecommender-system-treatment-personalisation-in-digital-mental-health.\\n112 Whelan, E. et al. (2018), ‘How Emotion-Sensing Technology Can Reshape the Workplace’, MIT Sloan \\nManagement Review, 5 February 2018, https://sloanreview.mit.edu/article/how-emotion-sensing-technology-\\ncan-reshape-the-workplace.\\n113 General Safety Regulation, Regulation (EU) 2019/2144 of the European Parliament and of the Council, \\nhttps://eur-lex.europa.eu/eli/reg/2019/2144/oj.\\n114 For a discussion of the potential of empathic AI, see McStay, A. (2018), Emotional AI: The Rise of Empathic \\nMedia, London: SAGE Publications Ltd, chap. 1.\\n115 Article 19 (2021), Emotional Entanglement: China’s emotion recognition market and its implications for human \\nrights, January 2021, https://www.article19.org/wp-content/uploads/2021/01/ER-Tech-China-Report.pdf.\\n116 UN Special Rapporteur on Freedom of Religion or Belief (2021), Freedom of Thought, A/76/380 \\n(October\\xa02021), https://undocs.org/Home/Mobile?FinalSymbol=A%2F76%2F380&Language=E&DeviceType \\n=Desktop&LangRequested=False, para. 54.\\n117 See the discussion of China’s social credit system in Taylor, E., Jones, K. and Caeiro, C. (2022), ‘Technical Standards \\nand Human Rights: The Case of New IP’, in Sabatini, C. (2022), Reclaiming human rights in a changing world order, \\nWashington, DC and London: Brookings Institution Press and Royal Institute of International Affairs, pp. 185–215.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 31, 'page_label': '30'}, page_content='AI governance and human rights\\nResetting the relationship\\n30 Chatham House\\nand\\xa0the decisions they make, without them being aware.118 The distinction\\xa0between \\nacceptable influence and unacceptable manipulation has long been blurred. At one \\nend of the spectrum, nudge tactics such as tailored advertising and\\xa0promotional \\nsubscriptions are commonly accepted as marketing tools. At the other, \\nmisrepresentation and the use of fake reviews are considered unacceptable and \\nattract legal consequences. Between those extremes, the boundaries are unclear.\\nRetail and other commercial sectors are increasingly harnessing empathic AI \\ntechnology. For example, just as advertising has long sought to take advantage \\nof\\xa0mood and feeling to promote sales, micro-targeting could be taken a step \\nfurther by including emotion detection as one of its parameters, with the aim of \\npersuading an individual to book a holiday or sign up for a therapy class, among \\nother things. There are currently no parameters by which to assess the acceptable \\nlimits of influence, even as persuasive tactics edge further towards manipulation.\\nIn social media, too, AI offers potential for emotional manipulation, not least \\nwhen\\xa0it comes to politics. In particular, the harnessing of empathic AI exacerbates \\nthe threat posed by campaigns of political disinformation and manipulation. \\nAI\\xa0use to harness emotion for political ends has already been widely reported. \\nThis includes the deployment of fake or distorted material, often micro-targeted, \\nto simulate empathy and inflame emotions.119 Regulation and other policies are \\nnow being targeted at extreme forms of online influence,120 but the parameters \\nof\\xa0acceptable behaviour by political actors remain unclear.\\nEmpathic AI could have major impacts on all aspects of life. Imagine, for example, \\ntechnology that alters children’s emotional development, or that tailors career \\nadvice to young people in an emotionally empathic manner that appears to expand \\nbut actually has the effect of limiting choice. Vulnerable groups, including minors \\nand adults with disabilities, are particularly at risk. Researchers of very large \\nlanguage models have argued for greater consideration of the risks of human \\nmimicry and abuse of empathy they create.121\\nThe draft EU Artificial Intelligence Act would ban the clearest potential \\nfor\\xa0manipulation inherent in AI by prohibiting AI that deploys subliminal \\ntechniques\\xa0to\\xa0distort people’s behaviour in a manner that may cause them \\n‘physical or psychological harm’.122 The Act would also limit the uses of individual \\n‘trustworthiness’ profiling. As most empathic AI involves the use of biometric \\n118 Council of Europe (2019), Declaration by the Committee of Ministers on the Manipulative Capabilities \\nof\\xa0Algorithmic Processes, Decl(13/02/2019)1.\\n119 Jones (2019), Online Disinformation and Political Discourse: Applying a Human Rights Framework.\\n120 For example, European Democracy Action Plan and related legislation: Communication from the Commission \\nto the European Parliament, the Council, the European Economic and Social Committee and the Committee of \\nthe Regions (2020), On the European Democracy Action Plan, COM/2020/790 final https://ec.europa.eu/info/\\nstrategy/priorities-2019-2024/new-push-european-democracy/european-democracy-action-plan_en#what-is-\\nthe-european-democracy-action-plan. In the UK, the National Security Bill, clauses 13 and 14 would criminalize \\nforeign interference, while the government has announced its intention to make foreign interference a prioritized \\noffence for the purposes of\\xa0the\\xa0Online Safety Bill.\\n121 Bender, E. et al. (2021), ‘On the Dangers of Stochastic Parrots: Can Language Models be Too Big?’, event, \\nFAccT 2021, 3–10 March 2021, https://dl.acm.org/doi/pdf/10.1145/3442188.3445922; Bender, E. (2022), \\n‘Human-like Programs Abuse Our Empathy\\xa0– even Google Engineers Aren’t Immune’, Guardian, 14 June 2022, \\nhttps://www.theguardian.com/commentisfree/2022/jun/14/human-like-programs-abuse-our-empathy-even-\\ngoogle-engineers-arent-immune.\\n122  Proposal for a Regulation of the European Parliament and of the Council laying down harmonised rules \\non\\xa0artificial intelligence (Artificial Intelligence Act) and amending certain Union legislative acts, COM/2021/206 \\nhttps://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX:52021PC0206, Article 5.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 32, 'page_label': '31'}, page_content='AI governance and human rights\\nResetting the relationship\\n31 Chatham House\\ndata,\\xa0it is likely to be subject to the Act’s enhanced scrutiny for ‘high-risk’ AI. \\nHowever, empathic AI that operates on an anonymous basis may not be covered.\\nInternational human rights law\\nAs well as privacy, human rights law protects autonomy. It protects the right to \\nfreedom of thought and the right to hold opinions without interference, as well \\nas the better-known and -understood rights to freedom of expression, freedom \\nof assembly and association, and freedom of conscience and religion. The EU \\nCharter of Fundamental Rights also protects the right to ‘mental integrity’. Prior \\nto\\xa0recent technological developments, the rights to freedom of thought and opinion \\nwere underexplored. Further guidance is now emerging: for example, the UN \\nspecial rapporteur on freedom of religion or belief has recently issued guidance \\non\\xa0freedom of thought.123\\nChildren’s rights merit special consideration in this area. In addition to questions \\nover privacy and the ability of minors to give consent when providing personal \\ndata, the UN Committee on the Rights of the Child has called for practices that \\nrely on neuromarketing and emotional analytics to be prohibited from direct \\nor\\xa0indirect engagement with children,124 and for states to prohibit manipulation \\nor interference with the child’s right to freedom of thought and belief through \\nemotional analytics and interference.125\\nApplication of human rights law to the challenges presented by AI\\nThere are considerable concerns about the extent to which emotion recognition, \\ncapture and simulation may infringe human rights, in ways that are not necessary \\nor proportionate to perceived benefits.\\nAt present, challenges to autonomy are generally viewed through the prism \\nof\\xa0privacy and data protection. While this enables consideration of the impacts \\nof surveillance, it is not a sufficient framework by which to consider issues of \\nmanipulation. Empathic AI can still be effective without capturing personal data\\xa0– \\nexamples include billboards that adapt their advertising according to the reactions \\nof people walking past, stores that adapt their advertising and marketing after \\ncapturing shoppers’ reactions in real time or bots that reflect unnamed users’ \\nemotions in order to influence their decision-making.\\nInitiatives to set limits on simulated empathy, such as the technical standard under \\ndevelopment by the IEEE,126 ought to take account of the absolute nature of the \\nrights to freedom of opinion and freedom of thought, as well as the right to mental \\nintegrity and the rights of the child. Further legislative and judicial consideration \\nis\\xa0needed to establish precisely what constraints human rights law imposes on \\npotentially manipulative uses of AI, and precisely what safeguards it imposes \\nto\\xa0prevent the erosion of autonomy.\\n123  UN Special Rapporteur on Freedom of Religion or Belief (2021), Freedom of Thought, A/76/380 \\n(October\\xa02021), https://undocs.org/Home/Mobile?FinalSymbol=A%2F76%2F380&Language=E&Device \\nType=Desktop&LangRequested=False, paras 68–72.\\n124  UN Committee on the Rights of the Child (2021), General Comment No. 25 on children’s rights in relation \\nto\\xa0the digital environment, CRC/C/GC/25, 2 March 2021, para. 42.\\n125  Ibid., para. 62.\\n126 IEEE 7000-P7014, Empathic Technology Working Group on a Standard for ethical considerations in emulated \\nempathy in autonomous and intelligent systems.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 33, 'page_label': '32'}, page_content='AI governance and human rights\\nResetting the relationship\\n32 Chatham House\\nMeanwhile, some are reaching their own conclusions on empathic AI. For \\nexample, a coalition of prominent civil society organizations has argued that the \\nEU’s Artificial Intelligence Act should prohibit all emotion recognition AI, subject \\nto limited exceptions for health, research and assistive technologies.127 In June \\n2022, Microsoft announced that it would phase out emotion recognition from \\nits Azure Face API facial recognition services. In that announcement, Microsoft \\nnoted the lack of scientific consensus on the definition of ‘emotions’, challenges \\nof generalizations across diverse populations, and privacy concerns as well as \\nawareness of potential misuse of the technology for stereotyping, discrimination \\nor\\xa0unfair denial of services.128\\n4.2.4 Equality: implementation of economic and social rights\\nInternational human rights law protects a wide range of economic and social \\nrights, and provides an anchor for sustainable development.129 Just as AI offers \\nopportunities to achieve implementation of the SDGs, so it offers significant \\npotential to improve the implementation of rights such as those to education, \\nhealth, social security and work. Equality is key to achieving this potential: not \\njust through the avoidance of discrimination, but through AI that benefits all \\ncommunities and through the provision of equal opportunity for all in accessing \\nthe benefits. Failure to realize such opportunities risks not only entrenching but \\nexacerbating current social divisions.\\nIdeally, such provision would begin with research into AI technologies that \\nwould\\xa0help to implement the SDGs, and funding for the development and rollout \\nof those technologies. The challenges are to incentivize developments that benefit \\nall communities, as well as those that are most profitable; and to ensure that no \\nAI\\xa0systems operate to the detriment of vulnerable communities.\\n4.2.5 Fairness and due process in decision-making\\nAI decision-making brings a risk that the ‘computer says no’ in respect of significant \\nlife decisions, without possibility of review or challenge. Aside from discrimination, \\nthis also raises questions as to fairness of process and quality of decision-making in \\nAI systems. It concerns both whether the use made of AI to reach the decision was \\nfair, and whether AI reached or contributed to a fair decision in the specific case\\xa0– \\nand if not, what the recourse might be.\\nIn making decisions, AI may segment people by reference to a wide range \\nof\\xa0factors\\xa0and without consideration as to whether segmentation is appropriate \\nin\\xa0the particular case. These factors may be unrelated to the decision in question, \\n127 Joint Civil Society Amendments to the Artificial Intelligence Act (2022), Prohibit Emotion Recognition in \\nthe Artificial Intelligence Act, May 2022, https://www.accessnow.org/cms/assets/uploads/2022/05/Prohibit-\\nemotion-recognition-in-the-Artificial-Intelligence-Act.pdf.\\n128 Bird, S. (2022), ‘Responsible AI investments and safeguards for facial recognition’, Microsoft Azure blog, \\n21 June 2022, https://azure.microsoft.com/en-us/blog/responsible-ai-investments-and-safeguards-for-\\nfacial-recognition.\\n129 UN Office of the High Commissioner on Human Rights (undated), ‘OHCHR and the 2030 Agenda \\nfor\\xa0Sustainable Development’, https://www.ohchr.org/en/sdgs.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 34, 'page_label': '33'}, page_content='AI governance and human rights\\nResetting the relationship\\n33 Chatham House\\nbut decisions that treat some people unfairly in comparison to others may still result. \\nFor example, if a travel insurance provider were to double the premiums offered \\nto people who had opted out of receiving unsolicited marketing material, it\\xa0would \\nnot be discriminating on the basis of a protected characteristic. Its decision-making \\nprocess would however be biased against those who have opted out.\\nWhere an individual’s human rights are affected by a decision made by \\na\\xa0public authority, they should be able to seek remedy130 and will usually be \\nable\\xa0to\\xa0challenge the decision in public law\\xa0– for example, by way of judicial review. \\nDecision-making processes need to be sufficiently transparent to enable such \\nreview. Individuals should know who the decision-maker is, the factors on which \\nthe decision is made and be able to verify the accuracy of any personal data used \\nin the process. There should be adequate human involvement or oversight\\xa0– \\nwhile acknowledging that human involvement may not be essential in every \\ncase\\xa0and\\xa0is\\xa0not necessarily a failsafe.131\\nInternational human rights law stipulates requirements for fairness in legal \\nproceedings. Public and private law bases of challenge to decisions commonly \\nreflect these requirements, and they can provide the basis for guidelines on \\nminimum standards for transparency, human control and accountability \\nthrough\\xa0possibility of review for all AI activities.\\n4.2.6 Other rights\\nAI, used in different contexts, may have serious implications for the full range \\nof human rights.\\nFor example, the use of AI for content curation and moderation in social media \\nmay affect the rights to freedom of expression and access to information. The use \\nof analytics to contribute to decisions on child safeguarding, meanwhile, may affect \\nthe right to family life.132 The use of facial recognition technology risks serious \\nimpact on the rights to freedom of assembly and association, and even on the right \\nto vote freely. In extreme cases\\xa0– for example, in weapons for military use\\xa0– AI risks \\nundermining the right to life and the right to integrity of the person if not closely \\ncircumscribed. In each of these areas, existing human rights can form the basis \\nfor\\xa0safeguards delimiting the appropriate scope of AI activity.\\n130 International Covenant on Civil and Political Rights, Art. 2(3); European Convention on Human Rights, Art. 13.\\n131 In the data protection context, there is pressure to change Article 22 of GDPR, which currently requires that \\ndecisions with legal or similarly significant effects for individuals, using their personal data, shall not be based \\nsolely on automated processing.\\n132  Anning, S. (2022), ‘The Interplay of Explicit and Tacit Knowledge With Automated Systems for Safeguarding \\nChildren’, techUK Industry Views blog, 21 March 2022, https://www.techuk.org/resource/the-interplay-of-\\nexplicit-and-tacit-knowledge-with-automated-systems-for-safeguarding-children.html.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 35, 'page_label': '34'}, page_content='34 Chatham House\\n05 \\nProcesses of AI \\ngovernance: the \\ncontribution of \\nhuman rights\\nRegulators and companies should follow human rights \\nprocess requirements as they devise and implement \\nAI\\xa0governance processes.\\n5.1 Processes: the landscape\\nThe processes that governments and companies should follow in order to meet \\nAI\\xa0governance standards are evolving rapidly.\\n5.1.1 Regulation\\nGovernments are increasingly considering cross-sectoral regulation of AI on \\nthe\\xa0basis that statutory obligations would help create a level playing field for safe \\nand ethical AI and bolster consumer trust, while mitigating the risk that pre-AI \\nregulation applies to AI in haphazard fashion.133 The EU is furthest along in this \\nprocess, with its draft Artificial Intelligence Act that would ban the highest-risk \\n133 In the UK, regulators have established the Digital Regulation Cooperation Forum to facilitate a joined-up approach \\nto technology regulation. In the US, the Federal Trade Commission has explained how it stands ready to\\xa0enforce \\nexisting legislation\\xa0– including the Federal Trade Commission Act, the Fair Credit Reporting Act, and the Equal \\nCredit Opportunity Act\\xa0– against bias or other unfair outcomes in automated decision-making. See Jillson,\\xa0E. (2021), \\n‘Aiming for truth, fairness, and equity in your company’s use of AI’, Federal Trade Commission Business Blog, 19 April \\n2021, https://www.ftc.gov/business-guidance/blog/2021/04/aiming-truth-fairness-equity-your-companys-use-ai.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 36, 'page_label': '35'}, page_content='AI governance and human rights\\nResetting the relationship\\n35 Chatham House\\nforms of AI and subject other ‘high risk’ AI to conformity assessments. In the US, \\nCongress is considering a draft Algorithmic Accountability Act.134 The British \\ngovernment, having considered the case for cross-cutting AI regulation, has \\nrecently announced plans for a non-statutory, context-specific approach that \\naims\\xa0to be pro-innovation and to focus primarily on high-risk concerns.135\\nWhile the British government, among others, has expressed concern that general \\nregulation of AI may stifle innovation, many researchers and specialists make \\nthe\\xa0opposite argument.136 Sector-specific regulation may not tackle AI risks that \\nstraddle sectors, such as the impact of AI in workplaces. Well-crafted regulation \\nshould only constrain undesirable activity, and should provide scope for \\nexperimentation without liability within its parameters, including for small \\ncompanies. Moreover, it is argued that responsible businesspeople would rather \\noperate in a marketplace regulated by high standards of conduct, with clear rules, \\na\\xa0level playing field and consequent consumer trust, than in an unregulated \\nenvironment in which they have to decide for themselves the limits of ethical \\nbehaviour. Most decision-makers in industry want to do things the right way \\nand\\xa0need the tools by which to do so.\\nIn addition to regulating AI itself, there are also calls for regulation to ensure that \\nrelated products are appropriately harnessed for the public good. For example, the \\nUK-based Ada Lovelace Institute has called for new legislation to govern biometric \\ntechnologies.137 Similarly, there is discussion of regulation of ‘digital twins’\\xa0– \\ni.e.\\xa0computer-generated digital facsimiles of physical objects or systems\\xa0– to ensure \\nthat the vast amounts of valuable data they generate is used for public good rather \\nthan for commercial exploitation or even public control.138\\nSome sector-specific laws are already being updated in light of AI’s expansion. \\nFor\\xa0example, the European Commission’s proposal to replace the current \\nConsumer Credit Directive aims to prohibit discrimination and ensure accuracy, \\ntransparency and use of appropriate data in creditworthiness assessments, with \\na right to human review of automated decisions.139 An analysis of legislation \\n134 H.R. 6580\\xa0– Algorithmic Accountability Act of 2022, https://www.congress.gov/bill/117th-congress/\\nhouse-bill/6580/text.\\n135 UK Government (2022), Establishing a pro-innovation approach to regulating AI, Policy Paper, 20 July 2022, \\nhttps://www.gov.uk/government/publications/establishing-a-pro-innovation-approach-to-regulating-ai/\\nestablishing-a-pro-innovation-approach-to-regulating-ai-policy-statement.\\n136 See, for example, Ada Lovelace Institute (2021), ‘Regulate to innovate’, 29 November 2021, \\nhttps://www.adalovelaceinstitute.org/report/regulate-innovate.\\n137 Chang, M. (2022), ‘Countermeasures: the need for new legislation to govern biometric technologies \\nin the UK’, London: Ada Lovelace Institute, 29 June 2022, https://www.adalovelaceinstitute.org/report/\\ncountermeasures-biometric-technologies.\\n138  See, for example, Centre for Digital Built Britain (2018), The Gemini Principles, Cambridge: University \\nof\\xa0Cambridge, https://www.cdbb.cam.ac.uk/system/files/documents/TheGeminiPrinciples.pdf.\\n139  European Commission (2021), Proposal for a Directive of the European Parliament and of the Council \\non Consumer Credits, COM/2021/347 final, 30 June 2021, https://eur-lex.europa.eu/legal-content/EN/\\nTXT/?uri=COM:2021:347:FIN.\\nWithout... clear standards and external involvement \\nor accountability, there\\xa0is\\xa0a\\xa0risk of\\xa0‘ethics-washing’ \\nrather than genuine mitigation\\xa0of\\xa0risks.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 37, 'page_label': '36'}, page_content='AI governance and human rights\\nResetting the relationship\\n36 Chatham House\\nin\\xa025 countries found that the pieces of primary legislation containing the phrase \\n‘artificial intelligence’ grew from one in 2016 to 18 in 2021, many of these specific \\nto a sector or issue.140 Governments are also considering amendments to existing \\ncross-sectoral regulation such as GDPR, which does not fully anticipate the \\nchallenges or the potential of AI.\\n5.1.2 Impact assessments and audit\\nThe most rapid area of growth concerns algorithmic impact assessments (AIAs) \\nand audits, which attempt to assess and manage ethical risks in the operation of \\nalgorithmic systems. While the terminology is not used consistently, AIAs tend \\nto assess impact prospectively (i.e.\\xa0before a system is in use), while audits are \\nretrospective (i.e.\\xa0looking back at a period of use).141\\nA number of bodies are currently developing template risk assessments for use \\nby creators or deployers of AI systems. For example, the US National Institute \\nof Standards and Technology (NIST) has released a draft AI Risk Management \\nFramework.142 The Singapore government is piloting a governance framework \\nand\\xa0toolkit known as AIVerify.143 The EU’s Artificial Intelligence Act will \\nencourage conformity assessment with technical standards for high-risk AI.144 \\nThe British government is keen to see a new market in AI assurance services \\nestablished in the UK, by which assurers would certify that AI systems meet their \\nstandards and so are trustworthy.145 The UK’s Alan Turing Institute has proposed \\nan assurance framework called HUDERIA.146 Technical standards bodies are \\ndeveloping frameworks, such as the IEEE’s Standard Model Process.147 There are \\nacademic versions, such as capAI,148 a conformity assessment process designed \\nby\\xa0a\\xa0consortium of Oxford-based ethicists, and the European Law Institute’s \\nModel Rules on Impact Assessment.149 There are also fledgling external review \\nprocesses\\xa0such as Z-Inspection.150\\n140 Stanford University (2022), Artificial Intelligence Index Report 2022, https://aiindex.stanford.edu/\\nwp-content/uploads/2022/03/2022-AI-Index-Report_Chapter-5.pdf, chap. 5.\\n141  The terminology of ‘impact assessment’ and ‘audit’ is used in different ways by different policymakers and \\nacademics. For a detailed discussion, see Ada Lovelace Institute and DataKind UK (2020), Examining the Black \\nBox, https://www.adalovelaceinstitute.org/wp-content/uploads/2020/04/Ada-Lovelace-Institute-DataKind-UK-\\nExamining-the-Black-Box-Report-2020.pdf.\\n142 National Institute of Standards and Technology (2022), AI Risk Management Framework: Initial Draft, \\n17\\xa0March 2022, https://www.nist.gov/system/files/documents/2022/03/17/AI-RMF-1stdraft.pdf.\\n143 Infocomm Media Development Authority (2022), Invitation to Pilot AI Verify AI Governance Testing Framework \\nand Toolkit, 25 May 2022, https://file.go.gov.sg/aiverify.pdf.\\n144 McFadden, M., Jones, K., Taylor, E. and Osborn, G. (2021), Harmonising Artificial Intelligence: The role of \\nstandards in the EU AI Regulation, Oxford Commission on AI & Good Governance, https://oxcaigg.oii.ox.ac.uk/\\nwp-content/uploads/sites/124/2021/12/Harmonising-AI-OXIL.pdf.\\n145 UK Centre for Data Ethics and Innovation (2021), The Roadmap to an Effective AI Assurance Ecosystem \\nhttps://www.gov.uk/government/publications/the-roadmap-to-an-effective-ai-assurance-ecosystem/the-\\nroadmap-to-an-effective-ai-assurance-ecosystem.\\n146 Alan Turing Institute (2021), Human Rights, Democracy, and the Rule of Law Assurance Framework \\nfor AI Systems: A proposal prepared for the Council of Europe’s Ad hoc Committee on Artificial Intelligence, \\nhttps://rm.coe.int/huderaf-coe-final-1-2752-6741-5300-v-1/1680a3f688.\\n147  IEEE Standard Model Process for Addressing Ethical Concerns during System Design, IEEE Std 7000-2021. \\nSee also ISO/IEC JTC 1/SC 42 Joint Committee SC 42 on Standardisation in the area of Artificial Intelligence.\\n148 Floridi, L. et al. (2022), capAI - A Procedure for Conducting Conformity Assessment of AI Systems in Line with \\nthe\\xa0EU Artificial Intelligence Act, 23 March 2022, http://dx.doi.org/10.2139/ssrn.4064091.\\n149 European Law Institute (2022), Model Rules on Impact Assessment of Algorithmic Decision-Making Systems Used \\nby Public Administration, https://www.europeanlawinstitute.eu/fileadmin/user_upload/p_eli/Publications/\\nELI_Model_Rules_on_Impact_Assessment_of_ADMSs_Used_by_Public_Administration.pdf.\\n150 Zicari, R. et al. (2021), ‘Z-Inspection®: A Process to Assess Trustworthy AI’, IEEE Transactions on Technology \\nand Society, 2(2), pp. 83–97, https://doi.org/10.1109/TTS.2021.3066209.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 38, 'page_label': '37'}, page_content='AI governance and human rights\\nResetting the relationship\\n37 Chatham House\\nLarger businesses have, meanwhile, established their own assessment processes. \\nFor example, Google conducts ethical reviews of AI applications it plans to \\nlaunch.151 IBM has an AI Ethics Board providing centralized governance, review \\nand decision-making.152 Rolls-Royce’s Aletheia Framework comprises a 32-step \\npractical toolkit for organizations developing and deploying AI.153\\nTypically, AIA processes invite AI developers, providers and users to elicit the \\nethical values engaged by their systems, refine those values and then assess their \\nproposed or actual AI products and systems (both data and models) against those \\nvalues, identifying and mitigating risks. Some models take a restrictive view of \\nethics, focusing primarily on data governance, fairness and procedural aspects \\nrather than all rights.154 A further tool proposed for data governance is data sheets \\nor ‘nutrition labels’ that summarize the characteristics and intended uses of data \\nsets, to reduce the risk of inappropriate transfer and use of datasets.155\\nSome governments are introducing impact assessments which are either \\nmandatory or carry strong incentives for compliance. For example, Canada’s \\nDirective on Automated Decision-Making requires Canadian government \\ndepartments to complete and publish an AIA prior to production of any automated \\ndecision system.156 The US’s draft Algorithmic Accountability Act, proposed \\nin Congress in 2019 and again in 2022, would require impact assessment of \\nsignificant automated decisions taken by larger entities.157 In the UK, the Ada \\nLovelace Institute has published a detailed proposal for an AIA to be completed \\nby any organization seeking professional access to the National Health Service \\n(NHS)’s proposed National Medical Imaging Platform\\xa0– the first known AIA \\nfor\\xa0data access in a healthcare context.158\\nWhile the identification and addressing of ethical risks is a positive step, these \\nprocesses come with challenges. Risk assessment of AI can mean identifying and \\nmitigating a broad range of impacts on individuals and communities\\xa0– a task that \\nis\\xa0potentially difficult, time-consuming and resource-intensive.159 The identification \\nand mitigation of ethical risks is not straightforward, particularly for teams whose \\nprior expertise may be technical rather than sociological. Extensive engagement \\nwith stakeholders may be necessary to obtain a balanced picture of risks. \\nResourcing challenges are magnified for smaller companies.\\n151  Google (2022), ‘AI Principles reviews and operations’, https://ai.google/responsibilities/review-process.\\n152 IBM (2022), ‘AI Ethics’, https://www.ibm.com/artificial-intelligence/ethics.\\n153 Rolls Royce (2020), ‘The Aletheia Framework’, https://www.rolls-royce.com/sustainability/ethics-and-\\ncompliance/the-aletheia-framework.aspx.\\n154 Infocomm Media Development Authority (2022), Invitation to Pilot AI Verify AI Governance Testing Framework \\nand Toolkit, 25 May 2022, https://file.go.gov.sg/aiverify.pdf.\\n155 Gebru, T. et al. (2018), ‘Datasheets for datasets’, arXiv, 1803.09010, https://doi.org/10.48550/arXiv.1803.09010; \\n Data Nutrition Project (2021), ‘The Dataset Nutrition Label’, https://datanutrition.org/labels \\n(accessed 12 Sep. 2022).\\n156 Government of Canada (2021), Directive on Automated Decision-Making, https://www.tbs-sct.canada.ca/\\npol/doc-eng.aspx?id=32592.\\n157  H.R. 6580\\xa0– Algorithmic Accountability Act of 2022, https://www.congress.gov/bill/117th-congress/\\nhouse-bill/6580/text.\\n158 Ada Lovelace Institute (2022), Algorithmic Impact Assessment: A Case Study in Healthcare, February 2022, \\nhttps://www.adalovelaceinstitute.org/report/algorithmic-impact-assessment-case-study-healthcare.\\n159  Nonnecke, B. and Dawson, P. (2021), ‘Human Rights Implications of Algorithmic Impact Assessments: Priority \\nConsiderations to Guide Effective Development’, Carr Center Discussion Paper Series, Harvard Kennedy School, \\nOctober 2021, https://carrcenter.hks.harvard.edu/files/cchr/files/nonnecke_and_dawson_human_rights_\\nimplications.pdf; Ada Lovelace Institute (2021), Regulate to innovate, p.52.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 39, 'page_label': '38'}, page_content='AI governance and human rights\\nResetting the relationship\\n38 Chatham House\\nIdentification of risks may not even be fully possible before an AI system enters \\ninto use, as some risks may only become apparent in the context of its deployment. \\nHence the importance of ongoing review, as well as review at the design stage. \\nYet, once a decision has been made to proceed with a technology, many companies \\nhave no vocabulary or structure for ongoing discussion of risks. In cases where \\nan AI system is developed by one organization and implemented by another, \\nthere may be no system for transferring the initial risk assessment to the recipient \\norganization and for the latter to implement ongoing risk management.\\nOnce risks have been identified, the models offer limited guidance on how to \\nbalance competing priorities, including on how to weigh ethical considerations \\nagainst commercial advantage. Subtle calculations cannot easily be rendered into \\nthe simple ‘stop’ or ‘go’ recommendation typically required by corporate boards.\\nSimilarly, the audit process presents challenges: auditors may require access \\nto extensive information, including on the operation of algorithms and their \\nimpact in context. There is a lack of benchmarks by which to identify or measure \\nfactors being audited (such as bias), while audits may not take account of \\ncontextual challenges.160\\nBritish regulators have identified various problems in the current AIA and \\naudit landscape, including a lack of agreed rules and standards; inconsistency \\nof audit\\xa0focus; lack of access to systems being audited; and insufficient action \\nfollowing audits.161 There is often inadequate inclusion of stakeholder groups; \\na lack of external verification; and little connection between these emerging \\nprocesses and any regulatory regimes or legislation.162 Recent UK research \\nconcluded that public sector policymakers should integrate practices that enable \\nregular policy monitoring and evaluation, including through institutional \\nincentives and binding legal frameworks; clear algorithmic accountability policies \\nand clear scope of algorithmic application; proper public participation and \\ninstitutional coordination across sectors and levels of governance.163\\nIt may be that many algorithms designed without regard to human rights \\nwill fail AIAs or audits. As awareness of human rights grows, so much current \\nAI may need adjusting. The Netherlands Court of Audit, having developed \\nan audit framework,164 recently audited nine algorithms used by the Dutch \\ngovernment. It\\xa0found that six of those nine failed to meet the requirements \\nof\\xa0the audit framework on such matters as privacy protection, absence of \\nbias\\xa0and\\xa0governance processes.165\\n160 Ada Lovelace Institute and DataKind UK (2020), Examining the Black Box, p. 10.\\n161 Digital Regulation Cooperation Forum (2022), Auditing algorithms: the existing landscape, role of regulators and future \\noutlook, 28 April 2022, https://www.gov.uk/government/publications/findings-from-the-drcf-algorithmic-processing-\\nworkstream-spring-2022/auditing-algorithms-the-existing-landscape-role-of-regulators-and-future-outlook.\\n162 Ibid., p. 16.\\n163 Ada Lovelace Institute, AI Now Institute and Open Government Partnership (2021), Algorithmic Accountability \\nfor the Public Sector, https://www.opengovpartnership.org/wp-content/uploads/2021/08/algorithmic-\\naccountability-public-sector.pdf.\\n164 Netherlands Court of Audit (2021), ‘Understanding Algorithms’, 26 January 2021, \\nhttps://english.rekenkamer.nl/publications/reports/2021/01/26/understanding-algorithms.\\n165 Netherlands Court of Audit (2022), ‘An Audit of 9 Algorithms Used by the Dutch Government’, 18 May\\xa02022, \\nhttps://english.rekenkamer.nl/publications/reports/2022/05/18/an-audit-of-9-algorithms-used-by-the-\\ndutch-government.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 40, 'page_label': '39'}, page_content='AI governance and human rights\\nResetting the relationship\\n39  Chatham House\\nOverall, without rigorous implementation of clear standards and external \\ninvolvement or accountability, there is a risk of ‘ethics-washing’ rather than \\ngenuine mitigation of risks.\\n5.1.3 Prohibition\\nGovernments and companies are beginning to prohibit forms of AI that raise \\nthe most serious ethical concerns. However, there is no consistency in such \\nprohibitions and the rationale behind them is often not openly acknowledged.\\nFor example, some US states have banned certain uses of facial recognition \\ntechnology, which remain in widespread use in other states. The EU’s Artificial \\nIntelligence Act would prohibit certain manipulative AI practices and most use of \\nbiometric identification systems in public spaces for law enforcement purposes.166 \\nTwitter decided to ban political advertising in 2019.167\\n5.1.4 Transparency\\nA further approach is public transparency measures through registries, \\nrelease\\xa0of\\xa0source code or algorithmic logic (required in France under the Digital \\nRepublic Law).168 In November 2021, the UK government launched the pilot of an \\nalgorithmic transparency standard, whereby public sector organizations provide \\ninformation on their use of algorithmic tools in a standardized format for publication \\nonline. Several government algorithms have since been made public as a result.169\\n5.1.5 Procurement conditions\\nThere is likely to be a rapid growth in the imposition of conditions in the sale of \\nalgorithmic systems, particularly where purchasers such as governments and local \\nauthorities will be seeking to use those systems in the public interest. Authorities \\nare likely to impose contractual conditions requiring the system to respect \\nstipulated criteria on such matters as bias and transparency. For example, the \\nCity of Amsterdam has developed contractual terms requiring suppliers of AI and \\nalgorithmic systems to meet standards of explainability and transparency, including \\non what data is used and how bias is counteracted.170 Such conditions imposed \\nby\\xa0the public sector may have the effect of driving up\\xa0standards more widely.\\n166 Proposal for a Regulation of the European Parliament and of the Council laying down harmonised rules on \\nartificial intelligence (Artificial Intelligence Act) and amending certain Union legislative acts, COM/2021/206 \\nhttps://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX:52021PC0206, Article 5.\\n167  Twitter (2019), ‘Political Content’, https://business.twitter.com/en/help/ads-policies/ads-content-policies/\\npolitical-content.html.\\n168 Loi No. 2016-1321 du 7 octobre 2016 pour une République Numerique.\\n169 Central Digital and Data Office (2021), ‘Algorithmic Transparency Standard’, https://www.gov.uk/\\ngovernment/collections/algorithmic-transparency-standard.\\n170  Gemeente Amsterdam (2022), ‘Contractual terms for algorithms’, https://www.amsterdam.nl/innovatie/\\ndigitalisering-technologie/algoritmen-ai/contractual-terms-for-algorithms.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 41, 'page_label': '40'}, page_content='AI governance and human rights\\nResetting the relationship\\n40 Chatham House\\n5.2 Processes: human rights law\\n5.2.1 Governmental duty to protect against breaches\\nGovernments have a duty both to comply with human rights in any uses of AI they \\nadopt\\xa0– for example, in public decision-making\\xa0– and to protect individuals from \\nabuses of human rights by companies and other non-state actors. States must take \\n‘appropriate steps to prevent, investigate, punish and redress such abuse through \\neffective policies, legislation, regulations and adjudication’.171\\nGovernments are expected to find the appropriate mix of laws, policies and \\nincentives to protect against human rights harms. A ‘smart mix’ of national and \\ninternational, mandatory and voluntary measures would help to foster business \\nrespect for human rights.172 This includes requiring companies to have suitable \\ncorporate structures to identify and address human rights risk on an ongoing basis, \\nand to engage appropriately with external stakeholders as part of their human \\nrights assessments. Where businesses are state-owned, or work closely with the \\npublic sector, the government should take additional steps to protect against \\nhuman rights abuses through management or contractual control.173\\nGovernments’ human rights obligations mean that they cannot simply wait and \\nsee how AI develops before engaging in governance activities. They are obliged to \\ntake action, including via regulation and/or the imposition of impact assessments \\nand audits, to ensure that AI does not infringe human rights. Governments should \\nensure that they understand the implications of human rights for AI governance, \\ndeploying a dedicated capacity-building effort or technology and human rights \\noffice where a gap exists.174\\nThere is an urgent need for governments to devise regulation that is both \\neffective\\xa0in ensuring that companies do not infringe individuals’ human \\nrights when designing and implementing AI systems, and that provides for \\neffective remedies in the event of any such infringement. Given the ambiguity \\nof commitments to ethics and the strength of countervailing commercial \\nconsiderations, a purely voluntary approach is unlikely to protect individuals’ \\nhuman rights adequately. Indeed, some argue that states are obliged to enact \\nlegally binding norms to protect human rights in light of the challenges posed by \\nAI\\xa0systems.175 Governments should regulate to either prohibit or require constraints \\non applications of AI, such as biometric technologies, that risk interfering \\nwith human rights in a manner clearly\\xa0disproportionate to any countervailing \\nlegitimate interest.\\n171 UN Office of the High Commissioner for Human Rights (2011), Guiding Principles on Business and Human \\nRights, principle 1.\\n172 Ibid., principle 3; and UN OHCHR B-Tech (2021), Bridging Governance Gaps in the Age of Technology\\xa0– Key \\ncharacteristics of the State Duty to Protect, https://www.ohchr.org/sites/default/files/Documents/Issues/\\nBusiness/B-Tech/b-tech-foundational-paper-state-duty-to-protect.pdf.\\n173 UN Office of the High Commissioner for Human Rights (2011), Guiding Principles on Business and Human \\nRights, principle 4; UN OHCHR B-Tech (2021), Bridging Governance Gaps in the Age of Technology\\xa0– Key \\ncharacteristics of the State Duty to Protect.\\n174  Element AI (2019), Closing the Human Rights Gap in AI Governance, http://mediaethics.ca/wp-content/\\nuploads/2019/11/closing-the-human-rights-gap-in-ai-governance_whitepaper.pdf.\\n175  Bello y Villarino, J.-M. and Vijeyarasa, R. (2022), ‘International Human Rights, Artificial Intelligence, and \\nthe Challenge for the Pondering State: Time to Regulate?’, Nordic Journal of Human Rights, 40(1), pp. 194–215, \\nhttps://doi.org/10.1080/18918131.2022.2069919.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 42, 'page_label': '41'}, page_content='AI governance and human rights\\nResetting the relationship\\n41  Chatham House\\nGovernments should ensure that AIA and audit processes are conducted systematically, \\nemploying rigorous standards and due process, and that such processes pay due \\nregard to potential human rights impacts of AI: for example by\\xa0making assessment \\nof\\xa0human rights risks an explicit feature of such processes.176 To\\xa0incentivize corporate \\ngood practice, demonstrate respect for human rights and\\xa0facilitate remedy, states \\nshould also consider requiring companies to report publicly on any due diligence \\nundertaken and on human rights impacts identified and addressed.\\nSupervision by regulatory and administrative authorities is an important element \\nof accountability for compliance with human rights responsibilities, in parallel with \\nlegal liability for harms. As some European countries and the EU begin to implement \\nmandatory human rights and environmental due diligence obligations for larger \\nbusinesses,177 human rights experts are exploring administrative supervision of \\ncorporate duties as a complement to liability for harms in the courts.178\\nGovernments have legal obligations not to breach human rights in their provision \\nof AI-assisted systems. Anyone involved in government procurement of AI should \\nhave enough knowledge and information to understand the capacity and potential \\nimplications of the technology they are buying, and to satisfy themselves that \\nit meets required standards on equality, privacy and other rights (such as the \\nPublic Sector Equality Duty in the UK). Governments should negotiate the terms \\nof\\xa0public–private contracts and deploy procurement conditions to ensure that \\nAI from private providers is implemented consistently with human rights. They \\nshould also take steps to satisfy themselves that this requirement is met. Public \\nprocurement is a\\xa0means of encouraging improvements to human rights standards \\nin\\xa0the AI industry as a whole.179 It is important also to ensure that AI systems already \\nadopted comply with human rights standards: the experience of the Netherlands \\ndemonstrates that systems adopted to date can be problematic.180\\n176  Nonnecke, B. and Dawson, P. (2022), Human Rights Impact Assessments for AI: Analysis and Recommendations, \\nNew York: Access Now, October 2022, https://www.accessnow.org/cms/assets/uploads/2022/11/Access-Now-\\nVersion-Human-Rights-Implications-of-Algorithmic-Impact-Assessments_-Priority-Recommendations-to-Guide-\\nEffective-Development-and-Use.pdf.\\n177 European Commission (2022), ‘Proposal for a Directive on Corporate Sustainability Due Diligence’, \\n23\\xa0February 2022, https://ec.europa.eu/info/publications/proposal-directive-corporate-sustainable-due-\\ndiligence-and-annex_en. Several EU member states and other states have implemented similar obligations or \\nelements of mandatory human rights due diligence. For example, see Office of the UN High Commissioner for \\nHuman Rights (2020), UN Human Rights “Issues Paper” on legislative proposals for mandatory human rights due \\ndiligence by companies, June 2020, https://www.ohchr.org/sites/default/files/Documents/Issues/Business/\\nMandatoryHR_Due_Diligence_Issues_Paper.pdf, pp. 3–5.\\n178  Shift and Office of the UN High Commissioner for Human Rights (2021), Enforcement of Mandatory Due Diligence: \\nKey Design Considerations for Administrative Supervision, Policy Paper, October 2021, https://shiftproject.org/wp-\\ncontent/uploads/2021/10/Enforcement-of-Mandatory-Due-Diligence_Shift_UN-Human-Rights_Policy-Paper-2.pdf.\\n179 Office of the UN High Commissioner for Human Rights (2022), The Practical Application of the Guiding Principles \\non Business and Human Rights to the Activities of the Technology Sector, April 2022, https://reliefweb.int/report/\\nworld/practical-application-guiding-principles-business-and-human-rights-activities-technology-companies-report-\\noffice-united-nations-high-commissioner-human-rights-ahrc5056-enarruzh, para. 20.\\n180 Netherlands Court of Audit (2022), ‘An Audit of 9 Algorithms Used by the Dutch Government’.\\nSupervision by regulatory and administrative \\nauthorities is an important element of accountability \\nfor compliance with human rights responsibilities, \\nin\\xa0parallel with legal liability for harms.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 43, 'page_label': '42'}, page_content='AI governance and human rights\\nResetting the relationship\\n42 Chatham House\\n5.2.2 Corporate responsibility to respect human rights\\nThe UN’s Guiding Principles on Business and Human Rights are clear that ‘business \\nenterprises should respect human rights’. In other words, companies (particularly \\nlarge ones)181 should avoid infringing human rights and should address any adverse \\nhuman rights impacts resulting from their activities.182 Companies should have \\na\\xa0policy commitment to meet their human rights responsibilities, approved at senior \\nlevel, publicly available and embedded in the culture of the business.183 Companies \\nmust also have an ongoing due diligence process of human rights impact assessment, \\ntracked for responsiveness and reported externally, which allows them to identify, \\nmitigate and remedy human rights impacts.184 By deploying a responsible business \\nagenda, identifying and mitigating risks, companies can forestall problems and \\nsave\\xa0themselves the time, money and acrimony of litigation.\\nDue diligence in the AI context is particularly challenging because of two \\ndistinguishing features. First, AI’s capacity for self-improvement may make it \\ndifficult to predict its consequences. Second, AI’s human rights impact will depend \\nnot only on the technology itself, but also on the context in which it is deployed. \\nIn\\xa0light of both these factors, due diligence on AI applications that may affect \\nhuman rights must be extensive and involve as wide a set of stakeholders as may \\nbe\\xa0affected by the AI. Further, given the risk of unanticipated consequences, \\nAI must be reviewed regularly once in operation. Hence, the former UN high \\ncommissioner on human rights called for comprehensive human rights due \\ndiligence to be conducted ‘when AI systems are acquired, developed, deployed \\nand\\xa0operated’,185 with that due diligence to continue ‘throughout the entire life cycle \\nof\\xa0an AI system’186 and to include consultations with stakeholders and involvement \\nof\\xa0experts.187 At present, many companies lack structures and processes to \\ndetect and act on human rights issues on an ongoing basis. The former UN high \\ncommissioner also called for the results of due diligence to be made public.188\\nSome companies’ AIAs are labelled as human rights assessment, like Verizon’s \\nongoing human rights due diligence.189 Other AI ethics assessments, such as that \\nadopted by the IEEE and the proposed AIA for the National Medical Imaging \\nPlatform, look similar to human rights due diligence, but are not labelled as such. \\nGoogle reviews proposals for new AI deployment by reference to its AI Principles, \\na\\xa0process that can include consultation with human rights experts.190\\n181 The UN Guiding Principles on Business and Human Rights apply to all businesses, but the extent of business \\nresponsibilities increases with the organization’s size and the impact of its work: see UN Office of the High \\nCommissioner on Human Rights (2011), Guiding Principles on Business and Human Rights (2011), principle 14.\\n182 UN Office of the High Commissioner on Human Rights (2011), Guiding Principles on Business and Human \\nRights, principles 11, 13.\\n183 UN Office of the High Commissioner on Human Rights (2011), Guiding Principles on Business and Human \\nRights, principles 15 and 16.\\n184 UN Office of the High Commissioner on Human Rights (2011), Guiding Principles on Business and \\nHuman Rights, principles 15–21. See also Data & Society and European Center for Non-Profit Law (2021), \\nRecommendations for Assessing AI Impacts to Human Rights, Democracy and the Rule of Law, https://ecnl.org/sites/\\ndefault/files/2021-11/HUDERIA%20paper%20ECNL%20and%20DataSociety.pdf.\\n185 United Nations High Commissioner for Human Rights (2021), The Right to Privacy in the Digital Age, \\nA/HRC/48/31, https://documents-dds-ny.un.org/doc/UNDOC/GEN/G21/249/21/PDF/G2124921.pdf, para. 48.\\n186 Ibid., para. 49.\\n187  Ibid., para. 50.\\n188 Ibid., para. 50.\\n189  Verizon (2022), ‘Human Rights at Verizon’, https://www.verizon.com/about/investors/human-\\nrights-at-verizon.\\n190 Google AI (2022), ‘AI Principles reviews and operations’. https://ai.google/responsibilities/review-process.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 44, 'page_label': '43'}, page_content='AI governance and human rights\\nResetting the relationship\\n43 Chatham House\\nWhatever the labelling, certain features of human rights impact assessment are \\ncommonly omitted from corporate processes:\\n — Transparency. General statements of corporate intention and activity are easier \\nto find than public statements of human rights risks actually identified and \\nmitigated through due diligence processes.\\n — Scope. Some corporate processes only cover specific issues, such as bias and \\nprivacy, rather than the full range of human rights, or make only brief mention \\nof other rights.191\\n — Effect. It is often not clear what effect impact assessments have on the \\ncompany’s activities.192 Human rights due diligence requires that human rights \\nrisks be mitigated, whereas some business processes seem to entail balancing \\nrisks against perceived benefits.193\\n — Duration. Human rights due diligence includes a requirement for ongoing \\nreview post-implementation, whereas many corporate reviews appear to focus \\nonly on product development. Ongoing review is particularly important in \\nlight of AI’s capacity for self-improvement over time. Otherwise, there is a risk \\nthat assessments give algorithmic processes a veneer of legitimacy rather than \\ngenuinely having an impact on activities.194 This risk is amplified when there \\nis\\xa0no transparency about the process, its results or impact.\\nIn addition to ensuring the adequacy of their impact assessment processes \\nfrom a human rights perspective, companies should foster a pro-human rights \\nculture throughout their organization. This means ensuring that AI teams are \\nrepresentative of society’s diversity and the diversity of intended consumers, \\nsuch that equality is ‘baked in’ to system design. It means engaging adequate \\ninternal and external expertise to conduct human rights due diligence and impact \\nassessments, including through involvement of stakeholders, and commitment \\nat board level to addressing human rights impacts identified. It also means public \\nreporting of any human rights risks and impacts identified and measures taken. \\nIt may mean providing training on human rights for all those working on AI\\xa0– \\nincluding technical experts, engineers and devisers of technical standards. It must \\ninclude ongoing monitoring of human rights impacts over time and preparedness \\nto address new concerns that may arise.\\n191 For example, Microsoft’s Responsible AI Impact Template (2022), https://blogs.microsoft.com/wp-content/\\nuploads/prod/sites/5/2022/06/Microsoft-RAI-Impact-Assessment-Template.pdf.\\n192  Ada Lovelace Institute and DataKind UK (2020), Examining the Black Box, p. 18.\\n193 For example, Google states that it will not pursue technologies that cause or are likely to cause overall harm, \\nand ‘where there is a material risk of harm, we will proceed only where we believe that the benefits substantially \\noutweigh the risks, and will incorporate appropriate safety constraints.’ They also say that they will not proceed \\nwith ‘technologies whose purpose contravenes widely accepted principles of international law and human rights.’ \\nSee Google AI (2022), ‘Artificial Intelligence at Google: Our Principles’, https://ai.google/principles.\\n194 Ada Lovelace Institute and DataKind UK give the example of an impact evaluation of a predictive risk \\nmodelling tool for Allegheny County, PA’s children’s welfare office, with positive results that both conflicted with \\nother reviews of the tool and may have provided legitimacy for further use of AI in children’s social services. \\nAda\\xa0Lovelace Institute and DataKind UK (2020), Examining the Black Box, p. 19.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 45, 'page_label': '44'}, page_content='44 Chatham House\\n06 \\nRemedies in \\nAI\\xa0governance: \\nthe\\xa0contribution \\nof\\xa0human rights\\nBoth governments and companies should provide \\nsuitable\\xa0access to remedy for when AI goes wrong. \\nThis\\xa0entails effective reparation, accountability \\nand\\xa0measures\\xa0to\\xa0prevent recurrences.\\n6.1 Remedies: the landscape\\nLittle attention has been given to the development of a scheme of remedies for \\nwhen AI goes wrong. Responsibility needs to be clarified, and transparency \\nis\\xa0required to assess whether and how AI has gone wrong.\\nWhile AI governance principles commonly include a principle of accountability, \\nthis\\xa0often refers to impact assessments, audit or oversight, rather than a requirement \\nof remedy in the event of harms.195 Many sets of AI governance principles in fact \\nhave no provision for remedy. As the UN special rapporteur on contemporary \\nforms of racism has pointed out, ‘[e]thical commitments have little measurable \\n195 For example, UNESCO’s Recommendation on the Ethics of Artificial Intelligence (2021) discusses oversight, \\nimpact assessment, audit and due diligence mechanisms (paras 42 and 43) and suggests that states may wish \\nto\\xa0consider establishing an ethics commission or ethics observatory (para. 133).'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 46, 'page_label': '45'}, page_content='AI governance and human rights\\nResetting the relationship\\n45 Chatham House\\neffect on software development practices if they are not directly tied to structures \\nof\\xa0accountability in the workplace’.196\\nTo some extent, legal remedies for wrongs caused by the application of AI already \\nexist in tort law (negligence) and administrative law, particularly where those \\nwrongs are on the part of public authorities. However, the law and its processes \\nwill need to develop metrics for evaluating AI. For example, English administrative \\nlaw typically has regard to whether the decision-maker took the right factors into \\naccount when making their decision. But AI relies on statistical inferences rather \\nthan reasoning. Factors such as the opacity of AI systems and imbalance of \\ninformation and knowledge between companies and users, scalability of errors \\nand\\xa0rigidity of\\xa0decision-making may also pose challenges.197 As yet, there is no \\nclear\\xa0‘remedy pathway’ for those who suffer abuses of human rights as a result \\nof\\xa0the operation of AI.198\\nThose at greatest risk from harms caused by AI are likely to be the most \\nmarginalized and vulnerable groups in society, such as immigrants and those \\nin\\xa0the\\xa0criminal justice system. This makes it all the more important to ensure \\nthat\\xa0avenues for remedy are accessible to all, whatever their situation.\\nThere has already been some litigation challenging the application of AI by \\nreference to human rights law or its local equivalent. Notable cases include:\\n — In 2016, State of Wisconsin v Eric L Loomis, which challenged the use of \\nAI\\xa0COMPAS risk assessments when sentencing defendants in criminal cases. \\nThe COMPAS risk assessment was an assessment of recidivism risk, based \\non\\xa0comparisons with other individuals with a similar history of offending. \\nThe\\xa0Supreme Court of Wisconsin held that a court’s consideration of a COMPAS \\nrisk assessment is consistent with the defendant’s right to due process, provided \\nthat the risk assessment is used in parallel with other factors and is not \\ndeterminative of the defendant’s sentence.199\\n — In May 2017, teachers in Houston successfully challenged the use of an \\nalgorithm known as EVAAS,200 developed by a private company to measure \\nteacher effectiveness.201 The aim of the algorithm was to enable the Houston \\n196 Report of the UN Special Rapporteur on contemporary forms of racism, racial discrimination, xenophobia and \\nrelated intolerance, Racial discrimination and emerging digital technologies: a human rights analysis, A/HRC/44/57 \\n18 June 2020, para. 62.\\n197  See Williams, R. (2021), ‘Rethinking Administrative Law for Algorithmic Decision Making’, Oxford Journal \\nof\\xa0Legal Studies, 2(2), https://doi.org/10.1093/ojls/gqab032, pp. 468–94.\\n198 Office of the UN High Commissioner for Human Rights (2022), The Practical Application of the Guiding \\nPrinciples on Business and Human Rights to the Activities of the Technology Sector, para. 58.\\n199 State of Wisconsin v Eric L Loomis 2016 WI 68, 881 N.W.2d 749.\\n200 EVAAS stands for Educational Value-Added Assessment System.\\n201 Houston Federation of Teachers v Houston Independent School District 251 F.Supp.3d 1168 (SD Tex 2017).\\nThose at greatest risk from harms caused by \\nAI are likely to be the most marginalized and \\nvulnerable groups in society, such as immigrants \\nand those in the criminal justice system.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 47, 'page_label': '46'}, page_content='AI governance and human rights\\nResetting the relationship\\n46 Chatham House\\nIndependent School District (HISD) to terminate the employment of teachers \\nwhose performance was deemed ineffective. The US district court denied \\nHISD’s application for summary judgment against the teachers’ claim. The \\ncourt found that the teachers were ‘unfairly subject to mistaken deprivation \\nof constitutionally protected property interests in their jobs’, contrary to \\nthe\\xa0Due Process Clause of the Fourteenth Amendment of the US Constitution, \\nbecause they had no meaningful way to ensure correct calculation of their \\nscores, nor opportunity to independently verify or replicate those scores. \\nAfter the summary judgment, the case was settled and HISD abandoned \\nthe EVAAS system.202\\n — In March 2018, Finland’s National Non-Discrimination and Equality Tribunal \\ndecided that a credit institution’s decision not to grant credit to an individual was \\ndiscriminatory. The tribunal ruled that the credit institution’s decision was made \\nnot on the basis of the individual’s own credit behaviour and creditworthiness, \\nbut by drawing assumptions from statistical data and information on payment \\ndefault relating to other people, by criteria such as gender, first language, age \\nand residential area. The tribunal prohibited the credit institution from using \\nthis decision-making method.203\\n — In February 2020, the Hague district court ordered the Dutch government to \\ncease its use of SyRI, an automated programme that reviewed the personal data \\nof social security claimants to predict how likely people were to commit benefit \\nor tax fraud. The Dutch government refused to reveal how SyRI used personal \\ndata, such that it was extremely difficult for individuals to challenge the \\ngovernment’s decisions to investigate them for fraud or the risk scores stored \\non file about them. The Court found that the legislation regulating SyRI did not \\ncomply with the right to respect for private life in Article 8 ECHR, as it failed \\nto balance adequately the benefits SyRI brought to society with the necessary \\nviolation of private life caused to those whose personal data it assessed. The \\nCourt also found that the system was discriminatory, as SyRI was only used \\nin so-called ‘problem neighbourhoods’, a proxy for discrimination on the basis \\nof\\xa0socio-economic background and immigration status.204\\n — In August 2020, R (Bridges) v Chief Constable of South Wales Police205 was \\nthe\\xa0first\\xa0challenge to AI invoking UK human rights law. South Wales Police \\nwas trialling the use of live automated facial recognition technology (AFR) \\nto\\xa0compare CCTV images of people attending public events with images \\nof persons on a database. If there was no match, the CCTV images were \\nimmediately deleted from the AFR system. The complainant challenged AFR’s \\nmomentary capture of his image and comparison with its watch-list database, \\nby reference to Article 8 ECHR and the UK Data Protection Act. The Court \\n202  McCully, J. (2017), ‘Houston Federation of Teachers and Others v HISD’, Atlas Lab blog, \\nhttps://www.atlaslab.org/post/houston-federation-of-teachers-and-others-v-hisd-secret-algorithm-used-\\nto-fire-teachers.\\n203 National Non-Discrimination and Equality Tribunal of Finland (2018), Assessment of creditworthiness, \\nauthority, direct multiple discrimination, gender, language, age, place of residence, financial reasons, conditional \\nfine, Register No. 216/2017, 21 March 2018, https://www.yvtltk.fi/material/attachments/ytaltk/\\ntapausselosteet/45LI2c6dD/YVTltk-tapausseloste-_21.3.2018-luotto-moniperusteinen_syrjinta-S-en_2.pdf.\\n204 Toh, A. (2020), ‘Dutch Ruling a Victory for Rights of the Poor’, Human Rights Watch Dispatches, 6 February \\n2020, https://www.hrw.org/news/2020/02/06/dutch-ruling-victory-rights-poor.\\n205 [2020] EWCA Civ 1058.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 48, 'page_label': '47'}, page_content='AI governance and human rights\\nResetting the relationship\\n47  Chatham House\\nof Appeal found that there was not a proper basis in law for the use of AFR. \\nConsequently, its use breached the Data Protection Act. The court declined \\nto\\xa0find that the police’s use of AFR struck the wrong balance between the rights \\nof the individual and the interests of the community. But it did find that South \\nWales Police had failed to discharge the statutory Public Sector Equality Duty,206 \\nbecause in buying the AFR software from a private company and deploying \\nit, they had failed to take all reasonable steps to satisfy themselves that the \\nsoftware did not have a\\xa0racial or gender bias (notwithstanding that there was \\nno evidence to support the contention that the software was biased). The case \\ntherefore temporarily halted South Wales Police’s use of facial recognition \\ntechnology, but allowed the possibility of its reintroduction in future with \\nproper legal footing and due regard to the Public Sector Equality Duty. Indeed, \\nSouth Wales Police has since reintroduced facial recognition technology for \\nuse\\xa0in certain circumstances.207\\n — The Italian courts, having held in 2019 that administrative decisions based \\non\\xa0algorithms are illegitimate, reversed that view in 2021. The courts welcomed \\nthe speed and efficiency of algorithmic decision-making but clarified that it is \\nsubject to general principles of administrative review in Italian law, including \\ntransparency, effectiveness, proportionality, rationality and non-discrimination. \\nComplainants about public decision-making are entitled to call for disclosure of \\nalgorithms and related source code in order to challenge decisions effectively.208\\n — In July 2022, the UK NGO Big Brother Watch issued a legal complaint to the \\nBritish information commissioner in respect of alleged use of facial recognition \\ntechnology by Facewatch and the supermarket chain Southern Co-op to scan, \\nmaintain and assess profiles of all supermarket visitors in breach of data \\nprotection and privacy rights.209\\n6.2 Remedies: human rights law\\nHuman rights law requires both governments and companies to provide a suitable \\nright to remedy in the event of breach of their obligations and responsibilities.210 \\nRemedy comprises effective reparation, appropriate accountability for those \\nresponsible, as well as measures to prevent recurrences. The availability of \\nremedy\\xa0is crucial if human rights or ethical principles are to have real impact \\nin\\xa0the\\xa0face of countervailing commercial considerations.\\n206 Section\\xa0149(1) Equality Act 2010: ‘A public authority must, in the exercise of its functions, have due regard \\nto\\xa0the need to- (a) eliminate discrimination, harassment, victimisation and any other conduct that is prohibited \\nby or under this Act; (b) advance equality of opportunity between persons who share a relevant protected \\ncharacteristic and persons who do not share it; (c) foster good relations between persons who share a relevant \\nprotected characteristic and persons who do not share it.’\\n207  South Wales Police (2022), ‘Facial Recognition Technology’, https://www.south-wales.police.uk/police-\\nforces/south-wales-police/areas/about-us/about-us/facial-recognition-technology.\\n208 Liguori, L. and Vittoria La Rosa, M. (2021), ‘Law and Policy of the Media in a Comparative Perspective’, \\nFilodiritto blog, 20 May 2021, https://www.filodiritto.com/law-and-policy-media-comparative-perspective.\\n209 Big Brother Watch (2022), Grounds of Complaint to the Information Commissioner under section\\xa0165 of the \\nData Protection Act 2018: Live Automated Facial Recognition by Facewatch Ltd and the Southern Cooperative Ltd, \\nhttps://docs.reclaimthenet.org/big-brother-watch-co-op-facewatch-legal-complaint.pdf.\\n210 International Covenant on Civil and Political Rights, Article 2(3); UN Office of the High Commissioner \\non\\xa0Human Rights (2011), Guiding Principles on Business and Human Rights, principles 25–31.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 49, 'page_label': '48'}, page_content='AI governance and human rights\\nResetting the relationship\\n48 Chatham House\\nThis means that, at all stages of design and deployment of AI, it must be clear \\nwho\\xa0bears responsibility for its operation. In particular, clarity is required on where \\nthe division of responsibilities lies between the developer of an AI system and \\nthe\\xa0purchaser and deployer of the system, including if the purchaser adapts the \\nAI\\xa0or uses it in a way for which it was not intended. Consequently, purchasers of AI \\nsystems will need adequate understanding or assurance as to how those systems \\nwork, as was demonstrated for the public sector in the Bridges case, discussed \\nabove. In that case, the court also held that commercial confidentiality around \\nany AI technology does not defeat or reduce the requirement for compliance \\nwith\\xa0the\\xa0Public Sector Equality Duty.211\\nComplainants need to know how to complain and to whom, and to be confident \\nthat their complaint will be addressed in a timely manner. Remedy relies on \\ntransparency and explainability\\xa0– complainants should have enough information \\nto understand how a decision about them was made, and the role and operation \\nof AI in the decision-making process. They may need access to data on how the AI \\nwas designed and tested, how it was intended to operate and how it has operated \\nin the specific case, as well as information on the role of human decision-making \\nor\\xa0oversight in the process.\\nRemedy may be provided by the courts, by other governmental mechanisms \\nsuch as regulators, ombudspersons and complaints processes, as well as by \\nnon-governmental mechanisms such as corporate remediation processes. The \\nUN Guiding Principles recommend that all businesses ‘establish or participate \\nin\\xa0effective operational-level grievance mechanisms’.212 Such mechanisms should \\nbe legitimate (i.e.\\xa0enabling trust); accessible; predictable; equitable; transparent; \\nrights-compatible; a source of continuous learning; and based on engagement \\nand\\xa0dialogue with stakeholders.213\\nThere are challenges in designing appropriate grievance mechanisms for \\naddressing harms caused by AI. Remedial systems that rely on individual \\ncomplaint tend to be better at addressing significant harms suffered by few than \\nharms suffered by many.214 But AI, with its capacity for operation at scale, risks \\ninfringing the rights of large numbers of people\\xa0– for example, by using personal \\ndata in violation of the right to privacy or engaging in widespread discriminatory \\ntreatment. Many of the people affected could be vulnerable or marginalized, \\nincluding asylum-seekers and those in the criminal justice system. Consequently, \\nthere needs to be provision both for individual complaints and for group or \\nrepresentative complaints against a whole system rather than a single decision. \\nOmbudsmen, national human rights institutions and civil society organizations \\nshould be adequately equipped to support victims’ complaints and to challenge \\nAI systems that are systematically causing harm. Remedies should consist both \\nof adequate remedy to victims and requirements to improve, or end the use of, \\nAI\\xa0systems to prevent recurrence of any harm identified.\\n211 R (Bridges) v Chief Constable of South Wales Police [2020] EWCA Civ 1058, para. 199.\\n212 UN Office of the High Commissioner on Human Rights (2011), Guiding Principles on Business and Human \\nRights, principle 29.\\n213 UN Office of the High Commissioner on Human Rights (2011), Guiding Principles on Business and Human \\nRights, principle 31.\\n214 Raso, F. et al. (2018), Artificial Intelligence & Human Rights: Opportunities & Risks, Berkman Klein Center \\nResearch Publication No. 2018-6, 25 September 2018, http://dx.doi.org/10.2139/ssrn.3259344, p. 56.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 50, 'page_label': '49'}, page_content='AI governance and human rights\\nResetting the relationship\\n49 Chatham House\\nSimilarly, a business should be able to pursue accountability against other \\ncompanies that have harmed its operations as a result of AI. This may be because \\nthe business has purchased an AI system that has not functioned as intended, \\nor\\xa0because another company’s AI has in some way interfered with its operations.\\nMany challenges are expected in this field in the coming years. The guiding \\nprinciple should remain provision of an effective right to remedy, including \\nfor\\xa0breach of human rights responsibilities.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 51, 'page_label': '50'}, page_content='50 Chatham House\\n07 \\nConclusion and \\nrecommendations\\nTo place human rights at the heart of AI governance, \\ncompanies, governments, international organizations, civil \\nsociety and investors must take effective practical steps.\\nAs AI begins to reshape the human experience, human rights must be central to its \\ngovernance. There is nothing to fear, and much to gain, from taking human rights \\nas the baseline for AI governance.\\nFailure to take account of human rights means setting aside well-established, \\nwidely acknowledged parameters of liberty, fairness and equality, as well as \\nprocesses and accountability for their implementation. It involves creating \\nconfusing and inadequate alternatives to existing norms. It also duplicates much \\nof the work of developing those norms, the processes for their implementation \\nand\\xa0the remedies for their breach.\\nIf human rights are to be placed at the centre of AI governance, the following \\npractical actions are necessary.\\nFor companies:\\n — Continue to promote AI ethics and responsible business agendas, while \\nacknowledging the important complementary role of existing human \\nrights frameworks;\\n — Champion a holistic commitment to all human rights standards from the top of \\nthe organization. Enable a\\xa0change of corporate mindset, such that human rights \\nare seen as a useful tool in the box rather than as a\\xa0constraint on innovation;\\n — Recruit people with human rights expertise to join AI ethics teams to \\nencourage multi-disciplinary thinking and spread awareness of human rights \\norganization-wide. Use human rights as the common language and framework \\nfor multi-disciplinary teams addressing aspects of AI governance;'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 52, 'page_label': '51'}, page_content='AI governance and human rights\\nResetting the relationship\\n51  Chatham House\\n — Conduct human rights due diligence and adopt a human rights-based \\napproach\\xa0to AI ethics and impact assessment. Create decision-making structures \\nthat allow human rights risks to be monitored, flagged and acted upon on \\nan ongoing basis;\\n — Ensure uses of AI are explainable and transparent, so that people affected \\ncan\\xa0find out how an AI or AI-assisted decision was, or will be, made; and\\n — Establish a mechanism for individuals to seek remedy if they are dissatisfied \\nwith the outcome of a decision made or informed by AI.\\nFor governments:\\n — Ensure adequate understanding of human rights among government officials \\nand place human rights at the heart of AI regulation and policies, either via \\nthe\\xa0establishment of a dedicated office or other existing mechanisms;\\n — Equip teams involved in government procurement of systems and services \\nwith expertise in AI and human rights. Use contracting policy and \\nprocurement conditions to increase compliance with human rights standards \\namong businesses;\\n — Establish a discussion forum on AI governance that engages all stakeholders, \\nincluding human rights advocates, to foster better understanding and mutual \\nbenefit from others’ perspectives;\\n — Ensure that technical standards bodies, AI assurance mechanisms and devisers \\nof algorithmic impact assessment and audit processes give due regard to human \\nrights when developing and monitoring standards for AI governance;\\n — Consider cross-cutting regulation to ensure that AI deployed by both the public \\nand private sectors meets human rights standards;\\n — Put in place human rights-compatible standards and oversight for AIAs and \\naudits, as well as adequate provision of remedy for alleged breaches;\\n — Educate the public on the vital role of human rights in protecting individual \\nfreedoms as AI technology develops. Offer guidance to schools and teachers so \\nthat children have an understanding of human rights before they encounter AI;\\n — Ensure that all uses of AI are explainable and transparent, such that people \\naffected can find out how an AI or AI-informed decision was, or will be, made;\\n — Provide adequate resources for national human rights bodies and regulators, \\nsuch as the UK Equalities and Human Rights Commission, to champion the \\nrole of human rights in AI governance. Ensure these bodies are included \\nin\\xa0discussions on emerging tech issues;\\n — Incentivize AI development that benefits society as widely as possible and \\ncontributes to implementation of the UN’s SDGs; and\\n — Liaise with other governments and international organizations with a view \\nto\\xa0harmonizing understanding of the impact of international human rights law \\non the development and implementation of AI (for example, through use of \\nsoft\\xa0law and guidance).'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 53, 'page_label': '52'}, page_content='AI governance and human rights\\nResetting the relationship\\n52 Chatham House\\nFor the UN and other international/regional organizations:\\n — Adopt consensus principles on AI and human rights that clarify the duties of \\nstates and responsibilities of companies in this field, as well as the requirements \\nfor remedy. Publish a sister document to the UN’s Guiding Principles on \\nBusiness and Human Rights to outline these principles, accessible to all \\nstakeholders including software developers and engineers;\\n — Establish a new multi-stakeholder forum that brings together the tech and \\nhuman rights communities, as well as technical standards bodies, to discuss \\nchallenges around the interaction of human rights and technology, including \\nAI.215 A regular, institutionalized dialogue would raise levels of understanding \\nand cooperation on all sides of the debate, and would help prevent business \\nexploitation of legal grey areas;216\\n — Ensure, via the UN secretary-general’s envoy on technology, that all parts of the \\nUN (including technical standards bodies and procurement offices) align with \\nthe OHCHR in placing human rights at the centre of their work on technology;\\n — Continue to promote UNESCO’s Recommendation on the Ethics of Artificial \\nIntelligence, including the international human rights obligations and \\ncommitments to which it refers, facilitating knowledge-sharing and capacity-\\nbuilding to enable effective implementation in all states;\\n — Advance dialogue and coherent approaches to the implications of AI for human \\nrights, via treaties or soft law, and support national governments in their \\ngovernance of AI;\\n — Conduct human rights due diligence before deploying AI; and\\n — Integrate AI into development and capacity-building activities to accelerate \\nimplementation of the SDGs.\\nFor civil society and academics:\\n — Push for inclusion in the AI governance conversation, including by fostering \\nconnections with the software development community and corporate \\npublic policy teams;\\n — Debunk human rights myths. Explain to a wide array of audiences (including \\nbusiness leaders, investors and governments) that human rights are reasonable \\nnot radical; that human rights do not stymie innovation but establish a level \\nplaying field in guarding against egregious development.\\n — Demonstrate the positive role of human rights as a regulatory system by \\nreference to existing processes of human rights due diligence and remedy;\\n215 As discussed at the Digital Democracy Dialogue 3D2, in Montreux, Switzerland, in November 2021. \\nSee\\xa0also\\xa0Universal Rights Group (2021), Placing Digital Technology at the Service of Democracy and Human Rights, \\nhttps://www.universal-rights.org/wp-content/uploads/2021/12/3D2_designed-report_V1.pdf.\\n216 Human Rights Council Advisory Committee (2021), Possible impacts, opportunities and challenges of new \\nand emerging digital technologies with regard to the promotion and protection of human rights, A/HRC/47/52, \\nhttps://documents-dds-ny.un.org/doc/UNDOC/GEN/G21/110/34/PDF/G2111034.pdf?OpenElement, para. 55.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 54, 'page_label': '53'}, page_content='AI governance and human rights\\nResetting the relationship\\n53 Chatham House\\n — Encourage inter-disciplinary engagement at universities and raise \\nawareness of human rights in technology-focused studies\\xa0– for example, \\nby\\xa0introducing human rights as an element of computer science degrees \\nand\\xa0coding ‘bootcamps’;\\n — Facilitate collaboration between civil society and the software development \\ncommunity on the development and use of AI to achieve of the SDGs; and\\n — Test the implications of human rights for AI through strategic litigation.\\nFor investors:\\n — Include assessment of the implications of AI for human rights in ESG \\nor\\xa0equivalent investment metrics.217\\n217 Minkkinen, M., Niukkanen, A., and Mäntymäki, M. (2022), ‘What about investors? ESG analyses as tools \\nfor\\xa0ethics-based AI auditing’, AI & Society, https://doi.org/10.1007/s00146-022-01415-0.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 55, 'page_label': '54'}, page_content='AI governance and human rights\\nResetting the relationship\\n54 Chatham House\\nAbout the author\\nKate Jones is an associate fellow with Chatham House’s International Law \\nProgramme. She is a consultant and researcher on human rights law, public \\ninternational law, governance and diplomacy, focusing on their intersection with \\ntechnology. With more than 20 years of legal experience, Kate has published and \\nspoken widely on aspects of tech governance, with specialisms in disinformation \\nand foreign interference, artificial intelligence and human rights. Her publications \\nat Chatham House include the 2019 research paper\\xa0Online Disinformation and \\nPolitical Discourse: Applying a Human Rights Framework.\\nAcknowledgments\\nThis research paper is published as part of the Human Rights Pathways initiative \\nof\\xa0Chatham House, funded by the Swiss Federal Department of Foreign Affairs.\\nThanks are due to all those whose ideas and comments have helped shape the \\npaper. This includes those who generously agreed to be interviewed; all the \\nparticipants in a Chatham House roundtable on Artificial Intelligence and Human \\nRights, convened with kind cooperation of the Geneva Human Rights Platform \\nat\\xa0the Villa Moynier, Geneva in May 2022; and all who attended a London meeting \\non AI and human rights kindly convened by the European Center for Not-for-Profit \\nLaw (ECNL) in June 2022.\\nThe author is grateful to all at Chatham House who have contributed to the \\ncontent, editing and publication of the paper, including Harriet Moynihan, \\nChanu Peiris, Rashmin Sagoo, Elizabeth Wilmshurst KC, Marjorie Buchser, David \\nGriffiths, Rowan Wilkinson, Rachel Mullally, Sophia Rose and Chris Matthews. \\nShe would also like to thank those outside Chatham House who reviewed drafts: \\nVanja Škorić of ECNL, Janis Wong of the Alan Turing Institute and the anonymous \\npeer reviewers.\\nFinally, thanks go to many others for interesting conversations and debates on this \\ntopic over the last year, including Lukas Madl and the advisory board of Innovethic, \\nChristian Hunt and his Human Risk Podcast, and the Digital Society Initiative \\nat Chatham House.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 56, 'page_label': '55'}, page_content='All rights reserved. No part of this publication may be reproduced or transmitted in any\\xa0form or by any means, \\nelectronic or mechanical including photocopying, recording or any information storage or retrieval system, \\nwithout the prior written permission of the\\xa0copyright holder. Please direct all enquiries\\xa0to the publishers.\\nChatham House does not express opinions of its own. The opinions expressed in this publication \\nare\\xa0the\\xa0responsibility of the author(s).\\nCopyright © The Royal Institute of International Affairs, 2023\\nCover image: An attendee tries a virtual reality experience during the Mobile World Congress trade show \\nin\\xa0Barcelona, Spain on 3 March 2022.\\nPhoto credit: Copyright © Joan Cros/NurPhoto/Getty Images\\nISBN 978 1 78413 549 2 \\nDOI 10.55317/9781784135492\\nCite this paper: Jones, K. (2023), AI governance and human rights: Resetting the relationship, Research Paper, \\nLondon: Royal Institute of International Affairs, https:/ /doi.org/10.55317/9781784135492.\\nThis publication is printed on FSC-certified paper.\\ndesignbysoapbox.com'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 57, 'page_label': '56'}, page_content='The Royal Institute of International Affairs \\nChatham House\\n10 St James’s Square, London SW1Y 4LE \\nT +44 (0)20 7957 5700 \\ncontact@chathamhouse.org  |  chathamhouse.org\\nCharity Registration Number: 208223\\nIndependent thinking since 1920')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader \n",
    "reader = PyPDFLoader(file_path=\"./data/AI_Governance.pdf\")\n",
    "doc = reader.load()\n",
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunking or Splitting..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': './data/AI_Governance.pdf', 'page': 0, 'page_label': 'i'}, page_content='Research \\nPaper\\nAI governance \\nand\\xa0human\\xa0rights\\nResetting the relationship\\nKate Jones\\nInternational Law \\nProgramme  \\nJanuary 2023'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 1, 'page_label': 'ii'}, page_content='Chatham House, the Royal Institute of International  \\nAffairs, is a world-leading policy institute based in\\xa0London. \\nOur mission is to help governments and societies build \\na\\xa0sustainably secure, prosperous and\\xa0just world.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 2, 'page_label': '1'}, page_content='1 Chatham House\\nContents\\n Summary 2\\n01 Introduction 3\\n02 What is AI? 5\\n03 Governing AI: why human rights? 9\\n04 Principles of AI governance: the contribution of human rights 21\\n05 Processes of AI governance: the contribution of human rights 34\\n06 Remedies in AI\\xa0governance: the\\xa0contribution of\\xa0human rights 44\\n07 Conclusion and recommendations 50\\n About the author 54\\n Acknowledgments 54'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 3, 'page_label': '2'}, page_content='2 Chatham House\\nSummary\\n — Artificial intelligence (AI) is redefining what it means to be human. Human rights \\nhave so far been largely overlooked in the governance of AI – particularly in the \\nUK and the US. This is an error and requires urgent correction.\\n — While human rights do not hold all the answers, they ought to be the baseline for \\nAI\\xa0governance. International human rights law is a crystallization of ethical principles \\ninto norms, their meanings and implications well-developed over the last 70 years. \\nThese norms command high international consensus, are relatively clear, and can \\nbe developed to account for new situations. They offer a well-calibrated method of \\nbalancing the rights of the individual against competing rights and interests using tests \\nof necessity and proportionality. Human rights provide processes of governance for \\nbusiness and governments, and an ecosystem for provision of remedy for breaches.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 3, 'page_label': '2'}, page_content='of necessity and proportionality. Human rights provide processes of governance for \\nbusiness and governments, and an ecosystem for provision of remedy for breaches.\\n — The omission of human rights has arisen in part because those with human \\nrights\\xa0expertise are often not included in AI governance, both in companies \\nand\\xa0in governments. Various myths about human rights have also contributed \\nto\\xa0their being overlooked: human rights are wrongly perceived as adding little to \\nethics; as preventing innovation; as being overly complex, vague, old-fashioned \\nor\\xa0radical; or as only concerning governments.\\n — Companies, governments and civil society are retreading the territory of human \\nrights with a new proliferation of AI ethics principles and compliance assessment \\nmethods. As a result, businesses developing or purchasing AI do not know what \\nstandards they should meet, and may find it difficult to justify the costs of ethical'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 3, 'page_label': '2'}, page_content='methods. As a result, businesses developing or purchasing AI do not know what \\nstandards they should meet, and may find it difficult to justify the costs of ethical \\nprocesses when competitors have no obligation to do the same. Meanwhile, \\nindividuals do not know what standards they can expect from AI affecting them \\nand often have no means of complaint. Consequently, many people do not trust \\nAI: they suspect that it may be biased or unfair, that it could be spying on them \\nor\\xa0manipulating their choices.\\n — The human rights to privacy and data protection, equality and non-discrimination \\nare key to the governance of AI, as are human rights’ protection of autonomy \\nand of economic, social and cultural rights in ensuring that AI will benefit \\neveryone. Human rights law imposes not only duties on governments to uphold, \\nbut also responsibilities on companies and organizations to comply, as well as \\nrequirements for legal remedies and reparation of harms.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 3, 'page_label': '2'}, page_content='but also responsibilities on companies and organizations to comply, as well as \\nrequirements for legal remedies and reparation of harms.\\n — Companies and investors, governments, international organizations and civil \\nsociety should take steps to establish human rights as the foundation on which \\nAI\\xa0governance is built, including through inclusive discussion, championing \\nhuman rights and establishing standards and processes for implementation \\nof\\xa0human rights law and remedy in case of breach.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 4, 'page_label': '3'}, page_content='3 Chatham House\\n01 \\nIntroduction\\nAI is redefining what it means to be human. As existing \\ninternational norms designed to allow every human being a life \\nof liberty and dignity, human rights ought to be the foundation \\nfor AI governance.\\nHuman rights are central to what it means to be human. They were drafted \\nand\\xa0agreed internationally, with worldwide popular support, to define freedoms \\nand\\xa0entitlements that would allow every human being to live a life of liberty and \\ndignity. Those fundamental human rights have been interpreted and developed \\nover decades to delineate the parameters of fairness, equality and liberty for \\nevery individual.\\nNow, artificial intelligence (AI) is redefining what it means to be human. \\nIts systems\\xa0and processes have the potential to alter the human experience \\nfundamentally. AI will affect not only public policy areas such as road safety and \\nhealthcare, but also human autonomy, relationships and dignity. It will affect'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 4, 'page_label': '3'}, page_content='fundamentally. AI will affect not only public policy areas such as road safety and \\nhealthcare, but also human autonomy, relationships and dignity. It will affect \\nlifestyles and professions, as well as the future course of human development \\nand the nature and scale of conflicts. It will change the relationships between \\ncommunities and those between the individual, the state and corporations. \\nAI offers tremendous benefits for all societies but also presents risks. These risks \\npotentially include further division between the privileged and the unprivileged; \\nthe erosion of individual freedoms through ubiquitous surveillance; and the \\nreplacement of independent thought and judgement with\\xa0automated control.\\nThis paper aims to explain why human rights ought to be the foundation for \\nAI governance, to explore the reasons why they are not\\xa0– except in the EU and \\nsome international organizations\\xa0– and to demonstrate how human rights can'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 4, 'page_label': '3'}, page_content='AI governance, to explore the reasons why they are not\\xa0– except in the EU and \\nsome international organizations\\xa0– and to demonstrate how human rights can \\nbe\\xa0embedded from the beginning in future AI governance initiatives.\\nWhile AI is being implemented rapidly around the world, most governance \\ninitiatives to date have emerged from developed states. This paper therefore \\nfocuses on practice and process primarily in the EU, the UK and the US. However, \\nthe paper also acknowledges the significance of AI initiatives elsewhere in the \\nworld\\xa0– China in particular is a leading developer and exporter of AI technology.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 5, 'page_label': '4'}, page_content='AI governance and human rights\\nResetting the relationship\\n4 Chatham House\\nThe following chapter explains AI and the risks and benefits it presents \\nfor human\\xa0rights. Chapter\\xa03 aims to dispel myths and fears about human \\nrights, before discussing why human rights should provide the baseline for AI \\ngovernance. Chapters 4, 5 and 6 outline the principal import of human rights \\nfor AI governance principles, processes and remedies respectively. Finally, \\nChapter\\xa07 offers recommendations on actions that governments, organizations, \\ncompanies and individuals can take to ensure that human rights are embedded \\nin\\xa0AI\\xa0governance in future.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 6, 'page_label': '5'}, page_content='5 Chatham House\\n02 \\nWhat is AI?\\nAI has capacity to transform human life\\xa0– \\nboth\\xa0for\\xa0better and for worse.\\nAI is increasingly present in our lives, and its impact will expand significantly \\nin the\\xa0coming years. From predictive text, to social media news feeds, to virtual \\nhomes and mobile phone voice assistants, AI is already a part of everyday life. \\nAI\\xa0offers automated translation, assists shoppers buying online and recommends \\nthe fastest route on the drive home. It is also a key component of much-debated, \\nrapidly developing technologies such as facial recognition and self-driving vehicles.\\nThere is no single agreed definition of AI: it is a general term referring to \\nmachines’\\xa0evolving capacity to take on tasks requiring some form of intelligence. \\nThe tasks that AI performs can include generating predictions, making decisions \\nand providing recommendations.1 This means that AI may make decisions itself, \\nor\\xa0provide information for use in human decision-making.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 6, 'page_label': '5'}, page_content='and providing recommendations.1 This means that AI may make decisions itself, \\nor\\xa0provide information for use in human decision-making.\\nAI systems are algorithmic\\xa0– the algorithm being the computational process or \\nset of rules that the computer follows to calculate a result. To learn, AI generally \\nrelies on synthesising and making inferences from large quantities of data. \\nIt\\xa0is\\xa0the machine’s capacity to learn by itself how to do tasks better, rather than \\nsimply following instructions, that distinguishes AI from traditional computer \\nprogrammes. Contrary to popular myth, self-improvement does not prevent \\nAI\\xa0from being constrained by rules.\\nGovernments are among the largest adopters of AI, deploying it to assist in \\nmaking\\xa0decisions that can have major consequences for the lives of individual \\ncitizens. For example, governments are using AI to assist with decisions on \\nentitlement to immigration status, welfare benefits, school entry and priority'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 6, 'page_label': '5'}, page_content='citizens. For example, governments are using AI to assist with decisions on \\nentitlement to immigration status, welfare benefits, school entry and priority \\n1 The European Commission’s High-Level Expert Group on Artificial Intelligence offers a fuller definition: \\n‘Artificial intelligence (AI) systems are software (and possibly also hardware) systems designed by humans \\nthat, given a complex goal, act in the physical or digital dimension by perceiving their environment through \\ndata acquisition, interpreting the collected structured or unstructured data, reasoning on the knowledge, or \\nprocessing the information, derived from this data and deciding the best action(s) to take to achieve the given \\ngoal. AI systems can either use symbolic rules or learn a numeric model, and they can also adapt their behaviour \\nby analysing how the environment is affected by their previous actions.’ Independent High-Level Expert'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 6, 'page_label': '5'}, page_content='by analysing how the environment is affected by their previous actions.’ Independent High-Level Expert \\nGroup on Artificial Intelligence (2019), Ethics Guidelines for Trustworthy AI, Brussels: European Commission, \\nhttps://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 7, 'page_label': '6'}, page_content='AI governance and human rights\\nResetting the relationship\\n6 Chatham House\\nvaccinations. They are adopting it to assist with provision of justice, in both civil \\nand criminal processes. And they may be using AI to assist in delivery of critical \\ninfrastructure and national security.\\nAI is likely to pervade almost every domain of human activity, and to become \\nincreasingly important as technology evolves towards greater interoperability, \\nincluding through the development of the metaverse.2 This paper discusses general \\nfeatures of AI, but by no means diminishes the need for parallel sector-specific \\ndiscussion. The use of AI in the healthcare system, in social media or in the criminal \\njustice process, for instance, each raise specific human rights issues that need \\nto\\xa0be\\xa0addressed in context, alongside the overarching issues discussed here.\\n2.1 What potential does AI hold for human \\nrights\\xa0and the common good?'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 7, 'page_label': '6'}, page_content='to\\xa0be\\xa0addressed in context, alongside the overarching issues discussed here.\\n2.1 What potential does AI hold for human \\nrights\\xa0and the common good?\\nDue to its speed and its power of self-learning, AI has the capacity to transform \\nour\\xa0societies. It can operate faster\\xa0– and potentially better\\xa0– than any human. \\nIt\\xa0can\\xa0achieve scientific breakthroughs, calculate fair distributions and outcomes, \\nand\\xa0make more accurate predictions.\\nAI holds enormous potential to enable human development and flourishing. \\nFor\\xa0example, AI is accelerating the battle against disease3 and mitigating the \\nimpact of disability;4 it is helping to tackle climate change5 and optimize efficiency \\nin agriculture;6 it can assist distribution of humanitarian aid;7 it has enormous \\npotential for improving access to, and quality of, education globally;8 and it can \\ntransform public and private transport.9 AI could help to ensure that policing is'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 7, 'page_label': '6'}, page_content='potential for improving access to, and quality of, education globally;8 and it can \\ntransform public and private transport.9 AI could help to ensure that policing is \\nfair and respectful of human dignity. It may make workplaces more productive, \\nreduce the load of manual labour and help developed countries to manage the \\nchallenges of an ageing population. To give a specific example of the benefits, \\nthe AI programme AlphaFold is predicting the structures of both human and \\nanimal\\xa0proteins with tremendous speed and remarkable accuracy, with potentially \\ntransformative effects on medical treatments, crop science and plastic reduction.10\\n2 Moynihan, H., Buchser, M. and Wallace, J. (2022), What is the metaverse?, Explainer, London: Royal Institute \\nof\\xa0International Affairs, https://www.chathamhouse.org/2022/04/what-metaverse.\\n3 For example, as regards COVID-19: Soomro, T. A. et al. (2021), ‘Artificial intelligence (AI) for medical imaging'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 7, 'page_label': '6'}, page_content='of\\xa0International Affairs, https://www.chathamhouse.org/2022/04/what-metaverse.\\n3 For example, as regards COVID-19: Soomro, T. A. et al. (2021), ‘Artificial intelligence (AI) for medical imaging \\nto combat coronavirus disease (COVID-19): a detailed review with direction for future research’, Artificial \\nIntelligence Review, 55(2), pp. 1409–39, https://doi.org/10.1007%2Fs10462-021-09985-z.\\n4 For example: Microsoft (undated), ‘AI for Accessibility’, https://www.microsoft.com/en-us/ai/\\nai-for-accessibility.\\n5 Rolnick, D. et al. (2019), ‘Tackling Climate Change with Machine Learning’, arXiv, 1906.05433v2 [cs.CY], \\nhttps://arxiv.org/pdf/1906.05433.pdf.\\n6 Cline, T. (2019), ‘Digital agriculture: making the most of machine learning on farm’, Spore, https://spore.cta.\\nint/en/dossiers/article/digital-agriculture-making-the-most-of-machine-learning-on-farm-sid0dbfbb123-30b2-\\n48fd-830e-71312f66af04?msclkid=d9322204a57311ecb7a36f2895e35dd1.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 7, 'page_label': '6'}, page_content='int/en/dossiers/article/digital-agriculture-making-the-most-of-machine-learning-on-farm-sid0dbfbb123-30b2-\\n48fd-830e-71312f66af04?msclkid=d9322204a57311ecb7a36f2895e35dd1.\\n7 For example: UN Global Pulse (2022), ‘Innovating Together for our Common Future’, www.unglobalpulse.org.\\n8 For example: UNESCO (2022), ‘Artificial Intelligence and the Futures of Learning’, https://en.unesco.org/\\nthemes/ict-education/ai-futures-learning.\\n9 For example, European Parliament Briefing (2019), ‘Artificial Intelligence in Transport: Current and \\nFuture Developments, Opportunities and Challenges’, https://www.europarl.europa.eu/RegData/etudes/\\nBRIE/2019/635609/EPRS_BRI(2019)635609_EN.pdf?msclkid=cd1a70d2aaa011ec9f9ff79af4f9d88d.\\n10 Tunyasuvunakool, K. et al. (2021), ‘Highly accurate protein structure prediction for the human proteome’, \\nNature, 596, 21 July 2021, pp. 590–96, https://doi.org/10.1038/s41586-021-03828-1.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 8, 'page_label': '7'}, page_content='AI governance and human rights\\nResetting the relationship\\n7 Chatham House\\nIn short, when properly managed, AI can enable delivery of the UN’s Sustainable \\nDevelopment Goals (SDGs) by the 2030 deadline,11 boost the implementation \\nof economic, social and cultural rights worldwide, and support improvements \\nin\\xa0many areas of life.\\nTo achieve these aims, AI must be harnessed for the good of all societies. Doing \\nso\\xa0involves not only goodwill, but also ensuring that commercial considerations \\ndo not dictate the development of AI entirely. Provision of funding for AI research \\nand development outside the commercial sector will be invaluable, as will access \\nto\\xa0data for AI developers such that they may generate applications of AI that benefit \\npeople in all communities.\\nJust as the industrial revolution brought progress at the expense of upheaval \\nin\\xa0traditional ways of living, so will AI bring change to our societies. Work must'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 8, 'page_label': '7'}, page_content='people in all communities.\\nJust as the industrial revolution brought progress at the expense of upheaval \\nin\\xa0traditional ways of living, so will AI bring change to our societies. Work must \\nbe\\xa0done now to mitigate the risk of negative impacts. Governments must anticipate \\nand manage the changes that widespread use of AI will herald. They must consider \\nboth the implications of AI for their own public policymaking, which may be \\nsubject to judicial review, and how to govern a society in which AI is increasingly \\nbeing developed by the private sector and becoming a feature of life for the world’s \\npopulation. This includes governance not only of AI itself but of its implications \\nfor current ways of life. For example, governments should address the risk that \\nAI will upend current practices and norms in the workplace, through mass \\nunemployment and an undermining of bargaining power between employers and \\nemployees. Governments should be taking active steps to ensure the benefits of AI'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 8, 'page_label': '7'}, page_content='unemployment and an undermining of bargaining power between employers and \\nemployees. Governments should be taking active steps to ensure the benefits of AI \\nare distributed equitably, avoiding the division of society into ‘winners’ and ‘losers’ \\nfrom emerging technology. To preserve and promote public interest, governments \\nmust not allow companies to develop AI in a policy and regulatory vacuum.\\n2.2 What are the key human rights \\nand\\xa0ethical\\xa0challenges posed by AI?\\nEvidence abounds of problematic uses of AI. At one end of the spectrum, AI is being \\ndeliberately used as a tool of suppression: for example, the Chinese government’s \\nuse of AI to conduct mass surveillance of its Uyghur minority.12 Some types of AI \\ncould be used deliberately to limit people’s freedom to express themselves and to \\nmeet with others, to monitor the general public for compliance with behavioural \\nrules,13 to detect ‘suspicious behaviour’14 or to restrict access to society’s benefits \\nto\\xa0a privileged few.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 8, 'page_label': '7'}, page_content='meet with others, to monitor the general public for compliance with behavioural \\nrules,13 to detect ‘suspicious behaviour’14 or to restrict access to society’s benefits \\nto\\xa0a privileged few.\\n11 Vinuesa, R. et al. (2020), ‘The role of artificial intelligence in achieving the Sustainable Development \\nGoals’, Nature Communications, 11(233), https://doi.org/10.1038/s41467-019-14108-y; Chui, M. et al. \\n(2019), ‘Using AI to help achieve Sustainable Development Goals’, New York: UN Development Programme, \\nhttps://www.undp.org/blog/using-ai-help-achieve-sustainable-development-goals.\\n12  Mozur, P. (2019), ‘One Month, 500,000 Face Scans: How China is using AI to profile a minority’, New York \\nTimes, 14 April 2019, https://www.nytimes.com/2019/04/14/technology/china-surveillance-artificial-\\nintelligence-racial-profiling.html.\\n13  Heikkila, M. (2021), ‘The rise of AI surveillance’, Politico, 26 May 2021, https://www.politico.eu/article/'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 8, 'page_label': '7'}, page_content='intelligence-racial-profiling.html.\\n13  Heikkila, M. (2021), ‘The rise of AI surveillance’, Politico, 26 May 2021, https://www.politico.eu/article/\\nthe-rise-of-ai-surveillance-coronavirus-data-collection-tracking-facial-recognition-monitoring.\\n14 Vinocur, N. (2020), ‘French politicians urge deployment of surveillance technology after series of attacks’, \\nPolitico, 30 October 2020, https://www.politico.eu/article/french-politicians-urge-deployment-of-surveillance-\\ntechnology-after-series-of-attacks.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 9, 'page_label': '8'}, page_content='AI governance and human rights\\nResetting the relationship\\n8 Chatham House\\nMany AI tools abuse human rights as a collateral consequence of their operation. \\nAI risks embedding and exaggerating bias and discrimination, invading privacy, \\nreducing personal autonomy and making society more, rather than less, unequal. \\nFor example, AI sentencing tools may discriminate against minorities, potentially \\nturning back decades of progress towards equality. AI in healthcare may harm \\nhuman health if algorithms are incorrect or biased,15 while AI in welfare-provision \\nor migration may make unfair decisions on eligibility. AI tools may infer sensitive \\ninformation about individuals in violation of their privacy.\\nEven an AI tool designed with the intention of implementing scrupulous standards \\nof fairness will fail if it does not replicate the complex range of factors and subtle, \\ncontext-specific decision-making processes of humans. Unchecked, AI systems'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 9, 'page_label': '8'}, page_content='of fairness will fail if it does not replicate the complex range of factors and subtle, \\ncontext-specific decision-making processes of humans. Unchecked, AI systems \\ntend to exacerbate structural imbalances of power and to disadvantage the most \\nmarginalized in society.16\\nFurther, some AI tools may have outputs detrimental to humanity through their \\npotential to shape human experience of the world. For example, AI algorithms \\nin social media may, by distorting the availability of information, manipulate \\naudience views in violation of the rights to freedom of thought and opinion,17 or \\nprioritize content that incites hatred and violence between social groups.18 AI used \\nto detect aptitudes or to select people for jobs, while intended to broaden human \\nhorizons and ambition, risks doing the opposite. Without safeguards, AI is likely \\nto entrench and exaggerate social divides and divisions, distort our impressions'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 9, 'page_label': '8'}, page_content='horizons and ambition, risks doing the opposite. Without safeguards, AI is likely \\nto entrench and exaggerate social divides and divisions, distort our impressions \\nof\\xa0the world and thus have negative consequences on aspects of human life. These \\nrisks are amplified by the difficulty of identifying when AI fails, for example when \\nit is malfunctioning, manipulative, acting illegally or making unfair decisions. \\nAt\\xa0present, companies rarely make public their identification of mistakes or errors \\nin their AI. Consumers cannot therefore see which standards have been met.\\nFinally, AI may entrench and even exacerbate social divides between rich \\nand poor, worsening the situation of the most vulnerable. As AI development \\nand implementation is largely driven by the commercial sector, it risks being \\nharnessed for the benefit of those who can pay rather than to resolve the world’s \\nmost significant challenges, and risks being deployed in ways that further'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 9, 'page_label': '8'}, page_content='harnessed for the benefit of those who can pay rather than to resolve the world’s \\nmost significant challenges, and risks being deployed in ways that further \\ndispossess\\xa0vulnerable communities around the world.19\\n15 Park, Y. et al. (2020), ‘Evaluating artificial intelligence in medicine: phases of clinical research’, \\nJAMIA\\xa0Open,\\xa03(3), October 2020, pp. 326–31, https://doi.org/10.1093/jamiaopen/ooaa033.\\n16 European Digital Rights (EDRi) et al. (2021), Civil Society Statement on an EU Artificial Intelligence Act for \\nFundamental Rights, 30 November 2021, https://edri.org/wp-content/uploads/2021/12/Political-statement-on-\\nAI-Act.pdf; Kalluri, P. (2020), ‘Don’t ask if artificial intelligence is good or fair, ask how it shifts power’, Nature, \\n7\\xa0July 2020, https://www.nature.com/articles/d41586-020-02003-2.\\n17 Jones, K. (2019), Online Disinformation and Political Discourse: Applying a Human Rights Framework,'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 9, 'page_label': '8'}, page_content='7\\xa0July 2020, https://www.nature.com/articles/d41586-020-02003-2.\\n17 Jones, K. (2019), Online Disinformation and Political Discourse: Applying a Human Rights Framework, \\nResearch\\xa0Paper, London: Royal Institute of International Affairs, https://www.chathamhouse.org/2019/11/\\nonline-disinformation-and-political-discourse-applying-human-rights-framework.\\n18 Kornbluh, K. (2022), ‘Disinformation, Radicalization, and Algorithmic Amplification: What Steps Can Congress \\nTake?’, Just Security blog, 7 February 2022, https://www.justsecurity.org/79995/disinformation-radicalization-\\nand-algorithmic-amplification-what-steps-can-congress-take.\\n19 Hao, K. (2022), ‘AI Colonialism’, MIT Technology Review, 19 April 2022, \\nhttps://www.technologyreview.com/2022/04/19/1049592/artificial-intelligence-colonialism.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 10, 'page_label': '9'}, page_content='9 Chatham House\\n03 \\nGoverning AI:  \\nwhy human rights?\\nHuman rights have been wrongly overlooked in AI \\ngovernance discussions. They\\xa0offer clarity and specificity, \\ninternational acceptance and legitimacy, and mechanisms \\nfor\\xa0implementation, oversight and accountability.\\nIn the 1940s, there was fervent belief that human rights would be central to \\nworld peace and to human flourishing, key not only to safeguarding humanity \\nfrom catastrophe but to the enjoyment of everyday life.20 Supporters of the ‘vast \\nmovement of public opinion’21 in favour of human rights at that time would be \\namazed at their relative absence from today’s debate on AI.\\n3.1 Human rights overlooked\\nAI governance has much to gain from a multidisciplinary (and potentially \\ninterdisciplinary) approach, drawing from, among others, philosophy, human rights \\nlaw, science and technology studies, sociology, statistics, diverse impact assessment \\nand audit practices and stakeholder theory. However, with some exceptions,22'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 10, 'page_label': '9'}, page_content='law, science and technology studies, sociology, statistics, diverse impact assessment \\nand audit practices and stakeholder theory. However, with some exceptions,22 \\n20 See David Maxwell Fyfe’s closing speech for the UK prosecution at Nuremberg, available at \\n‘The\\xa0Human’s\\xa0In\\xa0the\\xa0Telling’, https://thehumansinthetelling.wordpress.com.\\n21 René Brunet, former delegate to the League of Nations, quoted in Lauren, P.G. (2011), The Evolution \\nof\\xa0International Human Rights: Visions Seen, 3rd edn, Philadelphia: University of Pennsylvania Press, p. 153.\\n22 Exceptions include the EU’s Artificial Intelligence Act (discussed below) and academic texts including: \\nMcGregor,\\xa0L., Murray, D. and Ng, V. (2019), ‘International Human Rights Law as a Framework for \\nAlgorithmic Accountability’, International & Comparative Law Quarterly, 68(2), April 2019, pp. 309–43, \\nhttps://doi.org/10.1017/S0020589319000046; and Yeung, K., Howes, A. and Pogrebna, G. (2019), ‘AI\\xa0Governance'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 10, 'page_label': '9'}, page_content='https://doi.org/10.1017/S0020589319000046; and Yeung, K., Howes, A. and Pogrebna, G. (2019), ‘AI\\xa0Governance \\nby Human Rights-Centred Design, Deliberation and Oversight: an end to Ethics Washing’, in\\xa0Dubber, M. and \\nPasquale, F. (eds) (2019), The Oxford Handbook of AI Ethics, Oxford: Oxford University Press. The White House’s \\nrecent ‘Blueprint for an AI Bill of Rights’ helpfully introduces the language of rights into mainstream AI governance \\nin the US, albeit without focusing directly on the existing international human rights framework. See The White \\nHouse Office of Science and Technology Policy (2022), ’Blueprint for an AI\\xa0Bill of Rights: Making Automated \\nSystems Work For The American People’, https://www.whitehouse.gov/ostp/ai-bill-of-rights.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 11, 'page_label': '10'}, page_content='AI governance and human rights\\nResetting the relationship\\n10 Chatham House\\nthe\\xa0human rights framework has been overlooked as an existing and flexible \\nbaseline\\xa0for AI governance.\\nAI governance initiatives are often branded as ‘AI ethics’, ‘responsible AI’ or ‘value \\nsensitive design’. Some of these initiatives, such as the Asilomar AI Principles,23 \\nare\\xa0statements drawn primarily from the philosophical discipline of ethics. Many are \\nmultidisciplinary statements of principle, and so may include human rights law as an \\naspect of ‘ethics’. For example, the UNESCO Recommendation on the Ethics of Artificial \\nIntelligence lists ‘[r]espect, protection and promotion of human rights and fundamental \\nfreedoms and human dignity’ as the first of its ‘values’ to be respected by all actors in the \\nAI system life cycle.24 And the Institute of Electrical and Electronics Engineers (IEEE)’s \\nStandard Model Process for Addressing Ethical Concerns during System Design lists'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 11, 'page_label': '10'}, page_content='AI system life cycle.24 And the Institute of Electrical and Electronics Engineers (IEEE)’s \\nStandard Model Process for Addressing Ethical Concerns during System Design lists \\nas\\xa0its first ‘ethical principle’ that ‘[h]uman rights are to be protected’.25\\nMany sets of AI governance principles produced by companies, governments, \\ncivil\\xa0society and international organizations fail to mention human rights \\nat all. Of\\xa0those that do, only a small proportion (around 15\\xa0per\\xa0cent) take \\nhuman rights as a\\xa0framework.26 Most national AI strategies do not engage \\nwith\\xa0human\\xa0rights in depth.27\\nSo why, then, are human rights not central to AI governance?\\nFirst, in many arenas, human rights are simply omitted from discussions on AI \\ngovernance. Software developers and others in the AI industry generally do not \\ninvolve anyone from the human rights community in discussions on responsible \\nAI. There is a marked lack of human rights-focused papers or panels at the largest'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 11, 'page_label': '10'}, page_content='involve anyone from the human rights community in discussions on responsible \\nAI. There is a marked lack of human rights-focused papers or panels at the largest \\n23  Future of Life Institute (2017), Asilomar AI Principles, https://futureoflife.org/2017/08/11/ai-principles.\\n24  UN Educational, Scientific and Cultural Organization (2021), Recommendation on the Ethics of Artificial \\nIntelligence, Paris: UNESCO, https://unesdoc.unesco.org/ark:/48223/pf0000381137, section III.1.\\n25  Institute of Electrical and Electronics Engineers (IEEE), Standard Model Process for Addressing Ethical \\nConcerns during System Design, IEEE Std 7000-2021, Annex H.\\n26 A 2020 review of 36 prominent sets of AI principles from around the world, authored by a diverse range \\nof\\xa0governmental and non-governmental bodies, found that only 23 referred to international human rights. \\nOnly one-half of the government documents reviewed include any reference to human rights. Five of the 36\\xa0sets'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 11, 'page_label': '10'}, page_content='Only one-half of the government documents reviewed include any reference to human rights. Five of the 36\\xa0sets \\nof AI\\xa0principles used international human rights as a framework for their work. See Fjeld, J. et al. (2020), \\nPrincipled Artificial Intelligence: Mapping Consensus in Ethical and Rights-based Approaches to Principles for\\xa0AI, \\nBerkman Klein Center for Internet & Society, Research Publication No. 2020-1, http://dx.doi.org/10.2139/\\nssrn.3518482. A separate evaluation of 22 sets of guidelines makes no reference to human rights: Hagendorff,\\xa0T. \\n(2020), ‘The\\xa0Ethics of AI Ethics: An Evaluation of Guidelines’, Minds and Machines, 30, pp. 99–120, \\nhttps://link.springer.com/article/10.1007/s11023-020-09517-8.\\n27 A review of all national AI strategies published by 1 January 2020 found that while a majority referred to human \\nrights, most only mentioned them in passing rather than engaging with them in depth. Only European states and'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 11, 'page_label': '10'}, page_content='rights, most only mentioned them in passing rather than engaging with them in depth. Only European states and \\nIndia referred to human rights; many East and South East Asian states had developed a\\xa0strategy, but these did \\nnot refer to human rights. None of the strategies reviewed came from Africa, and only one from Latin America \\n(Colombia). Global Partners Digital and Stanford Global Digital Policy Incubator (2020), National Artificial \\nIntelligence Strategies and Human Rights: A Review, https://www.gp-digital.org/wp-content/uploads/2020/04/\\nNational-Artifical-Intelligence-Strategies-and-Human-Rights%E2%80%94A-Review_April2020.pdf.\\nMany sets of AI governance principles produced \\nby\\xa0companies, governments, civil\\xa0society\\xa0and \\ninternational organizations fail\\xa0to\\xa0mention human \\nrights at all.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 12, 'page_label': '11'}, page_content='AI governance and human rights\\nResetting the relationship\\n11 Chatham House\\ninternational conferences on responsible AI.28 Corporate-level discussion of AI \\nethics\\xa0and their implementation often fails to refer to, or engage with, human rights. \\nJob advertisements for corporate AI ethics specialists usually make no reference \\nto human rights. Governments focused on AI ethics may not involve human rights \\nlawyers in policy development until a late stage, if at all. In contrast, human rights \\nare often the focus of civil society and academic discussions in different venues\\xa0– \\nand with different participants\\xa0– to those where corporate and public sector AI \\ngovernance decisions are made.29 Notable exceptions are discussions hosted by \\ninternational organizations such as the UN and the Council of Europe, where human \\nrights law forms a well-established shared lexicon; and the European Union, which \\nhas placed human rights at the core of the draft\\xa0Artificial Intelligence Act.30'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 12, 'page_label': '11'}, page_content='rights law forms a well-established shared lexicon; and the European Union, which \\nhas placed human rights at the core of the draft\\xa0Artificial Intelligence Act.30\\nSecond, certain myths about human rights can too often lead to them being \\ndisregarded by those involved in AI governance discussions. The following \\nare\\xa0some of the most common.\\n3.2 Myths about human rights\\nMyth 1. ‘Ethics holds all the answers’\\nEthics and human rights are distinct disciplines with valuable, complementary \\nroles to play in AI governance. Both ethics and human rights share the rationale \\nof curbing state and corporate power by acting as a bulwark of the interests of the \\nindividual. But they offer different, complementary means for reaching this end. \\nOne cannot substitute for the other or be considered at the exclusion of the other. \\nBoth disciplines must be considered together.\\nEthics plays an important role in preceding and supplementing regulation.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 12, 'page_label': '11'}, page_content='Both disciplines must be considered together.\\nEthics plays an important role in preceding and supplementing regulation. \\nIt has been the subject of much pioneering research and implementation in \\nthe field of AI governance. However, ethics is a branch of philosophy, not \\na\\xa0system of norms: multiple versions are possible, and\\xa0– despite, or perhaps \\nexacerbated by, the efforts to draft so many sets of AI ethics principles\\xa0– there is \\ncurrently a lack of international consensus as to what precisely AI ethics entails. \\nSignificant differences of both substance and terminology between these sets of \\nprinciples make it difficult for companies and public bodies to understand their \\nresponsibilities, and for individuals to know what standards to expect.\\n28 See the analysis of research contributions and shortcomings, including significant influence of industry, at \\nthe ACM Conference on Fairness, Accountability and Transparency: Laufer, B. et al. (2022), ‘Four years of FAccT:'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 12, 'page_label': '11'}, page_content='the ACM Conference on Fairness, Accountability and Transparency: Laufer, B. et al. (2022), ‘Four years of FAccT: \\nA\\xa0Reflexive, Mixed-Methods Analysis of Research Contributions, Shortcomings, and Future Prospects’, ACM \\nDigital Library, https://doi.org/10.1145/3531146.3533107.\\n29 Of over 180 papers accepted for the ACM Conference on Fairness, Accountability and Transparency 2022\\xa0– \\n‘a\\xa0computer science conference with a cross-disciplinary focus’\\xa0– only three refer to human rights in their abstract. ACM \\nFAccT (2022), ‘Accepted Papers’, https://facctconference.org/2022/acceptedpapers.html. In contrast, Access Now’s \\nRightsCon Conference 2022, on technology and human rights, included Artificial Intelligence as one of its programme \\ntracks; but only 11\\xa0per\\xa0cent of its attendees came from the private sector and 4\\xa0per\\xa0cent from government. RightsCon \\n(2022), Outcomes Report, https://www.rightscon.org/cms/assets/uploads/2022/09/Outcomes-Report-2022-v10.pdf.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 12, 'page_label': '11'}, page_content='(2022), Outcomes Report, https://www.rightscon.org/cms/assets/uploads/2022/09/Outcomes-Report-2022-v10.pdf.\\n30 Proposal for a Regulation of the European Parliament and of the Council laying down harmonised rules on \\nartificial intelligence (Artificial Intelligence Act) and amending certain Union legislative acts, COM/2021/206 \\nhttps://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX:52021PC0206.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 13, 'page_label': '12'}, page_content='AI governance and human rights\\nResetting the relationship\\n12 Chatham House\\nThe malleability of ethics means that it is difficult for civil society to hold \\nother\\xa0actors to account. Some technology companies face criticism for so-called \\n‘ethics-washing’ undertaken for reputational purposes,31 and for exerting undue \\ninfluence on some ethics researchers through funding.32 Courts and tribunals do \\nnot allocate remedies for compliance with ethics. Moreover, while ethical principles \\nare intended to ensure that technology reflects moral values, a focus on ethics \\nmay\\xa0minimize the appetite for legal regulation.33\\nAlthough in some environments, the branding of ‘ethics’ may be more palatable than \\nthat of human rights for political reasons, it is of primary importance that human \\nrights are considered at all – whatever the branding. To avoid conceptual confusion, \\nhuman rights ought to be regarded as parallel to ethics rather than as a mere element'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 13, 'page_label': '12'}, page_content='rights are considered at all – whatever the branding. To avoid conceptual confusion, \\nhuman rights ought to be regarded as parallel to ethics rather than as a mere element \\nof it. Any principles and processes of ethics should complement, rather than compete \\nwith, the existing human rights legal system. Conflicts between norms are damaging \\nas they undermine the legal certainty and predictability of regulatory behaviour \\non\\xa0which states, businesses and individuals rely.\\nCurrent popular support for AI ethics in principle, without a shared understanding \\nof what AI ethics means in practice, has similarities with support for human rights \\nin\\xa0the 1940s. There was widespread support then for the concept of human rights, to \\nprevent a repetition of the atrocities of the Second World War and to end domination \\nand repression. However, there was no specific understanding or consensus on what \\nexactly ‘human rights’ meant. Establishing agreement on the content of the Universal'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 13, 'page_label': '12'}, page_content='and repression. However, there was no specific understanding or consensus on what \\nexactly ‘human rights’ meant. Establishing agreement on the content of the Universal \\nDeclaration of Human Rights\\xa0– and later the International Covenant on Civil and \\nPolitical Rights and International Covenant on Economic, Social and Cultural \\nRights\\xa0– required worldwide canvassing, expert input, negotiation and political \\ncompromise.34 There is no evidence that reaching universal agreement on AI ethics \\nwithout reference to the already-agreed human rights framework would be easier, \\nor\\xa0less politically charged, than those 20th century debates.\\nMyth 2. ‘Human rights prevent innovation’\\nHuman rights do not prevent innovation or undermine a ‘move fast and break \\nthings’ ethos, save that they entail compliance with minimum standards and \\ntherefore forbid certain egregious activities. Most innovators want a level playing \\nfield, and to avoid being undercut by actors with lower standards or being caught'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 13, 'page_label': '12'}, page_content='therefore forbid certain egregious activities. Most innovators want a level playing \\nfield, and to avoid being undercut by actors with lower standards or being caught \\nin a ‘race to the bottom’ with unscrupulous competitors. Innovators want to know \\nhow they can meet shared standards and inspire trust in their products. Human \\nrights provide an appropriate basis for standards and processes internationally. \\nFor businesses, considering human rights from the outset of AI development \\nand\\xa0deployment may help to foster customer trust and minimize potential \\ncosts\\xa0and\\xa0time expended in litigation at a later stage.\\n31 Carnegie Council for Ethics in International Affairs (undated), ‘Ethics Washing’, \\nhttps://www.carnegiecouncil.org/explore-engage/key-terms/ethics-washing (accessed 12 Sep. 2022).\\n32  Williams, O. (2019), ‘How Big Tech funds the debate on AI ethics’, New Statesman, 6 June 2019, \\nhttps://www.newstatesman.com/science-tech/2019/06/how-big-tech-funds-debate-ai-ethics.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 13, 'page_label': '12'}, page_content='32  Williams, O. (2019), ‘How Big Tech funds the debate on AI ethics’, New Statesman, 6 June 2019, \\nhttps://www.newstatesman.com/science-tech/2019/06/how-big-tech-funds-debate-ai-ethics.\\n33  Wagner, B., (2018), ‘Ethics as an escape from regulation. From “ethics-washing” to ethics-shopping?’ \\nin\\xa0Bayamlioglu, E. et al. (eds) (2018), Being Profiled:Cogitas Ergo Sum: 10 Years of Profiling the European Citizen, \\npp.\\xa084–8, Amsterdam: Amsterdam University Press, https://doi.org/10.1515/9789048550180-016.\\n34 Lauren (2011), The Evolution of International Human Rights.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 14, 'page_label': '13'}, page_content='AI governance and human rights\\nResetting the relationship\\n13 Chatham House\\nMyth 3. ‘Human rights are complex and entail \\nexpensive legal advice’\\nWhile human rights can appear complex to non-specialists, initiatives such \\nas\\xa0the UN’s B-Tech project show how the technology industry and investors can \\nimplement their human rights responsibilities.35 Routine inclusion of human rights \\nin computer science and coding training could reduce the perception of complexity. \\nIn reality, human rights are no more complex than any equivalent system of rules \\nor principles: they consist of clear rules, with steps to be followed in implementing \\nthem. While novel situations will still pose challenges, human rights have been \\ndeveloped over many years and are inherently flexible to adapt to such challenges. \\nIn this way, human rights have answers for many situations, in terms of steps to \\nfollow or outcomes to\\xa0reach.\\nA business trying to establish ethical credentials needs advice in order to do so'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 14, 'page_label': '13'}, page_content='In this way, human rights have answers for many situations, in terms of steps to \\nfollow or outcomes to\\xa0reach.\\nA business trying to establish ethical credentials needs advice in order to do so \\neffectively\\xa0– this is the case whatever the source of the rules followed. Following \\nhuman rights standards means following relatively clear, existing rules and \\nminimizing the chances of public censure or litigation for failure to comply.\\nMyth 4. ‘Human rights are about governments’\\nHuman rights are not commonly part of the lexicon of AI developers and corporate \\nethics advisers – particularly outside the EU\\xa0– because they are seen as regulating \\ngovernment, rather than corporate, activity.\\nWhile states are the primary bearer of duties under international human rights \\nlaw, all companies have responsibilities to respect human rights. The Office of \\nthe UN High Commissioner on Human Rights (OHCHR)’s Guiding Principles \\non\\xa0Business and Human Rights, unanimously endorsed by the UN Human Rights'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 14, 'page_label': '13'}, page_content='the UN High Commissioner on Human Rights (OHCHR)’s Guiding Principles \\non\\xa0Business and Human Rights, unanimously endorsed by the UN Human Rights \\nCouncil (HRC) and General Assembly (UNGA) in 2011, state that governments \\nare obliged to take reasonable steps to ensure that companies and other non-state \\nactors respect human rights, and that companies have a responsibility to respect \\nhuman rights in their activities worldwide, including through due diligence and \\nimpact assessment.36 Consideration of human rights impacts ought therefore \\nto\\xa0be\\xa0a\\xa0standard part of corporate practice.\\nHowever, the extent of corporate responsibilities is only patchily understood. \\nThis\\xa0situation is changing, slowly and gradually, as businesses find it in their \\ninterests to take account of human rights impacts.37 Increasingly, both national \\nlaws and investors’ environmental, social and governance (ESG) or equivalent \\nframeworks, plus civil society and public pressure, are obliging companies to'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 14, 'page_label': '13'}, page_content='laws and investors’ environmental, social and governance (ESG) or equivalent \\nframeworks, plus civil society and public pressure, are obliging companies to \\ngive due regard to human rights. The European Commission’s proposed directive \\n35 UN Office of the High Commissioner on Human Rights (undated), ‘B-Tech Project’, https://www.ohchr.org/en/\\nbusiness-and-human-rights/b-tech-project (accessed 12 Sep. 2022).\\n36 UN Office of the High Commissioner on Human Rights (2011), Guiding Principles on Business and Human  \\nRights, https://www.ohchr.org/sites/default/files/Documents/Publications/GuidingPrinciplesBusiness \\nHR_EN.pdf.\\n37 Moynihan, H. and Alves Pinto, T. (2021), The Role of the Private Sector in Protecting Civic Space, Synthesis Paper, \\nLondon: Royal Institute of International Affairs, https://www.chathamhouse.org/2021/02/role-private-sector-\\nprotecting-civic-space.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 15, 'page_label': '14'}, page_content='AI governance and human rights\\nResetting the relationship\\n14 Chatham House\\nrequiring mandatory human rights and environmental due diligence by larger \\ncompanies based or active in the EU would be transformative and should herald \\na\\xa0consistency of approach within the EU.38\\nMyth 5. ‘Human rights are radical’\\nThere are two dimensions to this particular myth: first, that\\xa0– in line with \\npopular\\xa0news coverage\\xa0– human rights are only relevant to extreme situations, \\nsuch as the treatment of criminals, immigrants or terrorists. This view is plain \\nwrong: human rights are about everyday protection from harm and discrimination \\nfor every adult and child, living free from state interference, and being provided \\nwith basic entitlements. In democracies, most people have a general, unspoken \\nassumption that their human rights will be respected: for example, if arrested \\nthey will be treated with dignity; if prosecuted they will be granted a fair trial'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 15, 'page_label': '14'}, page_content='assumption that their human rights will be respected: for example, if arrested \\nthey will be treated with dignity; if prosecuted they will be granted a fair trial \\nin a language they understand; or if voting their vote will be secret and will be \\ncounted fairly. Human rights routinely inform new legislation and policies, from \\ndata protection to\\xa0social housing and social security. They are not often politically \\ncontroversial. They are only newsworthy on the rare occasions when they are \\ndenied, or when they are portrayed as an obstacle to popular policies. The human \\nrights law framework is not a radical philosophy, but a check and balance against \\ndiscrimination or indignity in policy development.\\nThe second dimension to this myth is a misconception that human rights \\nare\\xa0absolutist in nature: that, for example, they prohibit developments such as \\nfacial recognition technology. The desire for quick political soundbites in today’s'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 15, 'page_label': '14'}, page_content='are\\xa0absolutist in nature: that, for example, they prohibit developments such as \\nfacial recognition technology. The desire for quick political soundbites in today’s \\nworld encourages absolutist positions that can do human rights a disservice. \\nThe reality of human rights is more nuanced. For example, many civil society \\norganizations currently assert that all facial recognition technology is contrary \\nto human rights law.39 But this is a shorthand for asserting that facial recognition \\nas commonly configured (i.e.\\xa0involving mass capturing and retention of personal \\ndata and potentially discriminatory judgements without regard to human rights \\nconsiderations) is contrary to human rights law. In fact, human rights law does not \\nlead to a conclusion that facial recognition, properly configured and constrained, \\nshould be banned where there are good reasons of safety or security for using it.40 \\nRather, in this case as elsewhere, human rights law balances rights and interests'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 15, 'page_label': '14'}, page_content='should be banned where there are good reasons of safety or security for using it.40 \\nRather, in this case as elsewhere, human rights law balances rights and interests \\nto\\xa0reach nuanced, subtle judgements.\\n38  European Commission (2022), Proposal for a Directive of the European Parliament and of the Council on \\nCorporate Sustainability Due Diligence, COM(2022) 71 final (23.02.22), https://ec.europa.eu/info/sites/default/\\nfiles/1_1_183885_prop_dir_susta_en.pdf.\\n39  For example, Liberty’s website states that: ‘Facial recognition technology…breaches everyone’s human rights, \\ndiscriminates against people of colour and is unlawful. It’s time to ban it.’ See Liberty (2022), ‘Facial Recognition’, \\nhttps://www.libertyhumanrights.org.uk/fundamental/facial-recognition.\\n40  See, for example, Information Commissioner’s Office (2021), Information Commissioner’s Opinion: The use \\nof\\xa0live facial recognition technology in public places, https://ico.org.uk/media/for-organisations/documents/'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 15, 'page_label': '14'}, page_content='of\\xa0live facial recognition technology in public places, https://ico.org.uk/media/for-organisations/documents/ \\n2619985/ico-opinion-the-use-of-lfr-in-public-places-20210618.pdf.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 16, 'page_label': '15'}, page_content='AI governance and human rights\\nResetting the relationship\\n15 Chatham House\\nMyth 6. ‘Human rights are vague’\\nThere is a perception that human rights norms are too vague to guide AI. For \\nexample, some advocates of ethics argue that human rights are unable to provide \\nguidance when values conflict.41 These objections are largely unfounded. One \\nstrength of human rights law is its system for weighing competing rights and \\ninterests, whether the balance is to be struck between competing individual \\nrights\\xa0or with other collective or societal interests.42\\nMany human rights are framed in terms that make this balancing explicit. \\nFor\\xa0example, Article 21 of the International Covenant on Civil and Political Rights \\nstates that the right of peaceful assembly shall be subject to no restrictions, ‘other \\nthan those imposed in\\xa0conformity with the law and… necessary in a democratic \\nsociety in the interests of national security or public safety, public order (ordre'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 16, 'page_label': '15'}, page_content='than those imposed in\\xa0conformity with the law and… necessary in a democratic \\nsociety in the interests of national security or public safety, public order (ordre \\npublic), the protection of public health or morals or the protection of the rights \\nand freedoms of others’. In\\xa0considering whether this right has been violated, \\nthe UN Human Rights Committee will consider first whether there has been an \\ninterference, then if so, whether that interference is lawful and both ‘necessary for \\nand proportionate to’ one or more of the legitimate grounds for restriction listed in \\nthe article.43 UN human rights bodies, national and regional courts have developed \\nextensive jurisprudence on the appropriate balancing of rights and interests, \\nbalancing flexibility with predictability. These well-established, well-understood \\nsystems have repeatedly proven themselves capable of adaptation in the face of new \\npolicy tools and novel situations. For example, the European Court of Human Rights'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 16, 'page_label': '15'}, page_content='systems have repeatedly proven themselves capable of adaptation in the face of new \\npolicy tools and novel situations. For example, the European Court of Human Rights \\n(ECtHR) recently developed new tests by which to assess bulk interception of\\xa0online \\ncommunications for intelligence purposes.44\\nThe impact of AI is a novel but not insurmountable challenge, as emerging \\njurisprudence is already demonstrating. Indeed, one strength of international human \\nrights law is its capacity to develop incrementally both as societal standards progress \\nand in the face of new factual situations.45\\nMyth 7. ‘Human rights get it wrong’\\nSome may consider that human rights protect the wrong values, apply protection \\nin the wrong ways or are too rigid to apply to technological or social developments. \\n41  Canca, C. (2019), ‘Why Ethics cannot be replaced by the Universal Declaration of Human Rights’, UN University'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 16, 'page_label': '15'}, page_content='41  Canca, C. (2019), ‘Why Ethics cannot be replaced by the Universal Declaration of Human Rights’, UN University \\nOur World, 15 August 2019, https://ourworld.unu.edu/en/why-ethics-cannot-be-replaced-by-the-universal-\\ndeclaration-of-human-rights.\\n42  Yeung, Howes and Pogrebna (2019), ‘AI Governance by Human Rights-Centred Design, Deliberation \\nand Oversight’.\\n43  UN Human Rights Committee (2020), General Comment No. 37 on the right of peaceful assembly \\n(Article 21), para. 36.\\n44  Big Brother Watch and others v UK (ECtHR App no 58170/13).\\n45  See section\\xa03.3 below.\\nOne strength of human rights law is its system \\nfor weighing competing rights and interests.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 17, 'page_label': '16'}, page_content='AI governance and human rights\\nResetting the relationship\\n16 Chatham House\\nFor example, it has been suggested by some policymakers and academics that \\nthe individual right to privacy should be replaced or augmented by a concept of \\ncollective interest in appropriate handling of data that is sensitive to the interests \\nof minority groups.46 Group privacy may be a useful political concept in assessing \\nappropriate limits of state or corporate power resulting from mass collection \\nand\\xa0processing of data.47 But it cannot substitute for human rights law. Such \\nclaims\\xa0underestimate the flexibility of human rights and its processes, including due \\ndiligence and human rights impact assessment, to secure the protection of human \\nrights for all rather than just for those who claim infringement. The right to privacy \\nis capable of evolution in light of competing interests, and enables a\\xa0balance to be \\nstruck between privacy and the public interest in data-sharing and accessibility,'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 17, 'page_label': '16'}, page_content='is capable of evolution in light of competing interests, and enables a\\xa0balance to be \\nstruck between privacy and the public interest in data-sharing and accessibility, \\nwhile safeguarding the interests of groups categorized as such by AI by insistence on \\nboth freedom from discrimination and fairness and due process in decision-making. \\nThere may be scope for considering greater empowerment of\\xa0data subjects48 and/\\nor group enforcement of rights; but it would be a rash move to abandon many \\nyears of\\xa0judicial interpretation and scholarship, including concerns about the \\ndisplacement of individual rights by group rights, by adding, or\\xa0replacing them \\nwith, new legal constructs.\\nMyth 8. ‘Human rights are organized around national models’\\nHuman rights obligations are primarily owed by a state’s government to people \\nwithin that state’s territory or jurisdiction. These jurisdictional limitations are \\nunder pressure: for example, UNGA has stressed that arbitrary surveillance and'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 17, 'page_label': '16'}, page_content='within that state’s territory or jurisdiction. These jurisdictional limitations are \\nunder pressure: for example, UNGA has stressed that arbitrary surveillance and \\ncollection of personal data can violate various human rights, including when \\nundertaken extraterritorially.49 Regarding businesses, the corporate responsibility \\nto respect human rights applies in respect of all individuals affected by a company’s \\noperations, regardless of location.50 In practical terms, businesses should consider \\ntheir human rights responsibilities towards everyone impacted by their work, \\nin any country.\\nMyth 9. ‘Human rights entail greater legal risk’\\nHuman rights are legal rules, and so do entail accountability through courts and \\ntribunals. But this accountability does not hinge on whether an organization pays \\nattention to human rights, but on whether it is liable by reference to a rule of law.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 17, 'page_label': '16'}, page_content='tribunals. But this accountability does not hinge on whether an organization pays \\nattention to human rights, but on whether it is liable by reference to a rule of law. \\n46 For example, Mantelero (2016), ‘Personal data for decisional purposes in the age of analytics: From an \\nindividual to a collective dimension of data protection’, Computer Law & Security Review, February 2016, 32(2), \\npp. 238–55, https://doi.org/10.1016/j.clsr.2016.01.014.\\n47  van der Sloot, B. (2017), ‘Do groups have a right to protect their group interest in privacy and should they? \\nPeeling the onion of rights and interests protected under Article 8 ECHR’, in Taylor, L., Floridi, L. and van der \\nSloot, B. (2017), Group Privacy: New Challenges of Data Technologies, Cham: Springer, p. 223.\\n48  Wong, J., Henderson, T. and Ball, K. (2022), ‘Data protection for the common good: Developing a framework \\nfor a data protection-focused data commons’, Data & Policy, 4(e3), https://doi.org/10.1017/dap.2021.40.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 17, 'page_label': '16'}, page_content='for a data protection-focused data commons’, Data & Policy, 4(e3), https://doi.org/10.1017/dap.2021.40.\\n49 UN General Assembly Resolution (2020), The right to privacy in the digital age, A/RES/75/176 \\n(28\\xa0December\\xa02020), preambular para. 24.\\n50 UN Office of the High Commissioner for Human Rights (2011), Guiding Principles on Business and Human \\nRights, principle 11.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 18, 'page_label': '17'}, page_content='AI governance and human rights\\nResetting the relationship\\n17 Chatham House\\nConsidering human rights will not place a company or government at greater risk \\nfrom human rights claims. On the contrary, addressing human rights issues should \\nhelp to protect against potential claims.\\n3.3 What human rights have to offer\\nHuman rights law provides a means to define the harm that AI should avoid.51 \\nIt\\xa0places its focus on the interests of each individual and addresses the most \\npressing societal concerns of AI, including non-discrimination, fairness and privacy. \\nIt\\xa0provides an excellent starting point by which to assess whether and to what extent \\nAI is ‘for good’. Economic and social rights offer a basis for considering societal \\ndistribution of AI’s potential benefits.\\nHuman rights offer a framework for regulating AI that is an existing system of \\ninternational, regional and domestic law, commanding international legitimacy'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 18, 'page_label': '17'}, page_content='distribution of AI’s potential benefits.\\nHuman rights offer a framework for regulating AI that is an existing system of \\ninternational, regional and domestic law, commanding international legitimacy \\nand a shared language across the world. This framework should be adopted \\nin respect of AI, not\\xa0only because of its intrinsic merit but because the current \\ngeopolitical stasis is likely to prevent effective multilateral cooperation on new \\nnormative frameworks. The focus of discussion should not be on whether human \\nrights can or should be applied to AI, nor on potential alternatives, but on how the \\nexisting framework of human rights does apply in the field of AI. This is already \\nthe\\xa0focus of international organizations at both regional and global level.52\\nHuman rights crystallize a set of ethical values into international norms.53 \\nThe\\xa0system is not perfect, and was not created with AI in mind, but is a universally'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 18, 'page_label': '17'}, page_content='Human rights crystallize a set of ethical values into international norms.53 \\nThe\\xa0system is not perfect, and was not created with AI in mind, but is a universally \\nagreed blueprint for the protection of human values and the common good \\nthat has proven itself capable of adaptation to new circumstances. It\\xa0avoids the \\nneed for\\xa0fresh theoretical debates on the relative merits of different approaches. \\nAs\\xa0a\\xa0set\\xa0of norms, human rights avoid the allegation\\xa0– often levelled at\\xa0ethics\\xa0– \\nof\\xa0being vague and malleable enough to suit corporate interests.\\nHuman rights are relatively clear. It is possible to list comprehensively the legally \\nbinding international, regional and domestic human rights obligations that apply \\nin each country in the world. The meaning of those obligations is reasonably \\nwell-understood.54\\nThe human rights approach has proved relatively successful over more than \\n70\\xa0years, developing incrementally with the benefit of several generations'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 18, 'page_label': '17'}, page_content='well-understood.54\\nThe human rights approach has proved relatively successful over more than \\n70\\xa0years, developing incrementally with the benefit of several generations \\nof academic input, governmental negotiation, civil society input and court \\n51  McGregor, L., Murray, D. and Ng, V. (2019), ‘International Human Rights Law as a Framework for Algorithmic \\nAccountability’, International & Comparative Law Quarterly, 68(2), April 2019, https://doi.org/10.1017/\\nS0020589319000046, pp. 324–27.\\n52 For example, the UN B-Tech Project, https://www.ohchr.org/en/business-and-human-rights/b-tech-\\nproject; and Council of Europe’s Committee on Artificial Intelligence, https://www.coe.int/en/web/artificial-\\nintelligence/cai.\\n53  ‘There is no conflict between ethical values and human rights, but the latter represent a specific crystallisation \\nof these values that are circumscribed and contextualised by legal provision and judicial decisions’. Mantelero,\\xa0A.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 18, 'page_label': '17'}, page_content='of these values that are circumscribed and contextualised by legal provision and judicial decisions’. Mantelero,\\xa0A. \\nand Esposito, S. (2021), ‘An evidence-based methodology for human rights impact assessment (HRIA) in \\nthe development of AI data-intensive systems’, Computer & Security Law Review, 2021, https://ssrn.com/\\nabstract=3829759, p. 6.\\n54  UN Special Rapporteur on Extreme Poverty (2019), Report on use of digital technologies in the welfare state, \\nA/74/493, https://digitallibrary.un.org/record/3834146?ln=en#record-files-collapse-header.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 19, 'page_label': '18'}, page_content='AI governance and human rights\\nResetting the relationship\\n18 Chatham House\\nrulings from many parts of the world. It has evolved in tandem with societal \\ndevelopment, its impact gradually increasing without meeting widespread \\ncalls\\xa0for\\xa0abandonment\\xa0or radical change.\\nHuman rights provide processes and accountability \\nas\\xa0well as principles\\nHuman rights law is accompanied by a vast range of practical tools for \\nimplementation, political oversight and legal accountability that are absent \\nfrom\\xa0ethics.55 Breaches of human rights entail legal as well as political avenues \\nof\\xa0redress. The international human rights framework includes a range of \\nremedial mechanisms with practical effect, ranging from civil society advocacy \\nthrough domestic and international courts, to scrutiny by UN bodies and other \\nstates. In\\xa0many parts of the world, violations of rights by government may \\nbe challenged in\\xa0court with legally binding effect\\xa0– acting as an important \\nconstraint\\xa0on state power.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 19, 'page_label': '18'}, page_content='states. In\\xa0many parts of the world, violations of rights by government may \\nbe challenged in\\xa0court with legally binding effect\\xa0– acting as an important \\nconstraint\\xa0on state power.\\nAs companies and governments already have human rights commitments, their \\nuse of AI will be scrutinized by human rights mechanisms in any case, including \\nthrough claims made to domestic courts in the event of alleged breach. Human \\nrights have already formed the basis for high-profile rulings on, for example, \\nimage\\xa0databases56 and uses of facial recognition technology.57\\nHuman rights have international acceptance and legitimacy\\nInternational human rights law benefits from a higher degree of international \\nacceptance and legitimacy than any other system of values. Governments in every \\ncontinent know and understand the core human rights treaties. Every state is party \\nto some of them, while some treaties have near-universal ratification. This remains'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 19, 'page_label': '18'}, page_content='continent know and understand the core human rights treaties. Every state is party \\nto some of them, while some treaties have near-universal ratification. This remains \\nthe case, despite an apparently waning commitment to the universality of human \\nrights in the rhetoric of certain countries.58 Human rights have played a role, to \\na greater or lesser extent, in shaping the policies and activities of governments \\naround the world.59\\n55 van Veen, C. and Cath, C., (2018), ‘Artificial Intelligence: What’s Human Rights Got to Do with It?’, Data & \\nSociety: Points blog, 14 May 2018, https://points.datasociety.net/artificial-intelligence-whats-human-rights-got-\\nto-do-with-it-4622ec1566d5; Latonero, M. (2020), AI Principle Proliferation as a Crisis of Legitimacy, Carr Center \\nDiscussion Paper Series, Issue 2020-011, https://carrcenter.hks.harvard.edu/files/cchr/files/mark_latonero_ai_\\nprinciples_6.pdf?m=1601910899.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 19, 'page_label': '18'}, page_content='Discussion Paper Series, Issue 2020-011, https://carrcenter.hks.harvard.edu/files/cchr/files/mark_latonero_ai_\\nprinciples_6.pdf?m=1601910899.\\n56 For example, by the Canadian Office of the Privacy Commissioner (2021), ‘Clearview AI’s unlawful \\npractices represented mass surveillance of Canadians, commissioners say’, news release, 2 February 2021, \\nhttps://www.priv.gc.ca/en/opc-news/news-and-announcements/2021/nr-c_210203/?=february-2-2021.\\n57  For example, the Marseille Administrative Tribunal ruled against the use of facial recognition technology at the \\nentrances to French high schools in the La Quadrature du Net case, https://www.laquadrature.net/wp-content/\\nuploads/sites/8/2020/02/1090394890_1901249.pdf.\\n58  For example, Position Paper of the People’s Republic of China for the 77th Session of the United Nations General \\nAssembly, 20 September 2022, http://geneva.china-mission.gov.cn/eng/dbdt/202209/t20220921_10768735.htm,  \\nsection IV.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 19, 'page_label': '18'}, page_content='Assembly, 20 September 2022, http://geneva.china-mission.gov.cn/eng/dbdt/202209/t20220921_10768735.htm,  \\nsection IV.\\n59  Latonero, M. (2020), AI Principle Proliferation as a Crisis of Legitimacy, Carr Center Discussion Paper \\nSeries, Issue 2020-011, https://carrcenter.hks.harvard.edu/files/cchr/files/mark_latonero_ai_principles_6.\\npdf?m=1601910899, p. 6.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 20, 'page_label': '19'}, page_content='AI governance and human rights\\nResetting the relationship\\n19 Chatham House\\nUN processes affecting all states, such as the HRC’s Universal Periodic Review \\nand\\xa0the UN treaty bodies’ periodic examinations of states’ compliance, entail that \\nevery UN member state engages with the international human rights architecture. \\nRegional treaties that have strong local support reinforce these UN instruments \\nin\\xa0some parts of the world.60 International human rights law has constitutional \\nor\\xa0quasi-constitutional status in many countries, notably in Europe, embedding \\nit\\xa0deep into systems of governance.61 Civil society uses the human rights law \\nframework as a basis for monitoring state and corporate activities worldwide.\\nThis international legitimacy has given human rights a significant role in the \\nproduction of internationally negotiated sets of AI governance principles. For \\nexample, the OECD AI Principles call on all actors to respect the rule of law, human'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 20, 'page_label': '19'}, page_content='production of internationally negotiated sets of AI governance principles. For \\nexample, the OECD AI Principles call on all actors to respect the rule of law, human \\nrights and democratic values throughout the AI system life cycle.62 As discussed \\npreviously, UNESCO’s Recommendation on the Ethics of Artificial Intelligence \\nnames human rights and fundamental freedoms as the first of the ‘values’ around \\nwhich it is crafted.63 The\\xa0Council of Europe’s Committee on Artificial Intelligence \\n(CAI) is working on a potential legal framework for the development, design and \\napplication of AI, based on the Council’s standards on human rights, democracy \\nand the rule of law.64 Although the universality of human rights is increasingly \\ncontested, there is still, to a large degree, a global consensus on the continued \\nrelevance of long-agreed human rights commitments.\\nHuman rights achieve a balance between universality \\nand\\xa0sensitivity to national contexts'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 20, 'page_label': '19'}, page_content='relevance of long-agreed human rights commitments.\\nHuman rights achieve a balance between universality \\nand\\xa0sensitivity to national contexts\\nInternational human rights law offers a degree of discretion to governments as to \\nhow they implement each right, within certain parameters. This flexibility is known \\nas the ‘margin of appreciation’ in Europe, now enshrined in the preamble to the \\nECHR,65 and has similar effect in the UN human rights system.66 It varies according \\nto the specific right in question and the impact of any interference: for example, \\nhuman rights law offers governments no discretion in implementing bans on \\ntorture or slavery, but European human rights law permits governments a narrow \\n60 For example, ECHR; Inter-American Charter on Human Rights; African Charter on Human and People’s Rights.\\n61 Yeung, Howes and Pogrebna (2019), ‘AI Governance by Human Rights-Centred Design, Deliberation \\nand Oversight’.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 20, 'page_label': '19'}, page_content='61 Yeung, Howes and Pogrebna (2019), ‘AI Governance by Human Rights-Centred Design, Deliberation \\nand Oversight’.\\n62 Organisation for Economic Co-operation and Development (2019), Recommendation of the Council on Artificial \\nIntelligence, https://legalinstruments.oecd.org/en/instruments/OECD-LEGAL-0449, Article 1.2(a).\\n63  UN Educational, Scientific and Cultural Organization (2021), Recommendation on the Ethics of \\nArtificial Intelligence.\\n64 Council of Europe (2022), ‘Inaugural Meeting of the Committee on Artificial Intelligence (CAI)’.\\n65 Protocol 15 to the ECHR, Article 1.\\n66 Shany, Y. (2018), ‘All Roads Lead to Strasbourg?: Application of the Margin of Appreciation Doctrine \\nby\\xa0the\\xa0European Court of Human Rights and the UN Human Rights Committee’,\\xa0Journal of International \\nDispute\\xa0Settlement, 9(2), May 2018, pp. 180–98,\\xa0https://doi.org/10.1093/jnlids/idx011.\\nInternational human rights law offers a\\xa0degree \\nof\\xa0discretion to governments as to\\xa0how they implement'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 20, 'page_label': '19'}, page_content='Dispute\\xa0Settlement, 9(2), May 2018, pp. 180–98,\\xa0https://doi.org/10.1093/jnlids/idx011.\\nInternational human rights law offers a\\xa0degree \\nof\\xa0discretion to governments as to\\xa0how they implement \\neach right, within certain parameters.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 21, 'page_label': '20'}, page_content='AI governance and human rights\\nResetting the relationship\\n20 Chatham House\\nmargin of appreciation concerning general bans on protest, and a wider margin \\nconcerning whether governments choose to sanction protestors who intentionally \\ndisrupt ordinary life.67\\nHuman rights are necessary but not sufficient for AI governance\\nInternational human rights law may not currently address all the potential harms \\nto people caused by AI. But it is adaptable to new circumstances and changing \\nsocial norms: the ECHR, for example, is ‘a living instrument which… must be \\ninterpreted in light of present-day conditions.’68 The UN secretary-general’s High \\nLevel Panel on Digital Cooperation has called for an urgent examination of how \\nhuman rights frameworks apply in the digital age.69\\nHuman rights law may develop through new attention to existing rights. For \\nexample, the rights to freedom of thought and opinion are absolute. However,'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 21, 'page_label': '20'}, page_content='Human rights law may develop through new attention to existing rights. For \\nexample, the rights to freedom of thought and opinion are absolute. However, \\ntheir parameters remain relatively unclear because they were largely taken for \\ngranted until challenged by the emergence of a technologically enabled industry \\nof influence.70 Further, new contexts may lead to new understandings and \\nformulations of rights. For example, explainability and human involvement\\xa0– \\ncommonly discussed elements of AI ethics\\xa0– are not usually considered as elements \\nof human rights, but might be found in existing requirements that individuals be \\nprovided with reasons for decisions made concerning them, and of the possibility \\nof contesting those decisions and securing adequate remedies. The Council of \\nEurope’s work on a potential convention is likely to clarify the application of \\nhuman\\xa0rights to\\xa0AI,71 as human rights litigation is already beginning to do.72'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 21, 'page_label': '20'}, page_content='Europe’s work on a potential convention is likely to clarify the application of \\nhuman\\xa0rights to\\xa0AI,71 as human rights litigation is already beginning to do.72\\nThe development of human rights law and its subsequent interpretation take time, \\nyet technology moves quickly. Human rights in their current form, while essential, \\nare not sufficient to act as an entire system for the ethical management of AI. \\nHuman rights should rather be the starting point for normative constraints on AI, \\nthe baseline to which new rights or further ethical guardrails might appropriately \\nbe added, including any ethical principles that businesses or other entities may \\nchoose to adopt.\\nThe second half of this paper explores the contributions of human rights in detail \\nand concludes by recommending practical actions to place human rights at the \\nheart of AI governance.\\n67  European Court of Human Rights (2021), Guide on Article 11 of the European Convention on Human Rights,'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 21, 'page_label': '20'}, page_content='heart of AI governance.\\n67  European Court of Human Rights (2021), Guide on Article 11 of the European Convention on Human Rights, \\nhttps://echr.coe.int/Documents/Guide_Art_11_ENG.pdf.\\n68 Tyrer v United Kingdom, ECtHR App No 5856/72, judgment of 25 April 1978, Series A No 26, para. 31.\\n69 UN Secretary-General’s High-Level Panel on Digital Cooperation (2019), The Age of Digital Interdependence \\nhttps://www.un.org/en/pdfs/DigitalCooperation-report-for%20web.pdf.\\n70  Jones (2019), Online Disinformation and Political Discourse: Applying a Human Rights Framework.\\n71 Council of Europe (2022), ‘Inaugural Meeting of the Committee on Artificial Intelligence (CAI)’.\\n72 See Chapter\\xa06.1 below.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 22, 'page_label': '21'}, page_content='21 Chatham House\\n04 \\nPrinciples of AI \\ngovernance: the \\ncontribution of \\nhuman rights\\nKey principles of human rights law have an important \\nrole\\xa0to\\xa0play in\\xa0determining AI governance standards.\\nThere are three dimensions to AI governance: (i) the substantive standards, \\nor\\xa0principles, that the developers and implementers of AI should meet; (ii) the \\nprocesses to ensure that substantive standards are met; and (iii) accountability \\nand\\xa0remedies for any breach of those standards.\\nIn each of these dimensions, AI governance is immature because technology \\nand\\xa0its\\xa0uses have developed much more rapidly than the rules constraining them. \\nHuman rights law offers baseline standards for all three dimensions.\\n4.1 Principles: the landscape\\nAI ethical principles from companies, civil society and intergovernmental \\norganizations have proliferated in recent years,73 causing more confusion \\nthan clarity through their overlapping nature, number and diversity.74 There'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 22, 'page_label': '21'}, page_content='organizations have proliferated in recent years,73 causing more confusion \\nthan clarity through their overlapping nature, number and diversity.74 There \\n73 The AI Ethics Guidelines Global Inventory lists over 165 sets of guidelines. AlgorithmWatch (2022), ‘AI Ethics \\nGuidelines Global Inventory’, https://inventory.algorithmwatch.org.\\n74 Floridi, L. and Cowls, J. (2019), ‘A Unified Framework of Five Principles for AI in Society’, Harvard Data Science \\nReview, 1.1, https://doi.org/10.1162/99608f92.8cd550d1.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 23, 'page_label': '22'}, page_content='AI governance and human rights\\nResetting the relationship\\n22 Chatham House\\nare common themes such as data protection, understandability, transparency \\nfor accountability and tackling bias. But the precise meaning of each of these \\nterms varies.75 Some ethics principles identified\\xa0– such as beneficence and \\nnon-maleficence76\\xa0– are so abstract that they are not easily translatable for practical \\nuse in governance. There is no unifying theme between rival sets of ethical principles \\nand there are debates on the representativeness of those principles, as\\xa0most stem \\nlargely from Europe and North America, from separate corporate and\\xa0national \\ncontexts, and from men.77\\nSome assert that, without unanimity as to what it entails, ethics offers a\\xa0lexicon \\nthat can be used to give a veneer of respectability to any corporate activity. In the \\nwords of Philip Alston, ‘as long as you are focused on ethics, it’s mine\\xa0against yours.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 23, 'page_label': '22'}, page_content='that can be used to give a veneer of respectability to any corporate activity. In the \\nwords of Philip Alston, ‘as long as you are focused on ethics, it’s mine\\xa0against yours. \\nI will define fairness, what is transparency, what is accountability. There are no \\nuniversal standards.’78\\n4.2 Principles: Human rights law\\nTo date, there are no international human rights treaties that specifically address \\nthe impact of AI,79 but existing human rights laws apply to applications of AI. \\nThe\\xa0former UN high commissioner for human rights, Michelle Bachelet, clarified \\nthat AI can have significant impacts on the implementation of many human rights, \\nincluding privacy, health, education, freedom of movement, freedom of assembly \\nand association, and freedom of expression.80 Bachelet noted that inferences and \\npredictions about individuals made by AI may profoundly affect not only those \\nindividuals’ privacy but also their autonomy, and may raise issues regarding'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 23, 'page_label': '22'}, page_content='predictions about individuals made by AI may profoundly affect not only those \\nindividuals’ privacy but also their autonomy, and may raise issues regarding \\nfreedom of thought and opinion, freedom of expression, the right to a fair trial \\nand\\xa0other related rights.81 Uses of faulty data may result in bias or discrimination,82 \\nas may faulty AI tools. Uses of AI in the criminal justice process may lead to \\nviolations of the rights to privacy, fair trial, freedom from arbitrary arrest and \\ndetention and even the right to life.83\\nWhile all rights are relevant, this section provides an overview of key rights that \\nshould form the basis of any safeguards for AI development.\\n75  Fjeld, J. et al. (2020), Principled Artificial Intelligence; Hagendorff (2020), ‘The Ethics of AI Ethics’; Floridi \\nand\\xa0Cowls (2019), ‘A Unified Framework of Five Principles for AI in Society’.\\n76  Floridi and Cowls (2019), ‘A Unified Framework of Five Principles for AI in Society’.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 23, 'page_label': '22'}, page_content='and\\xa0Cowls (2019), ‘A Unified Framework of Five Principles for AI in Society’.\\n76  Floridi and Cowls (2019), ‘A Unified Framework of Five Principles for AI in Society’.\\n77  Hagendorff (2020), ‘The Ethics of AI Ethics’; Montreal AI Ethics Institute (2021), ‘The Proliferation of AI \\nEthics\\xa0Principles: What’s Next?’, https://montrealethics.ai/the-proliferation-of-ai-ethics-principles-whats-next.\\n78  UN Special Rapporteur on Extreme Poverty (2019), Report on use of digital technologies in the welfare state, \\nA/74/493, https://digitallibrary.un.org/record/3834146?ln=en#record-files-collapse-header. See also Yeung, \\nHowes and Pogrebna (2019), ‘AI Governance by Human Rights-Centred Design, Deliberation and Oversight’, p.\\xa03: \\n‘Yet the vagueness and elasticity of the scope and content of “AI ethics” has meant that it currently operates as \\nan empty vessel into which anyone (including the tech industry, and the so-called Digital Titans) can pour their \\npreferred “ethical” content.’'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 23, 'page_label': '22'}, page_content='an empty vessel into which anyone (including the tech industry, and the so-called Digital Titans) can pour their \\npreferred “ethical” content.’\\n79  Work is under way at the Council of Europe for a legal instrument on AI, by reference to the Council of Europe’s \\nstandards on human rights, democracy and the rule of law. See Council of Europe (2022), ‘Inaugural Meeting \\nof\\xa0the Committee on Artificial Intelligence (CAI)’.\\n80 United Nations High Commissioner for Human Rights (2021), The Right to Privacy in the Digital Age, \\nA/HRC/48/31, https://documents-dds-ny.un.org/doc/UNDOC/GEN/G21/249/21/PDF/G2124921.pdf.\\n81 Ibid., para. 17.\\n82  Ibid., para. 19.\\n83  Ibid., para. 24.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 24, 'page_label': '23'}, page_content='AI governance and human rights\\nResetting the relationship\\n23 Chatham House\\n4.2.1 Privacy\\nThe challenges presented by AI\\nAI is having a huge impact on privacy and data protection. Far more information \\nabout individuals is collated now than ever before, increasing the potential \\nfor exploitation. A new equilibrium is needed between the value of personal \\ndata for AI on the one hand and personal privacy on the other. There are two \\nparallel challenges to overcome: (i) AI is causing, and contributing to, significant \\nbreaches of privacy and data protection; and (ii) use of extensive personal data \\nin AI decision-making and influencing is contributing to an accretion of state \\nand\\xa0corporate power.\\nExamples of breaches of privacy and data protection include:\\n — AI’s requirement for data sets may create an incentive for companies and \\npublic institutions to share personal data in breach of privacy requirements. \\nFor example, in 2017, a UK health trust was found to have shared the data of'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 24, 'page_label': '23'}, page_content='public institutions to share personal data in breach of privacy requirements. \\nFor example, in 2017, a UK health trust was found to have shared the data of \\n1.6\\xa0million patients with Google’s DeepMind, without adequate consent from \\nthe patients concerned.84\\n — AI may facilitate the harvesting of personal data without adequate consent. \\nBetween 2013 and 2018, Cambridge Analytica collated personal data of up \\nto 87 million Facebook users without their knowledge or consent for use \\nin\\xa0political advertising.85\\n — The practice of using publicly available images to create AI facial recognition \\ndatabases raises major privacy concerns. Projects such as Exposing.ai aim to \\nhighlight the privacy implications of extant large facial recognition datasets.86 \\nSome large companies, including Microsoft and Facebook, have closed their \\nfacial recognition operations.87 Clearview AI’s provision of facial recognition \\ntechnology for law enforcement purposes\\xa0– via a database of 10 billion images'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 24, 'page_label': '23'}, page_content='facial recognition operations.87 Clearview AI’s provision of facial recognition \\ntechnology for law enforcement purposes\\xa0– via a database of 10 billion images \\ngleaned from the internet\\xa0– has been found in breach of privacy laws in several \\ncountries, including Australia, Canada, France and the UK.88\\n — AI lends itself to bulk interception and assessment of online communications. \\nIn\\xa02021, the ECtHR found that the UK’s former regime for bulk interception, \\nusing digital and automated methods, lacked necessary end-to-end safeguards \\nfor compliance with privacy rights.89\\n84 BBC News (2017), ‘Google DeepMind NHS app test broke UK privacy law’, 3 July 2017, https://www.bbc.co.uk/ \\nnews/technology-40483202.\\n85 Information Commissioner’s Office (2018), Investigation into the use of data analytics in political campaigns, \\nhttps://ico.org.uk/media/action-weve-taken/2260271/investigation-into-the-use-of-data-analytics-in-political-\\ncampaigns-final-20181105.pdf.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 24, 'page_label': '23'}, page_content='https://ico.org.uk/media/action-weve-taken/2260271/investigation-into-the-use-of-data-analytics-in-political-\\ncampaigns-final-20181105.pdf.\\n86 Harvey, A. and LaPlace, J. (2021), ‘Exposing.ai’, 1 January 2021, https://exposing.ai (accessed 12 Sep. 2022).\\n87  Microsoft withdrew its MS Celeb database in 2019: Computing News (2019), ‘Microsoft withdraws facial \\nrecognition database of 100,000 people’, 6 June 2019, https://www.computing.co.uk/news/3076968/microsoft-\\nwithdraws-facial-recognition-database-of-100-000-people. Meta announced in November 2021 that it was \\nshutting down Facebook’s facial recognition system: Meta (2021), ‘An update on our use of face recognition’, \\n2\\xa0November 2021, https://about.fb.com/news/2021/11/update-on-use-of-face-recognition.\\n88 Lomas, N. (2021), ‘France latest to slap Clearview AI with order to delete data’, TechCrunch, 16\\xa0December\\xa02021, \\nhttps://techcrunch.com/2021/12/16/clearview-gdpr-breaches-france.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 24, 'page_label': '23'}, page_content='88 Lomas, N. (2021), ‘France latest to slap Clearview AI with order to delete data’, TechCrunch, 16\\xa0December\\xa02021, \\nhttps://techcrunch.com/2021/12/16/clearview-gdpr-breaches-france.\\n89  Big Brother Watch and others v UK (ECtHR App no 58170/13).'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 25, 'page_label': '24'}, page_content='AI governance and human rights\\nResetting the relationship\\n24  Chatham House\\n — ‘Smart’ devices, such as fridges and vehicles, may not only collate data on users \\nto improve performance, but also to sell to third parties. If not properly secured, \\nsuch devices may also expose users to surveillance by hackers. In 2017, for \\nexample, the German authorities withdrew the ‘My Friend Cayla’ doll from sale \\nover fears that children’s conversations could be listened to via Bluetooth.90\\nAI impacts privacy in several ways. First, its thirst for data creates compelling \\nreasons for increased collection and sharing of data, including personal data, \\nwith the aim of improving the technology’s operation. Second, AI may be used \\nto collate data, including that of a sensitive, personal nature, for purposes of \\nsurveillance. Third, AI may be used to develop profiles of individuals that are then \\nthe basis of decisions on matters fundamental to their lives\\xa0– from healthcare to'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 25, 'page_label': '24'}, page_content='surveillance. Third, AI may be used to develop profiles of individuals that are then \\nthe basis of decisions on matters fundamental to their lives\\xa0– from healthcare to \\nsocial benefits, to employment to insurance provision. As part of this profiling, \\nAI may infer further, potentially sensitive information about individuals without \\ntheir knowledge or consent, such as conclusions on their sexual orientation, \\nrelationship status or health conditions. Finally, AI may make use of personal data \\nto micro-target advertising and political messaging, to manipulate and exploit \\nindividual vulnerabilities, or even to facilitate crimes such as identity theft.\\nInternational human rights law\\nThe human right to privacy currently entails that any processing of personal data \\nshould be fair, lawful and transparent, based on free consent or another legitimate \\nbasis laid down in law. Data should only be held for a limited period and for'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 25, 'page_label': '24'}, page_content='should be fair, lawful and transparent, based on free consent or another legitimate \\nbasis laid down in law. Data should only be held for a limited period and for \\nspecific purposes, with those purposes not to be lightly changed. Data should \\nbe\\xa0held securely, and sensitive personal data should enjoy heightened protection. \\nPrivacy entails that individuals should know that their personal data has been \\nretained and processed, and that they have a right both to rectify or erase their \\npersonal data and to limit how it is used. Privacy further entails that individuals \\nmust not\\xa0be\\xa0exposed to mass surveillance or unlimited profiling. Personal data \\nshould\\xa0not be\\xa0transferred, particularly overseas, unless similar standards will \\nbe\\xa0upheld by the recipient of that data.91\\nHuman rights law is already the widely accepted basis for most legislation \\nprotecting privacy. The EU’s General Data Protection Regulation (GDPR) is founded'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 25, 'page_label': '24'}, page_content='Human rights law is already the widely accepted basis for most legislation \\nprotecting privacy. The EU’s General Data Protection Regulation (GDPR) is founded \\non the right to protection of personal data in Article 8(1) of the EU Charter of \\nFundamental Rights\\xa0– this is an aspect of the right to privacy in earlier human\\xa0rights \\ntreaties. Privacy and data protection is one of the European Commission’s Seven \\nPrinciples for Trustworthy AI, while most statements of AI principles include \\na\\xa0commitment to privacy.92\\nApplication of human rights law to the challenges of AI\\nWith the development of AI, it is becoming apparent that changes need \\nto\\xa0be\\xa0made\\xa0to the contours of the right to privacy.\\n90 BBC News (2017), ‘German parents told to destroy Cayla toys over hacking fears’, 17 February 2017, \\nhttps://www.bbc.co.uk/news/world-europe-39002142.\\n91 United Nations High Commissioner for Human Rights (2018), The Right to Privacy in the Digital Age,'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 25, 'page_label': '24'}, page_content='https://www.bbc.co.uk/news/world-europe-39002142.\\n91 United Nations High Commissioner for Human Rights (2018), The Right to Privacy in the Digital Age, \\nA/HRC/39/29, https://www.ohchr.org/en/documents/reports/ahrc3929-right-privacy-digital-age-report-\\nunited-nations-high-commissioner-human.\\n92  35 of the 36 statements of AI principles reviewed by Fjeld et al. included this commitment: Fjeld et al. (2020), \\nPrincipled Artificial Intelligence.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 26, 'page_label': '25'}, page_content='AI governance and human rights\\nResetting the relationship\\n25 Chatham House\\nThere is growing awareness of the tension between privacy’s requirement to restrict \\nflows of personal data on the one hand, and economic and commercial arguments \\nin favour of free flow on the other. There are many sound reasons for improved \\ndata accessibility: fostering developments in AI innovation; facilitating increased \\nuse of AI; and preventing data restrictions from distorting markets or acting \\nas\\xa0a\\xa0barrier to competition and innovation.\\nPrivacy should not be viewed as static: it is flexible enough to adapt and develop, \\nthrough new legislation or through judicial interpretation, in light of rapidly \\nchanging technological and social conditions. Individual privacy remains \\nvital to ensuring that individuals do not live in a surveillance state, and that \\nindividuals retain control over their own data and by whom and how it is seen'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 26, 'page_label': '25'}, page_content='vital to ensuring that individuals do not live in a surveillance state, and that \\nindividuals retain control over their own data and by whom and how it is seen \\nand used. This is critical at a time when the value of privacy is being steadily \\nand\\xa0unconsciously diluted.\\nThe human right to privacy should be used to resolve competing interests in \\nan\\xa0AI-dominated world\\xa0– whether those interests are commercial, individual or \\ntechnical. For example, rather than privacy impeding the transfer of anonymized \\ndata for use in AI data sets, the balancing between rights and interests allowed \\nby\\xa0the human right to privacy could be used to set appropriate limits on \\ndata-profiling and\\xa0micro-targeting.\\n4.2.2 Equality: discrimination and bias\\nThe challenges presented by AI\\nBecause AI generally operates by applying rules to the treatment of people, rather \\nthan by assessing each individual on their merits, it carries significant risks of'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 26, 'page_label': '25'}, page_content='The challenges presented by AI\\nBecause AI generally operates by applying rules to the treatment of people, rather \\nthan by assessing each individual on their merits, it carries significant risks of \\nembedding discrimination, as the rules that it applies may distinguish between \\npeople, directly or indirectly, by reference to protected characteristics. Indeed, \\nexamples of such bias and discrimination in the use of AI abound:\\n — In 2015, researchers found that female job seekers were much less likely than \\nmales to be shown adverts for highly paid jobs on Google.93\\n — In 2016, researchers found that an algorithm used to determine offenders’ risk \\nof recidivism often overstated the risk that black defendants would re-offend, \\nand understated the risk of reoffending by white defendants.94\\n93 Gibbs, S. (2015), ‘Women less likely to be shown ads for high-paid jobs on Google, study shows’, Guardian,'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 26, 'page_label': '25'}, page_content='and understated the risk of reoffending by white defendants.94\\n93 Gibbs, S. (2015), ‘Women less likely to be shown ads for high-paid jobs on Google, study shows’, Guardian, \\n8 July 2015, https://www.theguardian.com/technology/2015/jul/08/women-less-likely-ads-high-paid-\\njobs-google-study.\\n94 Larson, J. et al. (2016), ‘How We Analyzed the COMPAS Recidivism Algorithm’, ProPublica, 23 May 2016, \\nhttps://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm; see also State of \\nWisconsin v Eric L Loomis (2016) WI 68, 881 N.W.2d 749.\\nPrivacy should not be viewed as static: it is\\xa0flexible \\nenough to adapt and develop […] in light of rapidly \\nchanging technological and\\xa0social conditions.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 27, 'page_label': '26'}, page_content='AI governance and human rights\\nResetting the relationship\\n26 Chatham House\\n — In 2017, Amazon abandoned its automated recruitment platform, built on \\nobserving patterns in applicant CVs over the previous years, having been unable \\nto prevent it from discriminating on the basis of gender or from making other \\ninappropriate recommendations.95\\n — In 2018, Immigration New Zealand suspended its use of data-profiling, which \\nhad been predicting likely healthcare costs and criminality of immigrants on the \\nbasis of demographics including age, gender and ethnicity.96\\n — In 2019, researchers found that AI widely used to allocate healthcare in US \\nhospitals was systematically discriminating against black people, by referring \\nthem on to specialized care programmes less frequently than white people. \\nThe\\xa0algorithm was predicting future healthcare costs as a proxy for illness, using \\npast costs for individuals in similar situations. This failed to take account of the'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 27, 'page_label': '26'}, page_content='The\\xa0algorithm was predicting future healthcare costs as a proxy for illness, using \\npast costs for individuals in similar situations. This failed to take account of the \\nfact that less money had been spent historically on caring for black patients.97\\n — In 2020, the Austrian public employment service (AMS) began using an \\nalgorithm that enabled it to classify jobseekers according to their likelihood of \\nsuccessful re-employment. The algorithm has been criticized for discriminating \\non the basis of gender, disability and other factors, and for intersectional \\ndiscrimination.98 AMS has suspended use of the algorithm pending the \\noutcome\\xa0of legal challenges.99\\nAI makes it difficult to assess whether discrimination has occurred. An individual \\nusually becomes aware of discrimination by comparing their treatment, or its \\noutcome, with that of other people. But when complex AI is used to make each \\nindividual a personalized offer (for example, on social security payments) or'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 27, 'page_label': '26'}, page_content='outcome, with that of other people. But when complex AI is used to make each \\nindividual a personalized offer (for example, on social security payments) or \\ndecision (for example, on school or college entry), that individual may have no \\nmeans of knowing what criteria were used, nor how their result differs from others. \\nConsequently, individuals may not know, or have any accessible way of finding out, \\nwhether they have been disadvantaged or how.100\\nAI developers have learned from past problems and gone to considerable lengths \\nto devise systems that promote equality as much, or more, than human decision-\\nmaking.101 Nonetheless, several features of AI systems may cause them to make \\nbiased decisions. First, AI systems rely on training data to train the decision-making \\nalgorithm. Any imbalance or bias in that training data is likely then to be replicated \\nand become exaggerated in the AI system. If the training data is taken from the real'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 27, 'page_label': '26'}, page_content='algorithm. Any imbalance or bias in that training data is likely then to be replicated \\nand become exaggerated in the AI system. If the training data is taken from the real \\nworld, rather than artificially generated, AI is likely to replicate and exaggerate any \\n95 Dastin, J. (2018), ‘Amazon scraps secret AI recruiting tool that showed bias against women’, Reuters, \\n11\\xa0October 2018, https://www.reuters.com/article/us-amazon-com-jobs-automation-insight-idUSKCN1MK08G.\\n96 Bonnett, G. (2018), ‘Immigration NZ using data system to predict likely troublemakers’, RNZ News, \\n5\\xa0April\\xa02018, https://www.rnz.co.nz/news/national/354135/immigration-nz-using-data-system-to-predict-\\nlikely-troublemakers.\\n97  Obermeyer, Z. et al. (2019), ‘Dissecting racial bias in an algorithm used to manage the health of populations’, \\nScience, 25 October 2019, 366(6464), pp. 447–553, https://doi.org/10.1126/science.aax2342.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 27, 'page_label': '26'}, page_content='Science, 25 October 2019, 366(6464), pp. 447–553, https://doi.org/10.1126/science.aax2342.\\n98 Allhutter, D. et al. (2020), ‘Algorithmic profiling of job seekers in Austria: how austerity politics are made \\neffective’, Frontiers in Big Data, 21 February 2020, https://doi.org/10.3389/fdata.2020.00005.\\n99 Der Standard (2022), ‘“Zum In-die-Tonne-Treten”: Neue Kritik am AMS-Algorithmus’ [“To Be Thrown In The \\nBin”: New Criticism of the AMS Algorithm’], 28 April 2022, https://www.derstandard.at/story/2000135277980/\\nneuerliche-kritik-am-ams-algorithmus-zum-in-die-tonne-treten.\\n100 Obermeyer et al. (2019), ‘Dissecting racial bias in an algorithm used to manage the health of populations’.\\n101 For example, HireVue, an AI recruitment tool used by some large companies, claims to ‘[i]ncrease diversity \\nand mitigate bias’ by finding a wider candidate pool, evaluating objectively and consistently, and helping to avoid'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 27, 'page_label': '26'}, page_content='and mitigate bias’ by finding a wider candidate pool, evaluating objectively and consistently, and helping to avoid \\nunconscious bias. See HireVue (2022), https://www.hirevue.com/employment-diversity-bias.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 28, 'page_label': '27'}, page_content='AI governance and human rights\\nResetting the relationship\\n27 Chatham House\\nbias already present in society. Second, AI systems rely on the instructions given \\nto them, as well as their own self-learning. Any discrimination or bias deployed \\nby the designer risks being replicated and exaggerated in the AI system. Third, \\nAI\\xa0systems operate within a context: an AI system will lead to bias if it is deployed \\nwithin the context of social conditions that undermine enjoyment of rights by \\ncertain groups.102 Without human involvement, AI is currently unable to replicate \\ncontextual notions of fairness.\\nInternational human rights law\\nHuman rights law provides standards of equality and non-discrimination by which \\nto assess AI. It requires that all individuals’ rights be respected and ensured ‘without \\ndistinction of any kind, such as race, colour, sex, language, religion, political or other \\nopinion, national or social origin, property, birth or other status’.103 The law entails'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 28, 'page_label': '27'}, page_content='distinction of any kind, such as race, colour, sex, language, religion, political or other \\nopinion, national or social origin, property, birth or other status’.103 The law entails \\nprohibitions against not just direct discrimination (i.e.\\xa0treating people differently \\non prohibited grounds), but indirect discrimination (i.e.\\xa0treating people the same, \\nbut in a way that puts people from a protected group at a disadvantage without \\nan objective justification) and structural discrimination (i.e.\\xa0creating structural \\nconditions in society that prevent all groups from accessing the same opportunities). \\nAcknowledging that equality does not always mean treating everyone the \\nsame, discrimination law provides structured tests for assessing and preventing \\nunlawful treatment.\\nThis ban on discrimination has formed the basis for well-developed understandings \\nof, and jurisprudence on, non-discrimination in both the public and private sectors.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 28, 'page_label': '27'}, page_content='unlawful treatment.\\nThis ban on discrimination has formed the basis for well-developed understandings \\nof, and jurisprudence on, non-discrimination in both the public and private sectors. \\nHuman rights law obliges governments both to ensure there is no discrimination \\nin public sector decision-making and to protect individuals against discrimination \\nin the private sector. Human rights law does not forbid differential treatment that \\nstems from factors other than protected characteristics, but such treatment must \\nmeet standards of fairness and due process in decision-making (see below).\\nApplication of human rights law to the challenges presented by AI\\nHuman rights practitioners are accustomed to considering the prohibition of \\ndiscrimination by reference to well-established tests, and to resolving tensions \\nbetween non-discrimination and other rights like freedom of speech. Adopting the \\nstandards that are well established and internationally accepted in human rights'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 28, 'page_label': '27'}, page_content='between non-discrimination and other rights like freedom of speech. Adopting the \\nstandards that are well established and internationally accepted in human rights \\nlaw minimizes the need for fresh debates on highly contested concepts in ethics \\n(what is ‘justice’? what is ‘fairness’?).104 Further, it avoids the risk of confusion \\nfrom the imposition of parallel, non-human rights standards of discrimination \\nspecifically in the field of AI.\\n102  Wachter, S., Mittelstadt, B. and Russell, C. (2020), ‘Why Fairness Cannot be Automated: Bridging the \\nGap between EU Non-Discrimination Law and AI’, Computer Law & Security Review, 41(2021): 105567, \\nhttps://ssrn.com/abstract=3547922.\\n103 International Covenant on Civil and Political Rights, Article 2(1). Some non-discrimination laws forbid \\ndiscrimination in all circumstances, rather than merely in the implementation of rights: see Protocol 12 to the'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 28, 'page_label': '27'}, page_content='discrimination in all circumstances, rather than merely in the implementation of rights: see Protocol 12 to the \\nEuropean Convention on Human Rights and Articles 20 and 21 of the European Charter of Fundamental Rights.\\n104 For example on justice, Floridi, L. et al. (2018), ‘AI4People\\xa0– An Ethical Framework for a Good AI Society’, \\nMinds and Machines, 28, pp. 689–707, https://doi.org/10.1007/s11023-018-9482-5; Bartneck, C. et\\xa0al. (2021), \\nAn Introduction to Ethics in Robotics and AI, Springer Briefs in Ethics, p. 33.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 29, 'page_label': '28'}, page_content='AI governance and human rights\\nResetting the relationship\\n28 Chatham House\\nInternational human rights law does not simply require governments to ban \\ndiscrimination in AI. As the UN special rapporteur on contemporary forms \\nof\\xa0racism has observed, human rights law also requires governments to deploy \\na\\xa0structural understanding of discrimination risks from AI. To combat the potential \\nfor bias, the tech sector would benefit from more diversity among AI developers, \\nmore guidance on bias detection and mitigation and the collection and use of data \\nto monitor for bias, and more leadership by example from the public sector.105 \\nAI\\xa0developers and implementers must consider holistically the impact of all \\nalgorithms on individuals and groups, rather than merely the impact of each \\nalgorithm on each right separately.106 Algorithms should be reviewed regularly \\nto\\xa0ensure that their results are not discriminatory, even though obtaining data'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 29, 'page_label': '28'}, page_content='algorithm on each right separately.106 Algorithms should be reviewed regularly \\nto\\xa0ensure that their results are not discriminatory, even though obtaining data \\nfor\\xa0comparison purposes may be challenging.107 Vigilance is needed to ensure \\nthat\\xa0other factors are not used as proxies for protected characteristics\\xa0– for \\nexample, that postcode is not used as a proxy for ethnic origin.\\nLegislators, regulators (such as the UK’s Equality and Human Rights \\nCommission)\\xa0and courts need to consider the methodology for ensuring and \\noverseeing compliance with the right to non-discrimination with regard to AI. \\nNew tools may be necessary to detect discrimination, as AI systems operate \\ndifferently and are generally more opaque than non-AI decision-making processes. \\nTo be able to review the operation of AI effectively, the law and the courts may \\nhave to take more account of statistical method as well as context, while also \\nadopting more standardized thresholds where possible and appropriate.108'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 29, 'page_label': '28'}, page_content='have to take more account of statistical method as well as context, while also \\nadopting more standardized thresholds where possible and appropriate.108 \\nIn parallel, AI developers need to ensure that automated decision-making \\nmatches its human equivalent by developing capacity to take account of a rich \\ncomplexity of factors relevant to the circumstances of the individual. Legal and \\ntechnical communities should work together to find adequate ways of reducing \\ndiscrimination in algorithmic systems, including by embedding transparency \\nand\\xa0contextual approaches.\\n105 Centre for Data Ethics and Innovation (2020), Review into Bias in Algorithmic Decision-Making, \\nhttps://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/957259/\\nReview_into_bias_in_algorithmic_decision-making.pdf, pp. 9–10.\\n106 McGregor, L., Murray, D. and Ng, V. (2019), ‘International Human Rights Law as a Framework for Algorithmic'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 29, 'page_label': '28'}, page_content='Review_into_bias_in_algorithmic_decision-making.pdf, pp. 9–10.\\n106 McGregor, L., Murray, D. and Ng, V. (2019), ‘International Human Rights Law as a Framework for Algorithmic \\nAccountability’, International & Comparative Law Quarterly, 68(2), April 2019, https://doi.org/10.1017/\\nS0020589319000046, p. 326.\\n107  Centre for Data Ethics and Innovation (2020), Review into Bias in Algorithmic Decision-Making, pp. 9–10.\\n108 Wachter, S., Mittelstadt, B. and Russell, C. (2020), ‘Why Fairness Cannot be Automated: Bridging the \\nGap between EU Non-Discrimination Law and AI’, Computer Law & Security Review, 41 (2021): 105567, \\nhttps://ssrn.com/abstract=3547922.\\nAdopting well-established and internationally \\naccepted standards in human rights law \\nminimizes the need for fresh debates \\non\\xa0highly\\xa0contested concepts in ethics.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 30, 'page_label': '29'}, page_content='AI governance and human rights\\nResetting the relationship\\n29 Chatham House\\n4.2.3 Autonomy\\nThe challenges presented by AI\\nAI poses two principal risks to autonomy. First, empathic AI109 is developing \\nthe capacity to recognize and measure human emotion as expressed through \\nbehaviour, expressions, body language, voice and so on.110 Second, it is increasingly \\nable to react to and simulate human emotion, with the aim of generating empathy \\nfrom its human users. Empathic AI is beginning to appear in a multitude of devices \\nand settings, from games and mobile phones, to cars, homes and toys, and across \\nindustries including education, insurance and retail. Research is ongoing as to how \\nAI can monitor the mental111 and physical health of employees.112\\nSome empathic AI has clear benefits. From 2022, EU law requires that new vehicles \\nincorporate telemetrics for the detection of drowsiness and distraction in drivers.113'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 30, 'page_label': '29'}, page_content='Some empathic AI has clear benefits. From 2022, EU law requires that new vehicles \\nincorporate telemetrics for the detection of drowsiness and distraction in drivers.113 \\nBesides the obvious safety benefits for drivers and operators of machinery, empathic \\nAI offers assistive potential (particularly for disabled people) and prospects for \\nimproving mental health. Other possible enhancements to daily lives range from \\nrecommendations for cures to ailments to\\xa0curated music-streaming.114\\nHowever, empathic AI also carries major risks. The science of emotion detection \\nand recognition is still in development, meaning that, at present, any chosen \\nlabelling or scoring of emotion is neither definitive nor necessarily accurate. Aside \\nfrom these concerns, empathic AI also raises significant risks of both surveillance \\nand manipulation. The use of emotion recognition technology for surveillance \\nis likely to breach the right to privacy and other rights\\xa0– for example, when used'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 30, 'page_label': '29'}, page_content='and manipulation. The use of emotion recognition technology for surveillance \\nis likely to breach the right to privacy and other rights\\xa0– for example, when used \\nto monitor employee or student engagement or to identify criminal suspects.115 \\nMore broadly, monitoring of emotion, as of all behaviour, is likely to influence how \\npeople behave\\xa0– potentially having a chilling effect on the freedoms of expression, \\nassociation and assembly, and even of thought.116 This is particularly the case \\nwhere access to rights and benefits is made contingent on an individual meeting \\nstandards of behaviour, as for instance in China’s ‘social credit’ system.117\\nRegarding manipulation, empathic AI blurs the line between recommendation \\nand\\xa0direction. Algorithms may influence individuals’ emotions and thoughts, \\n109 Also known as ‘emotion AI’, ‘emotional AI’, and ‘affective computing’ (a term coined by Rosalind Picard in\\xa0her'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 30, 'page_label': '29'}, page_content='and\\xa0direction. Algorithms may influence individuals’ emotions and thoughts, \\n109 Also known as ‘emotion AI’, ‘emotional AI’, and ‘affective computing’ (a term coined by Rosalind Picard in\\xa0her \\n1995 book on the topic). One example is sentiment analysis, which entails the assessment of text (such as\\xa0customer \\nfeedback and comments) and, increasingly, of images (of people, objects or scenes) for\\xa0emotional tone.\\n110 For an overview and research in this field, see Emotional AI Lab (undated), www.emotionalai.org.\\n111 For example, Lewis, R. et al. (2022), ‘Can a Recommender System Support Treatment Personalisation \\nin\\xa0Digital Mental Health Therapy?’, MIT Media Lab, 21 April 2022, https://www.media.mit.edu/publications/\\nrecommender-system-treatment-personalisation-in-digital-mental-health.\\n112 Whelan, E. et al. (2018), ‘How Emotion-Sensing Technology Can Reshape the Workplace’, MIT Sloan \\nManagement Review, 5 February 2018, https://sloanreview.mit.edu/article/how-emotion-sensing-technology-'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 30, 'page_label': '29'}, page_content='Management Review, 5 February 2018, https://sloanreview.mit.edu/article/how-emotion-sensing-technology-\\ncan-reshape-the-workplace.\\n113 General Safety Regulation, Regulation (EU) 2019/2144 of the European Parliament and of the Council, \\nhttps://eur-lex.europa.eu/eli/reg/2019/2144/oj.\\n114 For a discussion of the potential of empathic AI, see McStay, A. (2018), Emotional AI: The Rise of Empathic \\nMedia, London: SAGE Publications Ltd, chap. 1.\\n115 Article 19 (2021), Emotional Entanglement: China’s emotion recognition market and its implications for human \\nrights, January 2021, https://www.article19.org/wp-content/uploads/2021/01/ER-Tech-China-Report.pdf.\\n116 UN Special Rapporteur on Freedom of Religion or Belief (2021), Freedom of Thought, A/76/380 \\n(October\\xa02021), https://undocs.org/Home/Mobile?FinalSymbol=A%2F76%2F380&Language=E&DeviceType \\n=Desktop&LangRequested=False, para. 54.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 30, 'page_label': '29'}, page_content='(October\\xa02021), https://undocs.org/Home/Mobile?FinalSymbol=A%2F76%2F380&Language=E&DeviceType \\n=Desktop&LangRequested=False, para. 54.\\n117 See the discussion of China’s social credit system in Taylor, E., Jones, K. and Caeiro, C. (2022), ‘Technical Standards \\nand Human Rights: The Case of New IP’, in Sabatini, C. (2022), Reclaiming human rights in a changing world order, \\nWashington, DC and London: Brookings Institution Press and Royal Institute of International Affairs, pp. 185–215.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 31, 'page_label': '30'}, page_content='AI governance and human rights\\nResetting the relationship\\n30 Chatham House\\nand\\xa0the decisions they make, without them being aware.118 The distinction\\xa0between \\nacceptable influence and unacceptable manipulation has long been blurred. At one \\nend of the spectrum, nudge tactics such as tailored advertising and\\xa0promotional \\nsubscriptions are commonly accepted as marketing tools. At the other, \\nmisrepresentation and the use of fake reviews are considered unacceptable and \\nattract legal consequences. Between those extremes, the boundaries are unclear.\\nRetail and other commercial sectors are increasingly harnessing empathic AI \\ntechnology. For example, just as advertising has long sought to take advantage \\nof\\xa0mood and feeling to promote sales, micro-targeting could be taken a step \\nfurther by including emotion detection as one of its parameters, with the aim of \\npersuading an individual to book a holiday or sign up for a therapy class, among'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 31, 'page_label': '30'}, page_content='further by including emotion detection as one of its parameters, with the aim of \\npersuading an individual to book a holiday or sign up for a therapy class, among \\nother things. There are currently no parameters by which to assess the acceptable \\nlimits of influence, even as persuasive tactics edge further towards manipulation.\\nIn social media, too, AI offers potential for emotional manipulation, not least \\nwhen\\xa0it comes to politics. In particular, the harnessing of empathic AI exacerbates \\nthe threat posed by campaigns of political disinformation and manipulation. \\nAI\\xa0use to harness emotion for political ends has already been widely reported. \\nThis includes the deployment of fake or distorted material, often micro-targeted, \\nto simulate empathy and inflame emotions.119 Regulation and other policies are \\nnow being targeted at extreme forms of online influence,120 but the parameters \\nof\\xa0acceptable behaviour by political actors remain unclear.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 31, 'page_label': '30'}, page_content='now being targeted at extreme forms of online influence,120 but the parameters \\nof\\xa0acceptable behaviour by political actors remain unclear.\\nEmpathic AI could have major impacts on all aspects of life. Imagine, for example, \\ntechnology that alters children’s emotional development, or that tailors career \\nadvice to young people in an emotionally empathic manner that appears to expand \\nbut actually has the effect of limiting choice. Vulnerable groups, including minors \\nand adults with disabilities, are particularly at risk. Researchers of very large \\nlanguage models have argued for greater consideration of the risks of human \\nmimicry and abuse of empathy they create.121\\nThe draft EU Artificial Intelligence Act would ban the clearest potential \\nfor\\xa0manipulation inherent in AI by prohibiting AI that deploys subliminal \\ntechniques\\xa0to\\xa0distort people’s behaviour in a manner that may cause them \\n‘physical or psychological harm’.122 The Act would also limit the uses of individual'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 31, 'page_label': '30'}, page_content='techniques\\xa0to\\xa0distort people’s behaviour in a manner that may cause them \\n‘physical or psychological harm’.122 The Act would also limit the uses of individual \\n‘trustworthiness’ profiling. As most empathic AI involves the use of biometric \\n118 Council of Europe (2019), Declaration by the Committee of Ministers on the Manipulative Capabilities \\nof\\xa0Algorithmic Processes, Decl(13/02/2019)1.\\n119 Jones (2019), Online Disinformation and Political Discourse: Applying a Human Rights Framework.\\n120 For example, European Democracy Action Plan and related legislation: Communication from the Commission \\nto the European Parliament, the Council, the European Economic and Social Committee and the Committee of \\nthe Regions (2020), On the European Democracy Action Plan, COM/2020/790 final https://ec.europa.eu/info/\\nstrategy/priorities-2019-2024/new-push-european-democracy/european-democracy-action-plan_en#what-is-'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 31, 'page_label': '30'}, page_content='strategy/priorities-2019-2024/new-push-european-democracy/european-democracy-action-plan_en#what-is-\\nthe-european-democracy-action-plan. In the UK, the National Security Bill, clauses 13 and 14 would criminalize \\nforeign interference, while the government has announced its intention to make foreign interference a prioritized \\noffence for the purposes of\\xa0the\\xa0Online Safety Bill.\\n121 Bender, E. et al. (2021), ‘On the Dangers of Stochastic Parrots: Can Language Models be Too Big?’, event, \\nFAccT 2021, 3–10 March 2021, https://dl.acm.org/doi/pdf/10.1145/3442188.3445922; Bender, E. (2022), \\n‘Human-like Programs Abuse Our Empathy\\xa0– even Google Engineers Aren’t Immune’, Guardian, 14 June 2022, \\nhttps://www.theguardian.com/commentisfree/2022/jun/14/human-like-programs-abuse-our-empathy-even-\\ngoogle-engineers-arent-immune.\\n122  Proposal for a Regulation of the European Parliament and of the Council laying down harmonised rules'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 31, 'page_label': '30'}, page_content='google-engineers-arent-immune.\\n122  Proposal for a Regulation of the European Parliament and of the Council laying down harmonised rules \\non\\xa0artificial intelligence (Artificial Intelligence Act) and amending certain Union legislative acts, COM/2021/206 \\nhttps://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX:52021PC0206, Article 5.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 32, 'page_label': '31'}, page_content='AI governance and human rights\\nResetting the relationship\\n31 Chatham House\\ndata,\\xa0it is likely to be subject to the Act’s enhanced scrutiny for ‘high-risk’ AI. \\nHowever, empathic AI that operates on an anonymous basis may not be covered.\\nInternational human rights law\\nAs well as privacy, human rights law protects autonomy. It protects the right to \\nfreedom of thought and the right to hold opinions without interference, as well \\nas the better-known and -understood rights to freedom of expression, freedom \\nof assembly and association, and freedom of conscience and religion. The EU \\nCharter of Fundamental Rights also protects the right to ‘mental integrity’. Prior \\nto\\xa0recent technological developments, the rights to freedom of thought and opinion \\nwere underexplored. Further guidance is now emerging: for example, the UN \\nspecial rapporteur on freedom of religion or belief has recently issued guidance \\non\\xa0freedom of thought.123'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 32, 'page_label': '31'}, page_content='were underexplored. Further guidance is now emerging: for example, the UN \\nspecial rapporteur on freedom of religion or belief has recently issued guidance \\non\\xa0freedom of thought.123\\nChildren’s rights merit special consideration in this area. In addition to questions \\nover privacy and the ability of minors to give consent when providing personal \\ndata, the UN Committee on the Rights of the Child has called for practices that \\nrely on neuromarketing and emotional analytics to be prohibited from direct \\nor\\xa0indirect engagement with children,124 and for states to prohibit manipulation \\nor interference with the child’s right to freedom of thought and belief through \\nemotional analytics and interference.125\\nApplication of human rights law to the challenges presented by AI\\nThere are considerable concerns about the extent to which emotion recognition, \\ncapture and simulation may infringe human rights, in ways that are not necessary \\nor proportionate to perceived benefits.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 32, 'page_label': '31'}, page_content='capture and simulation may infringe human rights, in ways that are not necessary \\nor proportionate to perceived benefits.\\nAt present, challenges to autonomy are generally viewed through the prism \\nof\\xa0privacy and data protection. While this enables consideration of the impacts \\nof surveillance, it is not a sufficient framework by which to consider issues of \\nmanipulation. Empathic AI can still be effective without capturing personal data\\xa0– \\nexamples include billboards that adapt their advertising according to the reactions \\nof people walking past, stores that adapt their advertising and marketing after \\ncapturing shoppers’ reactions in real time or bots that reflect unnamed users’ \\nemotions in order to influence their decision-making.\\nInitiatives to set limits on simulated empathy, such as the technical standard under \\ndevelopment by the IEEE,126 ought to take account of the absolute nature of the \\nrights to freedom of opinion and freedom of thought, as well as the right to mental'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 32, 'page_label': '31'}, page_content='development by the IEEE,126 ought to take account of the absolute nature of the \\nrights to freedom of opinion and freedom of thought, as well as the right to mental \\nintegrity and the rights of the child. Further legislative and judicial consideration \\nis\\xa0needed to establish precisely what constraints human rights law imposes on \\npotentially manipulative uses of AI, and precisely what safeguards it imposes \\nto\\xa0prevent the erosion of autonomy.\\n123  UN Special Rapporteur on Freedom of Religion or Belief (2021), Freedom of Thought, A/76/380 \\n(October\\xa02021), https://undocs.org/Home/Mobile?FinalSymbol=A%2F76%2F380&Language=E&Device \\nType=Desktop&LangRequested=False, paras 68–72.\\n124  UN Committee on the Rights of the Child (2021), General Comment No. 25 on children’s rights in relation \\nto\\xa0the digital environment, CRC/C/GC/25, 2 March 2021, para. 42.\\n125  Ibid., para. 62.\\n126 IEEE 7000-P7014, Empathic Technology Working Group on a Standard for ethical considerations in emulated'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 32, 'page_label': '31'}, page_content='to\\xa0the digital environment, CRC/C/GC/25, 2 March 2021, para. 42.\\n125  Ibid., para. 62.\\n126 IEEE 7000-P7014, Empathic Technology Working Group on a Standard for ethical considerations in emulated \\nempathy in autonomous and intelligent systems.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 33, 'page_label': '32'}, page_content='AI governance and human rights\\nResetting the relationship\\n32 Chatham House\\nMeanwhile, some are reaching their own conclusions on empathic AI. For \\nexample, a coalition of prominent civil society organizations has argued that the \\nEU’s Artificial Intelligence Act should prohibit all emotion recognition AI, subject \\nto limited exceptions for health, research and assistive technologies.127 In June \\n2022, Microsoft announced that it would phase out emotion recognition from \\nits Azure Face API facial recognition services. In that announcement, Microsoft \\nnoted the lack of scientific consensus on the definition of ‘emotions’, challenges \\nof generalizations across diverse populations, and privacy concerns as well as \\nawareness of potential misuse of the technology for stereotyping, discrimination \\nor\\xa0unfair denial of services.128\\n4.2.4 Equality: implementation of economic and social rights\\nInternational human rights law protects a wide range of economic and social'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 33, 'page_label': '32'}, page_content='or\\xa0unfair denial of services.128\\n4.2.4 Equality: implementation of economic and social rights\\nInternational human rights law protects a wide range of economic and social \\nrights, and provides an anchor for sustainable development.129 Just as AI offers \\nopportunities to achieve implementation of the SDGs, so it offers significant \\npotential to improve the implementation of rights such as those to education, \\nhealth, social security and work. Equality is key to achieving this potential: not \\njust through the avoidance of discrimination, but through AI that benefits all \\ncommunities and through the provision of equal opportunity for all in accessing \\nthe benefits. Failure to realize such opportunities risks not only entrenching but \\nexacerbating current social divisions.\\nIdeally, such provision would begin with research into AI technologies that \\nwould\\xa0help to implement the SDGs, and funding for the development and rollout'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 33, 'page_label': '32'}, page_content='exacerbating current social divisions.\\nIdeally, such provision would begin with research into AI technologies that \\nwould\\xa0help to implement the SDGs, and funding for the development and rollout \\nof those technologies. The challenges are to incentivize developments that benefit \\nall communities, as well as those that are most profitable; and to ensure that no \\nAI\\xa0systems operate to the detriment of vulnerable communities.\\n4.2.5 Fairness and due process in decision-making\\nAI decision-making brings a risk that the ‘computer says no’ in respect of significant \\nlife decisions, without possibility of review or challenge. Aside from discrimination, \\nthis also raises questions as to fairness of process and quality of decision-making in \\nAI systems. It concerns both whether the use made of AI to reach the decision was \\nfair, and whether AI reached or contributed to a fair decision in the specific case\\xa0– \\nand if not, what the recourse might be.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 33, 'page_label': '32'}, page_content='fair, and whether AI reached or contributed to a fair decision in the specific case\\xa0– \\nand if not, what the recourse might be.\\nIn making decisions, AI may segment people by reference to a wide range \\nof\\xa0factors\\xa0and without consideration as to whether segmentation is appropriate \\nin\\xa0the particular case. These factors may be unrelated to the decision in question, \\n127 Joint Civil Society Amendments to the Artificial Intelligence Act (2022), Prohibit Emotion Recognition in \\nthe Artificial Intelligence Act, May 2022, https://www.accessnow.org/cms/assets/uploads/2022/05/Prohibit-\\nemotion-recognition-in-the-Artificial-Intelligence-Act.pdf.\\n128 Bird, S. (2022), ‘Responsible AI investments and safeguards for facial recognition’, Microsoft Azure blog, \\n21 June 2022, https://azure.microsoft.com/en-us/blog/responsible-ai-investments-and-safeguards-for-\\nfacial-recognition.\\n129 UN Office of the High Commissioner on Human Rights (undated), ‘OHCHR and the 2030 Agenda'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 33, 'page_label': '32'}, page_content='facial-recognition.\\n129 UN Office of the High Commissioner on Human Rights (undated), ‘OHCHR and the 2030 Agenda \\nfor\\xa0Sustainable Development’, https://www.ohchr.org/en/sdgs.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 34, 'page_label': '33'}, page_content='AI governance and human rights\\nResetting the relationship\\n33 Chatham House\\nbut decisions that treat some people unfairly in comparison to others may still result. \\nFor example, if a travel insurance provider were to double the premiums offered \\nto people who had opted out of receiving unsolicited marketing material, it\\xa0would \\nnot be discriminating on the basis of a protected characteristic. Its decision-making \\nprocess would however be biased against those who have opted out.\\nWhere an individual’s human rights are affected by a decision made by \\na\\xa0public authority, they should be able to seek remedy130 and will usually be \\nable\\xa0to\\xa0challenge the decision in public law\\xa0– for example, by way of judicial review. \\nDecision-making processes need to be sufficiently transparent to enable such \\nreview. Individuals should know who the decision-maker is, the factors on which \\nthe decision is made and be able to verify the accuracy of any personal data used'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 34, 'page_label': '33'}, page_content='review. Individuals should know who the decision-maker is, the factors on which \\nthe decision is made and be able to verify the accuracy of any personal data used \\nin the process. There should be adequate human involvement or oversight\\xa0– \\nwhile acknowledging that human involvement may not be essential in every \\ncase\\xa0and\\xa0is\\xa0not necessarily a failsafe.131\\nInternational human rights law stipulates requirements for fairness in legal \\nproceedings. Public and private law bases of challenge to decisions commonly \\nreflect these requirements, and they can provide the basis for guidelines on \\nminimum standards for transparency, human control and accountability \\nthrough\\xa0possibility of review for all AI activities.\\n4.2.6 Other rights\\nAI, used in different contexts, may have serious implications for the full range \\nof human rights.\\nFor example, the use of AI for content curation and moderation in social media \\nmay affect the rights to freedom of expression and access to information. The use'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 34, 'page_label': '33'}, page_content='of human rights.\\nFor example, the use of AI for content curation and moderation in social media \\nmay affect the rights to freedom of expression and access to information. The use \\nof analytics to contribute to decisions on child safeguarding, meanwhile, may affect \\nthe right to family life.132 The use of facial recognition technology risks serious \\nimpact on the rights to freedom of assembly and association, and even on the right \\nto vote freely. In extreme cases\\xa0– for example, in weapons for military use\\xa0– AI risks \\nundermining the right to life and the right to integrity of the person if not closely \\ncircumscribed. In each of these areas, existing human rights can form the basis \\nfor\\xa0safeguards delimiting the appropriate scope of AI activity.\\n130 International Covenant on Civil and Political Rights, Art. 2(3); European Convention on Human Rights, Art. 13.\\n131 In the data protection context, there is pressure to change Article 22 of GDPR, which currently requires that'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 34, 'page_label': '33'}, page_content='131 In the data protection context, there is pressure to change Article 22 of GDPR, which currently requires that \\ndecisions with legal or similarly significant effects for individuals, using their personal data, shall not be based \\nsolely on automated processing.\\n132  Anning, S. (2022), ‘The Interplay of Explicit and Tacit Knowledge With Automated Systems for Safeguarding \\nChildren’, techUK Industry Views blog, 21 March 2022, https://www.techuk.org/resource/the-interplay-of-\\nexplicit-and-tacit-knowledge-with-automated-systems-for-safeguarding-children.html.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 35, 'page_label': '34'}, page_content='34 Chatham House\\n05 \\nProcesses of AI \\ngovernance: the \\ncontribution of \\nhuman rights\\nRegulators and companies should follow human rights \\nprocess requirements as they devise and implement \\nAI\\xa0governance processes.\\n5.1 Processes: the landscape\\nThe processes that governments and companies should follow in order to meet \\nAI\\xa0governance standards are evolving rapidly.\\n5.1.1 Regulation\\nGovernments are increasingly considering cross-sectoral regulation of AI on \\nthe\\xa0basis that statutory obligations would help create a level playing field for safe \\nand ethical AI and bolster consumer trust, while mitigating the risk that pre-AI \\nregulation applies to AI in haphazard fashion.133 The EU is furthest along in this \\nprocess, with its draft Artificial Intelligence Act that would ban the highest-risk \\n133 In the UK, regulators have established the Digital Regulation Cooperation Forum to facilitate a joined-up approach'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 35, 'page_label': '34'}, page_content='133 In the UK, regulators have established the Digital Regulation Cooperation Forum to facilitate a joined-up approach \\nto technology regulation. In the US, the Federal Trade Commission has explained how it stands ready to\\xa0enforce \\nexisting legislation\\xa0– including the Federal Trade Commission Act, the Fair Credit Reporting Act, and the Equal \\nCredit Opportunity Act\\xa0– against bias or other unfair outcomes in automated decision-making. See Jillson,\\xa0E. (2021), \\n‘Aiming for truth, fairness, and equity in your company’s use of AI’, Federal Trade Commission Business Blog, 19 April \\n2021, https://www.ftc.gov/business-guidance/blog/2021/04/aiming-truth-fairness-equity-your-companys-use-ai.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 36, 'page_label': '35'}, page_content='AI governance and human rights\\nResetting the relationship\\n35 Chatham House\\nforms of AI and subject other ‘high risk’ AI to conformity assessments. In the US, \\nCongress is considering a draft Algorithmic Accountability Act.134 The British \\ngovernment, having considered the case for cross-cutting AI regulation, has \\nrecently announced plans for a non-statutory, context-specific approach that \\naims\\xa0to be pro-innovation and to focus primarily on high-risk concerns.135\\nWhile the British government, among others, has expressed concern that general \\nregulation of AI may stifle innovation, many researchers and specialists make \\nthe\\xa0opposite argument.136 Sector-specific regulation may not tackle AI risks that \\nstraddle sectors, such as the impact of AI in workplaces. Well-crafted regulation \\nshould only constrain undesirable activity, and should provide scope for \\nexperimentation without liability within its parameters, including for small'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 36, 'page_label': '35'}, page_content='should only constrain undesirable activity, and should provide scope for \\nexperimentation without liability within its parameters, including for small \\ncompanies. Moreover, it is argued that responsible businesspeople would rather \\noperate in a marketplace regulated by high standards of conduct, with clear rules, \\na\\xa0level playing field and consequent consumer trust, than in an unregulated \\nenvironment in which they have to decide for themselves the limits of ethical \\nbehaviour. Most decision-makers in industry want to do things the right way \\nand\\xa0need the tools by which to do so.\\nIn addition to regulating AI itself, there are also calls for regulation to ensure that \\nrelated products are appropriately harnessed for the public good. For example, the \\nUK-based Ada Lovelace Institute has called for new legislation to govern biometric \\ntechnologies.137 Similarly, there is discussion of regulation of ‘digital twins’\\xa0–'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 36, 'page_label': '35'}, page_content='UK-based Ada Lovelace Institute has called for new legislation to govern biometric \\ntechnologies.137 Similarly, there is discussion of regulation of ‘digital twins’\\xa0– \\ni.e.\\xa0computer-generated digital facsimiles of physical objects or systems\\xa0– to ensure \\nthat the vast amounts of valuable data they generate is used for public good rather \\nthan for commercial exploitation or even public control.138\\nSome sector-specific laws are already being updated in light of AI’s expansion. \\nFor\\xa0example, the European Commission’s proposal to replace the current \\nConsumer Credit Directive aims to prohibit discrimination and ensure accuracy, \\ntransparency and use of appropriate data in creditworthiness assessments, with \\na right to human review of automated decisions.139 An analysis of legislation \\n134 H.R. 6580\\xa0– Algorithmic Accountability Act of 2022, https://www.congress.gov/bill/117th-congress/\\nhouse-bill/6580/text.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 36, 'page_label': '35'}, page_content='134 H.R. 6580\\xa0– Algorithmic Accountability Act of 2022, https://www.congress.gov/bill/117th-congress/\\nhouse-bill/6580/text.\\n135 UK Government (2022), Establishing a pro-innovation approach to regulating AI, Policy Paper, 20 July 2022, \\nhttps://www.gov.uk/government/publications/establishing-a-pro-innovation-approach-to-regulating-ai/\\nestablishing-a-pro-innovation-approach-to-regulating-ai-policy-statement.\\n136 See, for example, Ada Lovelace Institute (2021), ‘Regulate to innovate’, 29 November 2021, \\nhttps://www.adalovelaceinstitute.org/report/regulate-innovate.\\n137 Chang, M. (2022), ‘Countermeasures: the need for new legislation to govern biometric technologies \\nin the UK’, London: Ada Lovelace Institute, 29 June 2022, https://www.adalovelaceinstitute.org/report/\\ncountermeasures-biometric-technologies.\\n138  See, for example, Centre for Digital Built Britain (2018), The Gemini Principles, Cambridge: University'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 36, 'page_label': '35'}, page_content='countermeasures-biometric-technologies.\\n138  See, for example, Centre for Digital Built Britain (2018), The Gemini Principles, Cambridge: University \\nof\\xa0Cambridge, https://www.cdbb.cam.ac.uk/system/files/documents/TheGeminiPrinciples.pdf.\\n139  European Commission (2021), Proposal for a Directive of the European Parliament and of the Council \\non Consumer Credits, COM/2021/347 final, 30 June 2021, https://eur-lex.europa.eu/legal-content/EN/\\nTXT/?uri=COM:2021:347:FIN.\\nWithout... clear standards and external involvement \\nor accountability, there\\xa0is\\xa0a\\xa0risk of\\xa0‘ethics-washing’ \\nrather than genuine mitigation\\xa0of\\xa0risks.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 37, 'page_label': '36'}, page_content='AI governance and human rights\\nResetting the relationship\\n36 Chatham House\\nin\\xa025 countries found that the pieces of primary legislation containing the phrase \\n‘artificial intelligence’ grew from one in 2016 to 18 in 2021, many of these specific \\nto a sector or issue.140 Governments are also considering amendments to existing \\ncross-sectoral regulation such as GDPR, which does not fully anticipate the \\nchallenges or the potential of AI.\\n5.1.2 Impact assessments and audit\\nThe most rapid area of growth concerns algorithmic impact assessments (AIAs) \\nand audits, which attempt to assess and manage ethical risks in the operation of \\nalgorithmic systems. While the terminology is not used consistently, AIAs tend \\nto assess impact prospectively (i.e.\\xa0before a system is in use), while audits are \\nretrospective (i.e.\\xa0looking back at a period of use).141\\nA number of bodies are currently developing template risk assessments for use'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 37, 'page_label': '36'}, page_content='retrospective (i.e.\\xa0looking back at a period of use).141\\nA number of bodies are currently developing template risk assessments for use \\nby creators or deployers of AI systems. For example, the US National Institute \\nof Standards and Technology (NIST) has released a draft AI Risk Management \\nFramework.142 The Singapore government is piloting a governance framework \\nand\\xa0toolkit known as AIVerify.143 The EU’s Artificial Intelligence Act will \\nencourage conformity assessment with technical standards for high-risk AI.144 \\nThe British government is keen to see a new market in AI assurance services \\nestablished in the UK, by which assurers would certify that AI systems meet their \\nstandards and so are trustworthy.145 The UK’s Alan Turing Institute has proposed \\nan assurance framework called HUDERIA.146 Technical standards bodies are \\ndeveloping frameworks, such as the IEEE’s Standard Model Process.147 There are \\nacademic versions, such as capAI,148 a conformity assessment process designed'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 37, 'page_label': '36'}, page_content='developing frameworks, such as the IEEE’s Standard Model Process.147 There are \\nacademic versions, such as capAI,148 a conformity assessment process designed \\nby\\xa0a\\xa0consortium of Oxford-based ethicists, and the European Law Institute’s \\nModel Rules on Impact Assessment.149 There are also fledgling external review \\nprocesses\\xa0such as Z-Inspection.150\\n140 Stanford University (2022), Artificial Intelligence Index Report 2022, https://aiindex.stanford.edu/\\nwp-content/uploads/2022/03/2022-AI-Index-Report_Chapter-5.pdf, chap. 5.\\n141  The terminology of ‘impact assessment’ and ‘audit’ is used in different ways by different policymakers and \\nacademics. For a detailed discussion, see Ada Lovelace Institute and DataKind UK (2020), Examining the Black \\nBox, https://www.adalovelaceinstitute.org/wp-content/uploads/2020/04/Ada-Lovelace-Institute-DataKind-UK-\\nExamining-the-Black-Box-Report-2020.pdf.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 37, 'page_label': '36'}, page_content='Box, https://www.adalovelaceinstitute.org/wp-content/uploads/2020/04/Ada-Lovelace-Institute-DataKind-UK-\\nExamining-the-Black-Box-Report-2020.pdf.\\n142 National Institute of Standards and Technology (2022), AI Risk Management Framework: Initial Draft, \\n17\\xa0March 2022, https://www.nist.gov/system/files/documents/2022/03/17/AI-RMF-1stdraft.pdf.\\n143 Infocomm Media Development Authority (2022), Invitation to Pilot AI Verify AI Governance Testing Framework \\nand Toolkit, 25 May 2022, https://file.go.gov.sg/aiverify.pdf.\\n144 McFadden, M., Jones, K., Taylor, E. and Osborn, G. (2021), Harmonising Artificial Intelligence: The role of \\nstandards in the EU AI Regulation, Oxford Commission on AI & Good Governance, https://oxcaigg.oii.ox.ac.uk/\\nwp-content/uploads/sites/124/2021/12/Harmonising-AI-OXIL.pdf.\\n145 UK Centre for Data Ethics and Innovation (2021), The Roadmap to an Effective AI Assurance Ecosystem'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 37, 'page_label': '36'}, page_content='wp-content/uploads/sites/124/2021/12/Harmonising-AI-OXIL.pdf.\\n145 UK Centre for Data Ethics and Innovation (2021), The Roadmap to an Effective AI Assurance Ecosystem \\nhttps://www.gov.uk/government/publications/the-roadmap-to-an-effective-ai-assurance-ecosystem/the-\\nroadmap-to-an-effective-ai-assurance-ecosystem.\\n146 Alan Turing Institute (2021), Human Rights, Democracy, and the Rule of Law Assurance Framework \\nfor AI Systems: A proposal prepared for the Council of Europe’s Ad hoc Committee on Artificial Intelligence, \\nhttps://rm.coe.int/huderaf-coe-final-1-2752-6741-5300-v-1/1680a3f688.\\n147  IEEE Standard Model Process for Addressing Ethical Concerns during System Design, IEEE Std 7000-2021. \\nSee also ISO/IEC JTC 1/SC 42 Joint Committee SC 42 on Standardisation in the area of Artificial Intelligence.\\n148 Floridi, L. et al. (2022), capAI - A Procedure for Conducting Conformity Assessment of AI Systems in Line with'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 37, 'page_label': '36'}, page_content='148 Floridi, L. et al. (2022), capAI - A Procedure for Conducting Conformity Assessment of AI Systems in Line with \\nthe\\xa0EU Artificial Intelligence Act, 23 March 2022, http://dx.doi.org/10.2139/ssrn.4064091.\\n149 European Law Institute (2022), Model Rules on Impact Assessment of Algorithmic Decision-Making Systems Used \\nby Public Administration, https://www.europeanlawinstitute.eu/fileadmin/user_upload/p_eli/Publications/\\nELI_Model_Rules_on_Impact_Assessment_of_ADMSs_Used_by_Public_Administration.pdf.\\n150 Zicari, R. et al. (2021), ‘Z-Inspection®: A Process to Assess Trustworthy AI’, IEEE Transactions on Technology \\nand Society, 2(2), pp. 83–97, https://doi.org/10.1109/TTS.2021.3066209.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 38, 'page_label': '37'}, page_content='AI governance and human rights\\nResetting the relationship\\n37 Chatham House\\nLarger businesses have, meanwhile, established their own assessment processes. \\nFor example, Google conducts ethical reviews of AI applications it plans to \\nlaunch.151 IBM has an AI Ethics Board providing centralized governance, review \\nand decision-making.152 Rolls-Royce’s Aletheia Framework comprises a 32-step \\npractical toolkit for organizations developing and deploying AI.153\\nTypically, AIA processes invite AI developers, providers and users to elicit the \\nethical values engaged by their systems, refine those values and then assess their \\nproposed or actual AI products and systems (both data and models) against those \\nvalues, identifying and mitigating risks. Some models take a restrictive view of \\nethics, focusing primarily on data governance, fairness and procedural aspects \\nrather than all rights.154 A further tool proposed for data governance is data sheets'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 38, 'page_label': '37'}, page_content='ethics, focusing primarily on data governance, fairness and procedural aspects \\nrather than all rights.154 A further tool proposed for data governance is data sheets \\nor ‘nutrition labels’ that summarize the characteristics and intended uses of data \\nsets, to reduce the risk of inappropriate transfer and use of datasets.155\\nSome governments are introducing impact assessments which are either \\nmandatory or carry strong incentives for compliance. For example, Canada’s \\nDirective on Automated Decision-Making requires Canadian government \\ndepartments to complete and publish an AIA prior to production of any automated \\ndecision system.156 The US’s draft Algorithmic Accountability Act, proposed \\nin Congress in 2019 and again in 2022, would require impact assessment of \\nsignificant automated decisions taken by larger entities.157 In the UK, the Ada \\nLovelace Institute has published a detailed proposal for an AIA to be completed'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 38, 'page_label': '37'}, page_content='significant automated decisions taken by larger entities.157 In the UK, the Ada \\nLovelace Institute has published a detailed proposal for an AIA to be completed \\nby any organization seeking professional access to the National Health Service \\n(NHS)’s proposed National Medical Imaging Platform\\xa0– the first known AIA \\nfor\\xa0data access in a healthcare context.158\\nWhile the identification and addressing of ethical risks is a positive step, these \\nprocesses come with challenges. Risk assessment of AI can mean identifying and \\nmitigating a broad range of impacts on individuals and communities\\xa0– a task that \\nis\\xa0potentially difficult, time-consuming and resource-intensive.159 The identification \\nand mitigation of ethical risks is not straightforward, particularly for teams whose \\nprior expertise may be technical rather than sociological. Extensive engagement \\nwith stakeholders may be necessary to obtain a balanced picture of risks. \\nResourcing challenges are magnified for smaller companies.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 38, 'page_label': '37'}, page_content='with stakeholders may be necessary to obtain a balanced picture of risks. \\nResourcing challenges are magnified for smaller companies.\\n151  Google (2022), ‘AI Principles reviews and operations’, https://ai.google/responsibilities/review-process.\\n152 IBM (2022), ‘AI Ethics’, https://www.ibm.com/artificial-intelligence/ethics.\\n153 Rolls Royce (2020), ‘The Aletheia Framework’, https://www.rolls-royce.com/sustainability/ethics-and-\\ncompliance/the-aletheia-framework.aspx.\\n154 Infocomm Media Development Authority (2022), Invitation to Pilot AI Verify AI Governance Testing Framework \\nand Toolkit, 25 May 2022, https://file.go.gov.sg/aiverify.pdf.\\n155 Gebru, T. et al. (2018), ‘Datasheets for datasets’, arXiv, 1803.09010, https://doi.org/10.48550/arXiv.1803.09010; \\n Data Nutrition Project (2021), ‘The Dataset Nutrition Label’, https://datanutrition.org/labels \\n(accessed 12 Sep. 2022).\\n156 Government of Canada (2021), Directive on Automated Decision-Making, https://www.tbs-sct.canada.ca/'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 38, 'page_label': '37'}, page_content='(accessed 12 Sep. 2022).\\n156 Government of Canada (2021), Directive on Automated Decision-Making, https://www.tbs-sct.canada.ca/\\npol/doc-eng.aspx?id=32592.\\n157  H.R. 6580\\xa0– Algorithmic Accountability Act of 2022, https://www.congress.gov/bill/117th-congress/\\nhouse-bill/6580/text.\\n158 Ada Lovelace Institute (2022), Algorithmic Impact Assessment: A Case Study in Healthcare, February 2022, \\nhttps://www.adalovelaceinstitute.org/report/algorithmic-impact-assessment-case-study-healthcare.\\n159  Nonnecke, B. and Dawson, P. (2021), ‘Human Rights Implications of Algorithmic Impact Assessments: Priority \\nConsiderations to Guide Effective Development’, Carr Center Discussion Paper Series, Harvard Kennedy School, \\nOctober 2021, https://carrcenter.hks.harvard.edu/files/cchr/files/nonnecke_and_dawson_human_rights_\\nimplications.pdf; Ada Lovelace Institute (2021), Regulate to innovate, p.52.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 39, 'page_label': '38'}, page_content='AI governance and human rights\\nResetting the relationship\\n38 Chatham House\\nIdentification of risks may not even be fully possible before an AI system enters \\ninto use, as some risks may only become apparent in the context of its deployment. \\nHence the importance of ongoing review, as well as review at the design stage. \\nYet, once a decision has been made to proceed with a technology, many companies \\nhave no vocabulary or structure for ongoing discussion of risks. In cases where \\nan AI system is developed by one organization and implemented by another, \\nthere may be no system for transferring the initial risk assessment to the recipient \\norganization and for the latter to implement ongoing risk management.\\nOnce risks have been identified, the models offer limited guidance on how to \\nbalance competing priorities, including on how to weigh ethical considerations \\nagainst commercial advantage. Subtle calculations cannot easily be rendered into'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 39, 'page_label': '38'}, page_content='balance competing priorities, including on how to weigh ethical considerations \\nagainst commercial advantage. Subtle calculations cannot easily be rendered into \\nthe simple ‘stop’ or ‘go’ recommendation typically required by corporate boards.\\nSimilarly, the audit process presents challenges: auditors may require access \\nto extensive information, including on the operation of algorithms and their \\nimpact in context. There is a lack of benchmarks by which to identify or measure \\nfactors being audited (such as bias), while audits may not take account of \\ncontextual challenges.160\\nBritish regulators have identified various problems in the current AIA and \\naudit landscape, including a lack of agreed rules and standards; inconsistency \\nof audit\\xa0focus; lack of access to systems being audited; and insufficient action \\nfollowing audits.161 There is often inadequate inclusion of stakeholder groups; \\na lack of external verification; and little connection between these emerging'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 39, 'page_label': '38'}, page_content='following audits.161 There is often inadequate inclusion of stakeholder groups; \\na lack of external verification; and little connection between these emerging \\nprocesses and any regulatory regimes or legislation.162 Recent UK research \\nconcluded that public sector policymakers should integrate practices that enable \\nregular policy monitoring and evaluation, including through institutional \\nincentives and binding legal frameworks; clear algorithmic accountability policies \\nand clear scope of algorithmic application; proper public participation and \\ninstitutional coordination across sectors and levels of governance.163\\nIt may be that many algorithms designed without regard to human rights \\nwill fail AIAs or audits. As awareness of human rights grows, so much current \\nAI may need adjusting. The Netherlands Court of Audit, having developed \\nan audit framework,164 recently audited nine algorithms used by the Dutch'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 39, 'page_label': '38'}, page_content='AI may need adjusting. The Netherlands Court of Audit, having developed \\nan audit framework,164 recently audited nine algorithms used by the Dutch \\ngovernment. It\\xa0found that six of those nine failed to meet the requirements \\nof\\xa0the audit framework on such matters as privacy protection, absence of \\nbias\\xa0and\\xa0governance processes.165\\n160 Ada Lovelace Institute and DataKind UK (2020), Examining the Black Box, p. 10.\\n161 Digital Regulation Cooperation Forum (2022), Auditing algorithms: the existing landscape, role of regulators and future \\noutlook, 28 April 2022, https://www.gov.uk/government/publications/findings-from-the-drcf-algorithmic-processing-\\nworkstream-spring-2022/auditing-algorithms-the-existing-landscape-role-of-regulators-and-future-outlook.\\n162 Ibid., p. 16.\\n163 Ada Lovelace Institute, AI Now Institute and Open Government Partnership (2021), Algorithmic Accountability \\nfor the Public Sector, https://www.opengovpartnership.org/wp-content/uploads/2021/08/algorithmic-'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 39, 'page_label': '38'}, page_content='for the Public Sector, https://www.opengovpartnership.org/wp-content/uploads/2021/08/algorithmic-\\naccountability-public-sector.pdf.\\n164 Netherlands Court of Audit (2021), ‘Understanding Algorithms’, 26 January 2021, \\nhttps://english.rekenkamer.nl/publications/reports/2021/01/26/understanding-algorithms.\\n165 Netherlands Court of Audit (2022), ‘An Audit of 9 Algorithms Used by the Dutch Government’, 18 May\\xa02022, \\nhttps://english.rekenkamer.nl/publications/reports/2022/05/18/an-audit-of-9-algorithms-used-by-the-\\ndutch-government.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 40, 'page_label': '39'}, page_content='AI governance and human rights\\nResetting the relationship\\n39  Chatham House\\nOverall, without rigorous implementation of clear standards and external \\ninvolvement or accountability, there is a risk of ‘ethics-washing’ rather than \\ngenuine mitigation of risks.\\n5.1.3 Prohibition\\nGovernments and companies are beginning to prohibit forms of AI that raise \\nthe most serious ethical concerns. However, there is no consistency in such \\nprohibitions and the rationale behind them is often not openly acknowledged.\\nFor example, some US states have banned certain uses of facial recognition \\ntechnology, which remain in widespread use in other states. The EU’s Artificial \\nIntelligence Act would prohibit certain manipulative AI practices and most use of \\nbiometric identification systems in public spaces for law enforcement purposes.166 \\nTwitter decided to ban political advertising in 2019.167\\n5.1.4 Transparency\\nA further approach is public transparency measures through registries,'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 40, 'page_label': '39'}, page_content='Twitter decided to ban political advertising in 2019.167\\n5.1.4 Transparency\\nA further approach is public transparency measures through registries, \\nrelease\\xa0of\\xa0source code or algorithmic logic (required in France under the Digital \\nRepublic Law).168 In November 2021, the UK government launched the pilot of an \\nalgorithmic transparency standard, whereby public sector organizations provide \\ninformation on their use of algorithmic tools in a standardized format for publication \\nonline. Several government algorithms have since been made public as a result.169\\n5.1.5 Procurement conditions\\nThere is likely to be a rapid growth in the imposition of conditions in the sale of \\nalgorithmic systems, particularly where purchasers such as governments and local \\nauthorities will be seeking to use those systems in the public interest. Authorities \\nare likely to impose contractual conditions requiring the system to respect \\nstipulated criteria on such matters as bias and transparency. For example, the'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 40, 'page_label': '39'}, page_content='are likely to impose contractual conditions requiring the system to respect \\nstipulated criteria on such matters as bias and transparency. For example, the \\nCity of Amsterdam has developed contractual terms requiring suppliers of AI and \\nalgorithmic systems to meet standards of explainability and transparency, including \\non what data is used and how bias is counteracted.170 Such conditions imposed \\nby\\xa0the public sector may have the effect of driving up\\xa0standards more widely.\\n166 Proposal for a Regulation of the European Parliament and of the Council laying down harmonised rules on \\nartificial intelligence (Artificial Intelligence Act) and amending certain Union legislative acts, COM/2021/206 \\nhttps://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX:52021PC0206, Article 5.\\n167  Twitter (2019), ‘Political Content’, https://business.twitter.com/en/help/ads-policies/ads-content-policies/\\npolitical-content.html.\\n168 Loi No. 2016-1321 du 7 octobre 2016 pour une République Numerique.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 40, 'page_label': '39'}, page_content='political-content.html.\\n168 Loi No. 2016-1321 du 7 octobre 2016 pour une République Numerique.\\n169 Central Digital and Data Office (2021), ‘Algorithmic Transparency Standard’, https://www.gov.uk/\\ngovernment/collections/algorithmic-transparency-standard.\\n170  Gemeente Amsterdam (2022), ‘Contractual terms for algorithms’, https://www.amsterdam.nl/innovatie/\\ndigitalisering-technologie/algoritmen-ai/contractual-terms-for-algorithms.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 41, 'page_label': '40'}, page_content='AI governance and human rights\\nResetting the relationship\\n40 Chatham House\\n5.2 Processes: human rights law\\n5.2.1 Governmental duty to protect against breaches\\nGovernments have a duty both to comply with human rights in any uses of AI they \\nadopt\\xa0– for example, in public decision-making\\xa0– and to protect individuals from \\nabuses of human rights by companies and other non-state actors. States must take \\n‘appropriate steps to prevent, investigate, punish and redress such abuse through \\neffective policies, legislation, regulations and adjudication’.171\\nGovernments are expected to find the appropriate mix of laws, policies and \\nincentives to protect against human rights harms. A ‘smart mix’ of national and \\ninternational, mandatory and voluntary measures would help to foster business \\nrespect for human rights.172 This includes requiring companies to have suitable \\ncorporate structures to identify and address human rights risk on an ongoing basis,'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 41, 'page_label': '40'}, page_content='respect for human rights.172 This includes requiring companies to have suitable \\ncorporate structures to identify and address human rights risk on an ongoing basis, \\nand to engage appropriately with external stakeholders as part of their human \\nrights assessments. Where businesses are state-owned, or work closely with the \\npublic sector, the government should take additional steps to protect against \\nhuman rights abuses through management or contractual control.173\\nGovernments’ human rights obligations mean that they cannot simply wait and \\nsee how AI develops before engaging in governance activities. They are obliged to \\ntake action, including via regulation and/or the imposition of impact assessments \\nand audits, to ensure that AI does not infringe human rights. Governments should \\nensure that they understand the implications of human rights for AI governance, \\ndeploying a dedicated capacity-building effort or technology and human rights \\noffice where a gap exists.174'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 41, 'page_label': '40'}, page_content='ensure that they understand the implications of human rights for AI governance, \\ndeploying a dedicated capacity-building effort or technology and human rights \\noffice where a gap exists.174\\nThere is an urgent need for governments to devise regulation that is both \\neffective\\xa0in ensuring that companies do not infringe individuals’ human \\nrights when designing and implementing AI systems, and that provides for \\neffective remedies in the event of any such infringement. Given the ambiguity \\nof commitments to ethics and the strength of countervailing commercial \\nconsiderations, a purely voluntary approach is unlikely to protect individuals’ \\nhuman rights adequately. Indeed, some argue that states are obliged to enact \\nlegally binding norms to protect human rights in light of the challenges posed by \\nAI\\xa0systems.175 Governments should regulate to either prohibit or require constraints \\non applications of AI, such as biometric technologies, that risk interfering'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 41, 'page_label': '40'}, page_content='AI\\xa0systems.175 Governments should regulate to either prohibit or require constraints \\non applications of AI, such as biometric technologies, that risk interfering \\nwith human rights in a manner clearly\\xa0disproportionate to any countervailing \\nlegitimate interest.\\n171 UN Office of the High Commissioner for Human Rights (2011), Guiding Principles on Business and Human \\nRights, principle 1.\\n172 Ibid., principle 3; and UN OHCHR B-Tech (2021), Bridging Governance Gaps in the Age of Technology\\xa0– Key \\ncharacteristics of the State Duty to Protect, https://www.ohchr.org/sites/default/files/Documents/Issues/\\nBusiness/B-Tech/b-tech-foundational-paper-state-duty-to-protect.pdf.\\n173 UN Office of the High Commissioner for Human Rights (2011), Guiding Principles on Business and Human \\nRights, principle 4; UN OHCHR B-Tech (2021), Bridging Governance Gaps in the Age of Technology\\xa0– Key \\ncharacteristics of the State Duty to Protect.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 41, 'page_label': '40'}, page_content='Rights, principle 4; UN OHCHR B-Tech (2021), Bridging Governance Gaps in the Age of Technology\\xa0– Key \\ncharacteristics of the State Duty to Protect.\\n174  Element AI (2019), Closing the Human Rights Gap in AI Governance, http://mediaethics.ca/wp-content/\\nuploads/2019/11/closing-the-human-rights-gap-in-ai-governance_whitepaper.pdf.\\n175  Bello y Villarino, J.-M. and Vijeyarasa, R. (2022), ‘International Human Rights, Artificial Intelligence, and \\nthe Challenge for the Pondering State: Time to Regulate?’, Nordic Journal of Human Rights, 40(1), pp. 194–215, \\nhttps://doi.org/10.1080/18918131.2022.2069919.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 42, 'page_label': '41'}, page_content='AI governance and human rights\\nResetting the relationship\\n41  Chatham House\\nGovernments should ensure that AIA and audit processes are conducted systematically, \\nemploying rigorous standards and due process, and that such processes pay due \\nregard to potential human rights impacts of AI: for example by\\xa0making assessment \\nof\\xa0human rights risks an explicit feature of such processes.176 To\\xa0incentivize corporate \\ngood practice, demonstrate respect for human rights and\\xa0facilitate remedy, states \\nshould also consider requiring companies to report publicly on any due diligence \\nundertaken and on human rights impacts identified and addressed.\\nSupervision by regulatory and administrative authorities is an important element \\nof accountability for compliance with human rights responsibilities, in parallel with \\nlegal liability for harms. As some European countries and the EU begin to implement \\nmandatory human rights and environmental due diligence obligations for larger'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 42, 'page_label': '41'}, page_content='legal liability for harms. As some European countries and the EU begin to implement \\nmandatory human rights and environmental due diligence obligations for larger \\nbusinesses,177 human rights experts are exploring administrative supervision of \\ncorporate duties as a complement to liability for harms in the courts.178\\nGovernments have legal obligations not to breach human rights in their provision \\nof AI-assisted systems. Anyone involved in government procurement of AI should \\nhave enough knowledge and information to understand the capacity and potential \\nimplications of the technology they are buying, and to satisfy themselves that \\nit meets required standards on equality, privacy and other rights (such as the \\nPublic Sector Equality Duty in the UK). Governments should negotiate the terms \\nof\\xa0public–private contracts and deploy procurement conditions to ensure that \\nAI from private providers is implemented consistently with human rights. They'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 42, 'page_label': '41'}, page_content='of\\xa0public–private contracts and deploy procurement conditions to ensure that \\nAI from private providers is implemented consistently with human rights. They \\nshould also take steps to satisfy themselves that this requirement is met. Public \\nprocurement is a\\xa0means of encouraging improvements to human rights standards \\nin\\xa0the AI industry as a whole.179 It is important also to ensure that AI systems already \\nadopted comply with human rights standards: the experience of the Netherlands \\ndemonstrates that systems adopted to date can be problematic.180\\n176  Nonnecke, B. and Dawson, P. (2022), Human Rights Impact Assessments for AI: Analysis and Recommendations, \\nNew York: Access Now, October 2022, https://www.accessnow.org/cms/assets/uploads/2022/11/Access-Now-\\nVersion-Human-Rights-Implications-of-Algorithmic-Impact-Assessments_-Priority-Recommendations-to-Guide-\\nEffective-Development-and-Use.pdf.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 42, 'page_label': '41'}, page_content='Version-Human-Rights-Implications-of-Algorithmic-Impact-Assessments_-Priority-Recommendations-to-Guide-\\nEffective-Development-and-Use.pdf.\\n177 European Commission (2022), ‘Proposal for a Directive on Corporate Sustainability Due Diligence’, \\n23\\xa0February 2022, https://ec.europa.eu/info/publications/proposal-directive-corporate-sustainable-due-\\ndiligence-and-annex_en. Several EU member states and other states have implemented similar obligations or \\nelements of mandatory human rights due diligence. For example, see Office of the UN High Commissioner for \\nHuman Rights (2020), UN Human Rights “Issues Paper” on legislative proposals for mandatory human rights due \\ndiligence by companies, June 2020, https://www.ohchr.org/sites/default/files/Documents/Issues/Business/\\nMandatoryHR_Due_Diligence_Issues_Paper.pdf, pp. 3–5.\\n178  Shift and Office of the UN High Commissioner for Human Rights (2021), Enforcement of Mandatory Due Diligence:'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 42, 'page_label': '41'}, page_content='MandatoryHR_Due_Diligence_Issues_Paper.pdf, pp. 3–5.\\n178  Shift and Office of the UN High Commissioner for Human Rights (2021), Enforcement of Mandatory Due Diligence: \\nKey Design Considerations for Administrative Supervision, Policy Paper, October 2021, https://shiftproject.org/wp-\\ncontent/uploads/2021/10/Enforcement-of-Mandatory-Due-Diligence_Shift_UN-Human-Rights_Policy-Paper-2.pdf.\\n179 Office of the UN High Commissioner for Human Rights (2022), The Practical Application of the Guiding Principles \\non Business and Human Rights to the Activities of the Technology Sector, April 2022, https://reliefweb.int/report/\\nworld/practical-application-guiding-principles-business-and-human-rights-activities-technology-companies-report-\\noffice-united-nations-high-commissioner-human-rights-ahrc5056-enarruzh, para. 20.\\n180 Netherlands Court of Audit (2022), ‘An Audit of 9 Algorithms Used by the Dutch Government’.\\nSupervision by regulatory and administrative'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 42, 'page_label': '41'}, page_content='180 Netherlands Court of Audit (2022), ‘An Audit of 9 Algorithms Used by the Dutch Government’.\\nSupervision by regulatory and administrative \\nauthorities is an important element of accountability \\nfor compliance with human rights responsibilities, \\nin\\xa0parallel with legal liability for harms.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 43, 'page_label': '42'}, page_content='AI governance and human rights\\nResetting the relationship\\n42 Chatham House\\n5.2.2 Corporate responsibility to respect human rights\\nThe UN’s Guiding Principles on Business and Human Rights are clear that ‘business \\nenterprises should respect human rights’. In other words, companies (particularly \\nlarge ones)181 should avoid infringing human rights and should address any adverse \\nhuman rights impacts resulting from their activities.182 Companies should have \\na\\xa0policy commitment to meet their human rights responsibilities, approved at senior \\nlevel, publicly available and embedded in the culture of the business.183 Companies \\nmust also have an ongoing due diligence process of human rights impact assessment, \\ntracked for responsiveness and reported externally, which allows them to identify, \\nmitigate and remedy human rights impacts.184 By deploying a responsible business \\nagenda, identifying and mitigating risks, companies can forestall problems and'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 43, 'page_label': '42'}, page_content='mitigate and remedy human rights impacts.184 By deploying a responsible business \\nagenda, identifying and mitigating risks, companies can forestall problems and \\nsave\\xa0themselves the time, money and acrimony of litigation.\\nDue diligence in the AI context is particularly challenging because of two \\ndistinguishing features. First, AI’s capacity for self-improvement may make it \\ndifficult to predict its consequences. Second, AI’s human rights impact will depend \\nnot only on the technology itself, but also on the context in which it is deployed. \\nIn\\xa0light of both these factors, due diligence on AI applications that may affect \\nhuman rights must be extensive and involve as wide a set of stakeholders as may \\nbe\\xa0affected by the AI. Further, given the risk of unanticipated consequences, \\nAI must be reviewed regularly once in operation. Hence, the former UN high \\ncommissioner on human rights called for comprehensive human rights due'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 43, 'page_label': '42'}, page_content='AI must be reviewed regularly once in operation. Hence, the former UN high \\ncommissioner on human rights called for comprehensive human rights due \\ndiligence to be conducted ‘when AI systems are acquired, developed, deployed \\nand\\xa0operated’,185 with that due diligence to continue ‘throughout the entire life cycle \\nof\\xa0an AI system’186 and to include consultations with stakeholders and involvement \\nof\\xa0experts.187 At present, many companies lack structures and processes to \\ndetect and act on human rights issues on an ongoing basis. The former UN high \\ncommissioner also called for the results of due diligence to be made public.188\\nSome companies’ AIAs are labelled as human rights assessment, like Verizon’s \\nongoing human rights due diligence.189 Other AI ethics assessments, such as that \\nadopted by the IEEE and the proposed AIA for the National Medical Imaging \\nPlatform, look similar to human rights due diligence, but are not labelled as such.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 43, 'page_label': '42'}, page_content='adopted by the IEEE and the proposed AIA for the National Medical Imaging \\nPlatform, look similar to human rights due diligence, but are not labelled as such. \\nGoogle reviews proposals for new AI deployment by reference to its AI Principles, \\na\\xa0process that can include consultation with human rights experts.190\\n181 The UN Guiding Principles on Business and Human Rights apply to all businesses, but the extent of business \\nresponsibilities increases with the organization’s size and the impact of its work: see UN Office of the High \\nCommissioner on Human Rights (2011), Guiding Principles on Business and Human Rights (2011), principle 14.\\n182 UN Office of the High Commissioner on Human Rights (2011), Guiding Principles on Business and Human \\nRights, principles 11, 13.\\n183 UN Office of the High Commissioner on Human Rights (2011), Guiding Principles on Business and Human \\nRights, principles 15 and 16.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 43, 'page_label': '42'}, page_content='Rights, principles 11, 13.\\n183 UN Office of the High Commissioner on Human Rights (2011), Guiding Principles on Business and Human \\nRights, principles 15 and 16.\\n184 UN Office of the High Commissioner on Human Rights (2011), Guiding Principles on Business and \\nHuman Rights, principles 15–21. See also Data & Society and European Center for Non-Profit Law (2021), \\nRecommendations for Assessing AI Impacts to Human Rights, Democracy and the Rule of Law, https://ecnl.org/sites/\\ndefault/files/2021-11/HUDERIA%20paper%20ECNL%20and%20DataSociety.pdf.\\n185 United Nations High Commissioner for Human Rights (2021), The Right to Privacy in the Digital Age, \\nA/HRC/48/31, https://documents-dds-ny.un.org/doc/UNDOC/GEN/G21/249/21/PDF/G2124921.pdf, para. 48.\\n186 Ibid., para. 49.\\n187  Ibid., para. 50.\\n188 Ibid., para. 50.\\n189  Verizon (2022), ‘Human Rights at Verizon’, https://www.verizon.com/about/investors/human-\\nrights-at-verizon.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 43, 'page_label': '42'}, page_content='186 Ibid., para. 49.\\n187  Ibid., para. 50.\\n188 Ibid., para. 50.\\n189  Verizon (2022), ‘Human Rights at Verizon’, https://www.verizon.com/about/investors/human-\\nrights-at-verizon.\\n190 Google AI (2022), ‘AI Principles reviews and operations’. https://ai.google/responsibilities/review-process.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 44, 'page_label': '43'}, page_content='AI governance and human rights\\nResetting the relationship\\n43 Chatham House\\nWhatever the labelling, certain features of human rights impact assessment are \\ncommonly omitted from corporate processes:\\n — Transparency. General statements of corporate intention and activity are easier \\nto find than public statements of human rights risks actually identified and \\nmitigated through due diligence processes.\\n — Scope. Some corporate processes only cover specific issues, such as bias and \\nprivacy, rather than the full range of human rights, or make only brief mention \\nof other rights.191\\n — Effect. It is often not clear what effect impact assessments have on the \\ncompany’s activities.192 Human rights due diligence requires that human rights \\nrisks be mitigated, whereas some business processes seem to entail balancing \\nrisks against perceived benefits.193\\n — Duration. Human rights due diligence includes a requirement for ongoing'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 44, 'page_label': '43'}, page_content='risks be mitigated, whereas some business processes seem to entail balancing \\nrisks against perceived benefits.193\\n — Duration. Human rights due diligence includes a requirement for ongoing \\nreview post-implementation, whereas many corporate reviews appear to focus \\nonly on product development. Ongoing review is particularly important in \\nlight of AI’s capacity for self-improvement over time. Otherwise, there is a risk \\nthat assessments give algorithmic processes a veneer of legitimacy rather than \\ngenuinely having an impact on activities.194 This risk is amplified when there \\nis\\xa0no transparency about the process, its results or impact.\\nIn addition to ensuring the adequacy of their impact assessment processes \\nfrom a human rights perspective, companies should foster a pro-human rights \\nculture throughout their organization. This means ensuring that AI teams are \\nrepresentative of society’s diversity and the diversity of intended consumers,'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 44, 'page_label': '43'}, page_content='culture throughout their organization. This means ensuring that AI teams are \\nrepresentative of society’s diversity and the diversity of intended consumers, \\nsuch that equality is ‘baked in’ to system design. It means engaging adequate \\ninternal and external expertise to conduct human rights due diligence and impact \\nassessments, including through involvement of stakeholders, and commitment \\nat board level to addressing human rights impacts identified. It also means public \\nreporting of any human rights risks and impacts identified and measures taken. \\nIt may mean providing training on human rights for all those working on AI\\xa0– \\nincluding technical experts, engineers and devisers of technical standards. It must \\ninclude ongoing monitoring of human rights impacts over time and preparedness \\nto address new concerns that may arise.\\n191 For example, Microsoft’s Responsible AI Impact Template (2022), https://blogs.microsoft.com/wp-content/'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 44, 'page_label': '43'}, page_content='to address new concerns that may arise.\\n191 For example, Microsoft’s Responsible AI Impact Template (2022), https://blogs.microsoft.com/wp-content/\\nuploads/prod/sites/5/2022/06/Microsoft-RAI-Impact-Assessment-Template.pdf.\\n192  Ada Lovelace Institute and DataKind UK (2020), Examining the Black Box, p. 18.\\n193 For example, Google states that it will not pursue technologies that cause or are likely to cause overall harm, \\nand ‘where there is a material risk of harm, we will proceed only where we believe that the benefits substantially \\noutweigh the risks, and will incorporate appropriate safety constraints.’ They also say that they will not proceed \\nwith ‘technologies whose purpose contravenes widely accepted principles of international law and human rights.’ \\nSee Google AI (2022), ‘Artificial Intelligence at Google: Our Principles’, https://ai.google/principles.\\n194 Ada Lovelace Institute and DataKind UK give the example of an impact evaluation of a predictive risk'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 44, 'page_label': '43'}, page_content='194 Ada Lovelace Institute and DataKind UK give the example of an impact evaluation of a predictive risk \\nmodelling tool for Allegheny County, PA’s children’s welfare office, with positive results that both conflicted with \\nother reviews of the tool and may have provided legitimacy for further use of AI in children’s social services. \\nAda\\xa0Lovelace Institute and DataKind UK (2020), Examining the Black Box, p. 19.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 45, 'page_label': '44'}, page_content='44 Chatham House\\n06 \\nRemedies in \\nAI\\xa0governance: \\nthe\\xa0contribution \\nof\\xa0human rights\\nBoth governments and companies should provide \\nsuitable\\xa0access to remedy for when AI goes wrong. \\nThis\\xa0entails effective reparation, accountability \\nand\\xa0measures\\xa0to\\xa0prevent recurrences.\\n6.1 Remedies: the landscape\\nLittle attention has been given to the development of a scheme of remedies for \\nwhen AI goes wrong. Responsibility needs to be clarified, and transparency \\nis\\xa0required to assess whether and how AI has gone wrong.\\nWhile AI governance principles commonly include a principle of accountability, \\nthis\\xa0often refers to impact assessments, audit or oversight, rather than a requirement \\nof remedy in the event of harms.195 Many sets of AI governance principles in fact \\nhave no provision for remedy. As the UN special rapporteur on contemporary \\nforms of racism has pointed out, ‘[e]thical commitments have little measurable'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 45, 'page_label': '44'}, page_content='have no provision for remedy. As the UN special rapporteur on contemporary \\nforms of racism has pointed out, ‘[e]thical commitments have little measurable \\n195 For example, UNESCO’s Recommendation on the Ethics of Artificial Intelligence (2021) discusses oversight, \\nimpact assessment, audit and due diligence mechanisms (paras 42 and 43) and suggests that states may wish \\nto\\xa0consider establishing an ethics commission or ethics observatory (para. 133).'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 46, 'page_label': '45'}, page_content='AI governance and human rights\\nResetting the relationship\\n45 Chatham House\\neffect on software development practices if they are not directly tied to structures \\nof\\xa0accountability in the workplace’.196\\nTo some extent, legal remedies for wrongs caused by the application of AI already \\nexist in tort law (negligence) and administrative law, particularly where those \\nwrongs are on the part of public authorities. However, the law and its processes \\nwill need to develop metrics for evaluating AI. For example, English administrative \\nlaw typically has regard to whether the decision-maker took the right factors into \\naccount when making their decision. But AI relies on statistical inferences rather \\nthan reasoning. Factors such as the opacity of AI systems and imbalance of \\ninformation and knowledge between companies and users, scalability of errors \\nand\\xa0rigidity of\\xa0decision-making may also pose challenges.197 As yet, there is no'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 46, 'page_label': '45'}, page_content='information and knowledge between companies and users, scalability of errors \\nand\\xa0rigidity of\\xa0decision-making may also pose challenges.197 As yet, there is no \\nclear\\xa0‘remedy pathway’ for those who suffer abuses of human rights as a result \\nof\\xa0the operation of AI.198\\nThose at greatest risk from harms caused by AI are likely to be the most \\nmarginalized and vulnerable groups in society, such as immigrants and those \\nin\\xa0the\\xa0criminal justice system. This makes it all the more important to ensure \\nthat\\xa0avenues for remedy are accessible to all, whatever their situation.\\nThere has already been some litigation challenging the application of AI by \\nreference to human rights law or its local equivalent. Notable cases include:\\n — In 2016, State of Wisconsin v Eric L Loomis, which challenged the use of \\nAI\\xa0COMPAS risk assessments when sentencing defendants in criminal cases. \\nThe COMPAS risk assessment was an assessment of recidivism risk, based'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 46, 'page_label': '45'}, page_content='AI\\xa0COMPAS risk assessments when sentencing defendants in criminal cases. \\nThe COMPAS risk assessment was an assessment of recidivism risk, based \\non\\xa0comparisons with other individuals with a similar history of offending. \\nThe\\xa0Supreme Court of Wisconsin held that a court’s consideration of a COMPAS \\nrisk assessment is consistent with the defendant’s right to due process, provided \\nthat the risk assessment is used in parallel with other factors and is not \\ndeterminative of the defendant’s sentence.199\\n — In May 2017, teachers in Houston successfully challenged the use of an \\nalgorithm known as EVAAS,200 developed by a private company to measure \\nteacher effectiveness.201 The aim of the algorithm was to enable the Houston \\n196 Report of the UN Special Rapporteur on contemporary forms of racism, racial discrimination, xenophobia and \\nrelated intolerance, Racial discrimination and emerging digital technologies: a human rights analysis, A/HRC/44/57 \\n18 June 2020, para. 62.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 46, 'page_label': '45'}, page_content='related intolerance, Racial discrimination and emerging digital technologies: a human rights analysis, A/HRC/44/57 \\n18 June 2020, para. 62.\\n197  See Williams, R. (2021), ‘Rethinking Administrative Law for Algorithmic Decision Making’, Oxford Journal \\nof\\xa0Legal Studies, 2(2), https://doi.org/10.1093/ojls/gqab032, pp. 468–94.\\n198 Office of the UN High Commissioner for Human Rights (2022), The Practical Application of the Guiding \\nPrinciples on Business and Human Rights to the Activities of the Technology Sector, para. 58.\\n199 State of Wisconsin v Eric L Loomis 2016 WI 68, 881 N.W.2d 749.\\n200 EVAAS stands for Educational Value-Added Assessment System.\\n201 Houston Federation of Teachers v Houston Independent School District 251 F.Supp.3d 1168 (SD Tex 2017).\\nThose at greatest risk from harms caused by \\nAI are likely to be the most marginalized and \\nvulnerable groups in society, such as immigrants \\nand those in the criminal justice system.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 47, 'page_label': '46'}, page_content='AI governance and human rights\\nResetting the relationship\\n46 Chatham House\\nIndependent School District (HISD) to terminate the employment of teachers \\nwhose performance was deemed ineffective. The US district court denied \\nHISD’s application for summary judgment against the teachers’ claim. The \\ncourt found that the teachers were ‘unfairly subject to mistaken deprivation \\nof constitutionally protected property interests in their jobs’, contrary to \\nthe\\xa0Due Process Clause of the Fourteenth Amendment of the US Constitution, \\nbecause they had no meaningful way to ensure correct calculation of their \\nscores, nor opportunity to independently verify or replicate those scores. \\nAfter the summary judgment, the case was settled and HISD abandoned \\nthe EVAAS system.202\\n — In March 2018, Finland’s National Non-Discrimination and Equality Tribunal \\ndecided that a credit institution’s decision not to grant credit to an individual was'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 47, 'page_label': '46'}, page_content='the EVAAS system.202\\n — In March 2018, Finland’s National Non-Discrimination and Equality Tribunal \\ndecided that a credit institution’s decision not to grant credit to an individual was \\ndiscriminatory. The tribunal ruled that the credit institution’s decision was made \\nnot on the basis of the individual’s own credit behaviour and creditworthiness, \\nbut by drawing assumptions from statistical data and information on payment \\ndefault relating to other people, by criteria such as gender, first language, age \\nand residential area. The tribunal prohibited the credit institution from using \\nthis decision-making method.203\\n — In February 2020, the Hague district court ordered the Dutch government to \\ncease its use of SyRI, an automated programme that reviewed the personal data \\nof social security claimants to predict how likely people were to commit benefit \\nor tax fraud. The Dutch government refused to reveal how SyRI used personal'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 47, 'page_label': '46'}, page_content='of social security claimants to predict how likely people were to commit benefit \\nor tax fraud. The Dutch government refused to reveal how SyRI used personal \\ndata, such that it was extremely difficult for individuals to challenge the \\ngovernment’s decisions to investigate them for fraud or the risk scores stored \\non file about them. The Court found that the legislation regulating SyRI did not \\ncomply with the right to respect for private life in Article 8 ECHR, as it failed \\nto balance adequately the benefits SyRI brought to society with the necessary \\nviolation of private life caused to those whose personal data it assessed. The \\nCourt also found that the system was discriminatory, as SyRI was only used \\nin so-called ‘problem neighbourhoods’, a proxy for discrimination on the basis \\nof\\xa0socio-economic background and immigration status.204\\n — In August 2020, R (Bridges) v Chief Constable of South Wales Police205 was'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 47, 'page_label': '46'}, page_content='of\\xa0socio-economic background and immigration status.204\\n — In August 2020, R (Bridges) v Chief Constable of South Wales Police205 was \\nthe\\xa0first\\xa0challenge to AI invoking UK human rights law. South Wales Police \\nwas trialling the use of live automated facial recognition technology (AFR) \\nto\\xa0compare CCTV images of people attending public events with images \\nof persons on a database. If there was no match, the CCTV images were \\nimmediately deleted from the AFR system. The complainant challenged AFR’s \\nmomentary capture of his image and comparison with its watch-list database, \\nby reference to Article 8 ECHR and the UK Data Protection Act. The Court \\n202  McCully, J. (2017), ‘Houston Federation of Teachers and Others v HISD’, Atlas Lab blog, \\nhttps://www.atlaslab.org/post/houston-federation-of-teachers-and-others-v-hisd-secret-algorithm-used-\\nto-fire-teachers.\\n203 National Non-Discrimination and Equality Tribunal of Finland (2018), Assessment of creditworthiness,'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 47, 'page_label': '46'}, page_content='to-fire-teachers.\\n203 National Non-Discrimination and Equality Tribunal of Finland (2018), Assessment of creditworthiness, \\nauthority, direct multiple discrimination, gender, language, age, place of residence, financial reasons, conditional \\nfine, Register No. 216/2017, 21 March 2018, https://www.yvtltk.fi/material/attachments/ytaltk/\\ntapausselosteet/45LI2c6dD/YVTltk-tapausseloste-_21.3.2018-luotto-moniperusteinen_syrjinta-S-en_2.pdf.\\n204 Toh, A. (2020), ‘Dutch Ruling a Victory for Rights of the Poor’, Human Rights Watch Dispatches, 6 February \\n2020, https://www.hrw.org/news/2020/02/06/dutch-ruling-victory-rights-poor.\\n205 [2020] EWCA Civ 1058.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 48, 'page_label': '47'}, page_content='AI governance and human rights\\nResetting the relationship\\n47  Chatham House\\nof Appeal found that there was not a proper basis in law for the use of AFR. \\nConsequently, its use breached the Data Protection Act. The court declined \\nto\\xa0find that the police’s use of AFR struck the wrong balance between the rights \\nof the individual and the interests of the community. But it did find that South \\nWales Police had failed to discharge the statutory Public Sector Equality Duty,206 \\nbecause in buying the AFR software from a private company and deploying \\nit, they had failed to take all reasonable steps to satisfy themselves that the \\nsoftware did not have a\\xa0racial or gender bias (notwithstanding that there was \\nno evidence to support the contention that the software was biased). The case \\ntherefore temporarily halted South Wales Police’s use of facial recognition \\ntechnology, but allowed the possibility of its reintroduction in future with'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 48, 'page_label': '47'}, page_content='therefore temporarily halted South Wales Police’s use of facial recognition \\ntechnology, but allowed the possibility of its reintroduction in future with \\nproper legal footing and due regard to the Public Sector Equality Duty. Indeed, \\nSouth Wales Police has since reintroduced facial recognition technology for \\nuse\\xa0in certain circumstances.207\\n — The Italian courts, having held in 2019 that administrative decisions based \\non\\xa0algorithms are illegitimate, reversed that view in 2021. The courts welcomed \\nthe speed and efficiency of algorithmic decision-making but clarified that it is \\nsubject to general principles of administrative review in Italian law, including \\ntransparency, effectiveness, proportionality, rationality and non-discrimination. \\nComplainants about public decision-making are entitled to call for disclosure of \\nalgorithms and related source code in order to challenge decisions effectively.208\\n — In July 2022, the UK NGO Big Brother Watch issued a legal complaint to the'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 48, 'page_label': '47'}, page_content='algorithms and related source code in order to challenge decisions effectively.208\\n — In July 2022, the UK NGO Big Brother Watch issued a legal complaint to the \\nBritish information commissioner in respect of alleged use of facial recognition \\ntechnology by Facewatch and the supermarket chain Southern Co-op to scan, \\nmaintain and assess profiles of all supermarket visitors in breach of data \\nprotection and privacy rights.209\\n6.2 Remedies: human rights law\\nHuman rights law requires both governments and companies to provide a suitable \\nright to remedy in the event of breach of their obligations and responsibilities.210 \\nRemedy comprises effective reparation, appropriate accountability for those \\nresponsible, as well as measures to prevent recurrences. The availability of \\nremedy\\xa0is crucial if human rights or ethical principles are to have real impact \\nin\\xa0the\\xa0face of countervailing commercial considerations.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 48, 'page_label': '47'}, page_content='remedy\\xa0is crucial if human rights or ethical principles are to have real impact \\nin\\xa0the\\xa0face of countervailing commercial considerations.\\n206 Section\\xa0149(1) Equality Act 2010: ‘A public authority must, in the exercise of its functions, have due regard \\nto\\xa0the need to- (a) eliminate discrimination, harassment, victimisation and any other conduct that is prohibited \\nby or under this Act; (b) advance equality of opportunity between persons who share a relevant protected \\ncharacteristic and persons who do not share it; (c) foster good relations between persons who share a relevant \\nprotected characteristic and persons who do not share it.’\\n207  South Wales Police (2022), ‘Facial Recognition Technology’, https://www.south-wales.police.uk/police-\\nforces/south-wales-police/areas/about-us/about-us/facial-recognition-technology.\\n208 Liguori, L. and Vittoria La Rosa, M. (2021), ‘Law and Policy of the Media in a Comparative Perspective’,'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 48, 'page_label': '47'}, page_content='forces/south-wales-police/areas/about-us/about-us/facial-recognition-technology.\\n208 Liguori, L. and Vittoria La Rosa, M. (2021), ‘Law and Policy of the Media in a Comparative Perspective’, \\nFilodiritto blog, 20 May 2021, https://www.filodiritto.com/law-and-policy-media-comparative-perspective.\\n209 Big Brother Watch (2022), Grounds of Complaint to the Information Commissioner under section\\xa0165 of the \\nData Protection Act 2018: Live Automated Facial Recognition by Facewatch Ltd and the Southern Cooperative Ltd, \\nhttps://docs.reclaimthenet.org/big-brother-watch-co-op-facewatch-legal-complaint.pdf.\\n210 International Covenant on Civil and Political Rights, Article 2(3); UN Office of the High Commissioner \\non\\xa0Human Rights (2011), Guiding Principles on Business and Human Rights, principles 25–31.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 49, 'page_label': '48'}, page_content='AI governance and human rights\\nResetting the relationship\\n48 Chatham House\\nThis means that, at all stages of design and deployment of AI, it must be clear \\nwho\\xa0bears responsibility for its operation. In particular, clarity is required on where \\nthe division of responsibilities lies between the developer of an AI system and \\nthe\\xa0purchaser and deployer of the system, including if the purchaser adapts the \\nAI\\xa0or uses it in a way for which it was not intended. Consequently, purchasers of AI \\nsystems will need adequate understanding or assurance as to how those systems \\nwork, as was demonstrated for the public sector in the Bridges case, discussed \\nabove. In that case, the court also held that commercial confidentiality around \\nany AI technology does not defeat or reduce the requirement for compliance \\nwith\\xa0the\\xa0Public Sector Equality Duty.211\\nComplainants need to know how to complain and to whom, and to be confident'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 49, 'page_label': '48'}, page_content='any AI technology does not defeat or reduce the requirement for compliance \\nwith\\xa0the\\xa0Public Sector Equality Duty.211\\nComplainants need to know how to complain and to whom, and to be confident \\nthat their complaint will be addressed in a timely manner. Remedy relies on \\ntransparency and explainability\\xa0– complainants should have enough information \\nto understand how a decision about them was made, and the role and operation \\nof AI in the decision-making process. They may need access to data on how the AI \\nwas designed and tested, how it was intended to operate and how it has operated \\nin the specific case, as well as information on the role of human decision-making \\nor\\xa0oversight in the process.\\nRemedy may be provided by the courts, by other governmental mechanisms \\nsuch as regulators, ombudspersons and complaints processes, as well as by \\nnon-governmental mechanisms such as corporate remediation processes. The'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 49, 'page_label': '48'}, page_content='such as regulators, ombudspersons and complaints processes, as well as by \\nnon-governmental mechanisms such as corporate remediation processes. The \\nUN Guiding Principles recommend that all businesses ‘establish or participate \\nin\\xa0effective operational-level grievance mechanisms’.212 Such mechanisms should \\nbe legitimate (i.e.\\xa0enabling trust); accessible; predictable; equitable; transparent; \\nrights-compatible; a source of continuous learning; and based on engagement \\nand\\xa0dialogue with stakeholders.213\\nThere are challenges in designing appropriate grievance mechanisms for \\naddressing harms caused by AI. Remedial systems that rely on individual \\ncomplaint tend to be better at addressing significant harms suffered by few than \\nharms suffered by many.214 But AI, with its capacity for operation at scale, risks \\ninfringing the rights of large numbers of people\\xa0– for example, by using personal \\ndata in violation of the right to privacy or engaging in widespread discriminatory'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 49, 'page_label': '48'}, page_content='infringing the rights of large numbers of people\\xa0– for example, by using personal \\ndata in violation of the right to privacy or engaging in widespread discriminatory \\ntreatment. Many of the people affected could be vulnerable or marginalized, \\nincluding asylum-seekers and those in the criminal justice system. Consequently, \\nthere needs to be provision both for individual complaints and for group or \\nrepresentative complaints against a whole system rather than a single decision. \\nOmbudsmen, national human rights institutions and civil society organizations \\nshould be adequately equipped to support victims’ complaints and to challenge \\nAI systems that are systematically causing harm. Remedies should consist both \\nof adequate remedy to victims and requirements to improve, or end the use of, \\nAI\\xa0systems to prevent recurrence of any harm identified.\\n211 R (Bridges) v Chief Constable of South Wales Police [2020] EWCA Civ 1058, para. 199.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 49, 'page_label': '48'}, page_content='AI\\xa0systems to prevent recurrence of any harm identified.\\n211 R (Bridges) v Chief Constable of South Wales Police [2020] EWCA Civ 1058, para. 199.\\n212 UN Office of the High Commissioner on Human Rights (2011), Guiding Principles on Business and Human \\nRights, principle 29.\\n213 UN Office of the High Commissioner on Human Rights (2011), Guiding Principles on Business and Human \\nRights, principle 31.\\n214 Raso, F. et al. (2018), Artificial Intelligence & Human Rights: Opportunities & Risks, Berkman Klein Center \\nResearch Publication No. 2018-6, 25 September 2018, http://dx.doi.org/10.2139/ssrn.3259344, p. 56.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 50, 'page_label': '49'}, page_content='AI governance and human rights\\nResetting the relationship\\n49 Chatham House\\nSimilarly, a business should be able to pursue accountability against other \\ncompanies that have harmed its operations as a result of AI. This may be because \\nthe business has purchased an AI system that has not functioned as intended, \\nor\\xa0because another company’s AI has in some way interfered with its operations.\\nMany challenges are expected in this field in the coming years. The guiding \\nprinciple should remain provision of an effective right to remedy, including \\nfor\\xa0breach of human rights responsibilities.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 51, 'page_label': '50'}, page_content='50 Chatham House\\n07 \\nConclusion and \\nrecommendations\\nTo place human rights at the heart of AI governance, \\ncompanies, governments, international organizations, civil \\nsociety and investors must take effective practical steps.\\nAs AI begins to reshape the human experience, human rights must be central to its \\ngovernance. There is nothing to fear, and much to gain, from taking human rights \\nas the baseline for AI governance.\\nFailure to take account of human rights means setting aside well-established, \\nwidely acknowledged parameters of liberty, fairness and equality, as well as \\nprocesses and accountability for their implementation. It involves creating \\nconfusing and inadequate alternatives to existing norms. It also duplicates much \\nof the work of developing those norms, the processes for their implementation \\nand\\xa0the remedies for their breach.\\nIf human rights are to be placed at the centre of AI governance, the following \\npractical actions are necessary.\\nFor companies:'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 51, 'page_label': '50'}, page_content='and\\xa0the remedies for their breach.\\nIf human rights are to be placed at the centre of AI governance, the following \\npractical actions are necessary.\\nFor companies:\\n — Continue to promote AI ethics and responsible business agendas, while \\nacknowledging the important complementary role of existing human \\nrights frameworks;\\n — Champion a holistic commitment to all human rights standards from the top of \\nthe organization. Enable a\\xa0change of corporate mindset, such that human rights \\nare seen as a useful tool in the box rather than as a\\xa0constraint on innovation;\\n — Recruit people with human rights expertise to join AI ethics teams to \\nencourage multi-disciplinary thinking and spread awareness of human rights \\norganization-wide. Use human rights as the common language and framework \\nfor multi-disciplinary teams addressing aspects of AI governance;'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 52, 'page_label': '51'}, page_content='AI governance and human rights\\nResetting the relationship\\n51  Chatham House\\n — Conduct human rights due diligence and adopt a human rights-based \\napproach\\xa0to AI ethics and impact assessment. Create decision-making structures \\nthat allow human rights risks to be monitored, flagged and acted upon on \\nan ongoing basis;\\n — Ensure uses of AI are explainable and transparent, so that people affected \\ncan\\xa0find out how an AI or AI-assisted decision was, or will be, made; and\\n — Establish a mechanism for individuals to seek remedy if they are dissatisfied \\nwith the outcome of a decision made or informed by AI.\\nFor governments:\\n — Ensure adequate understanding of human rights among government officials \\nand place human rights at the heart of AI regulation and policies, either via \\nthe\\xa0establishment of a dedicated office or other existing mechanisms;\\n — Equip teams involved in government procurement of systems and services \\nwith expertise in AI and human rights. Use contracting policy and'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 52, 'page_label': '51'}, page_content='— Equip teams involved in government procurement of systems and services \\nwith expertise in AI and human rights. Use contracting policy and \\nprocurement conditions to increase compliance with human rights standards \\namong businesses;\\n — Establish a discussion forum on AI governance that engages all stakeholders, \\nincluding human rights advocates, to foster better understanding and mutual \\nbenefit from others’ perspectives;\\n — Ensure that technical standards bodies, AI assurance mechanisms and devisers \\nof algorithmic impact assessment and audit processes give due regard to human \\nrights when developing and monitoring standards for AI governance;\\n — Consider cross-cutting regulation to ensure that AI deployed by both the public \\nand private sectors meets human rights standards;\\n — Put in place human rights-compatible standards and oversight for AIAs and \\naudits, as well as adequate provision of remedy for alleged breaches;'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 52, 'page_label': '51'}, page_content='and private sectors meets human rights standards;\\n — Put in place human rights-compatible standards and oversight for AIAs and \\naudits, as well as adequate provision of remedy for alleged breaches;\\n — Educate the public on the vital role of human rights in protecting individual \\nfreedoms as AI technology develops. Offer guidance to schools and teachers so \\nthat children have an understanding of human rights before they encounter AI;\\n — Ensure that all uses of AI are explainable and transparent, such that people \\naffected can find out how an AI or AI-informed decision was, or will be, made;\\n — Provide adequate resources for national human rights bodies and regulators, \\nsuch as the UK Equalities and Human Rights Commission, to champion the \\nrole of human rights in AI governance. Ensure these bodies are included \\nin\\xa0discussions on emerging tech issues;\\n — Incentivize AI development that benefits society as widely as possible and \\ncontributes to implementation of the UN’s SDGs; and'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 52, 'page_label': '51'}, page_content='in\\xa0discussions on emerging tech issues;\\n — Incentivize AI development that benefits society as widely as possible and \\ncontributes to implementation of the UN’s SDGs; and\\n — Liaise with other governments and international organizations with a view \\nto\\xa0harmonizing understanding of the impact of international human rights law \\non the development and implementation of AI (for example, through use of \\nsoft\\xa0law and guidance).'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 53, 'page_label': '52'}, page_content='AI governance and human rights\\nResetting the relationship\\n52 Chatham House\\nFor the UN and other international/regional organizations:\\n — Adopt consensus principles on AI and human rights that clarify the duties of \\nstates and responsibilities of companies in this field, as well as the requirements \\nfor remedy. Publish a sister document to the UN’s Guiding Principles on \\nBusiness and Human Rights to outline these principles, accessible to all \\nstakeholders including software developers and engineers;\\n — Establish a new multi-stakeholder forum that brings together the tech and \\nhuman rights communities, as well as technical standards bodies, to discuss \\nchallenges around the interaction of human rights and technology, including \\nAI.215 A regular, institutionalized dialogue would raise levels of understanding \\nand cooperation on all sides of the debate, and would help prevent business \\nexploitation of legal grey areas;216'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 53, 'page_label': '52'}, page_content='AI.215 A regular, institutionalized dialogue would raise levels of understanding \\nand cooperation on all sides of the debate, and would help prevent business \\nexploitation of legal grey areas;216\\n — Ensure, via the UN secretary-general’s envoy on technology, that all parts of the \\nUN (including technical standards bodies and procurement offices) align with \\nthe OHCHR in placing human rights at the centre of their work on technology;\\n — Continue to promote UNESCO’s Recommendation on the Ethics of Artificial \\nIntelligence, including the international human rights obligations and \\ncommitments to which it refers, facilitating knowledge-sharing and capacity-\\nbuilding to enable effective implementation in all states;\\n — Advance dialogue and coherent approaches to the implications of AI for human \\nrights, via treaties or soft law, and support national governments in their \\ngovernance of AI;\\n — Conduct human rights due diligence before deploying AI; and'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 53, 'page_label': '52'}, page_content='rights, via treaties or soft law, and support national governments in their \\ngovernance of AI;\\n — Conduct human rights due diligence before deploying AI; and\\n — Integrate AI into development and capacity-building activities to accelerate \\nimplementation of the SDGs.\\nFor civil society and academics:\\n — Push for inclusion in the AI governance conversation, including by fostering \\nconnections with the software development community and corporate \\npublic policy teams;\\n — Debunk human rights myths. Explain to a wide array of audiences (including \\nbusiness leaders, investors and governments) that human rights are reasonable \\nnot radical; that human rights do not stymie innovation but establish a level \\nplaying field in guarding against egregious development.\\n — Demonstrate the positive role of human rights as a regulatory system by \\nreference to existing processes of human rights due diligence and remedy;'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 53, 'page_label': '52'}, page_content='— Demonstrate the positive role of human rights as a regulatory system by \\nreference to existing processes of human rights due diligence and remedy;\\n215 As discussed at the Digital Democracy Dialogue 3D2, in Montreux, Switzerland, in November 2021. \\nSee\\xa0also\\xa0Universal Rights Group (2021), Placing Digital Technology at the Service of Democracy and Human Rights, \\nhttps://www.universal-rights.org/wp-content/uploads/2021/12/3D2_designed-report_V1.pdf.\\n216 Human Rights Council Advisory Committee (2021), Possible impacts, opportunities and challenges of new \\nand emerging digital technologies with regard to the promotion and protection of human rights, A/HRC/47/52, \\nhttps://documents-dds-ny.un.org/doc/UNDOC/GEN/G21/110/34/PDF/G2111034.pdf?OpenElement, para. 55.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 54, 'page_label': '53'}, page_content='AI governance and human rights\\nResetting the relationship\\n53 Chatham House\\n — Encourage inter-disciplinary engagement at universities and raise \\nawareness of human rights in technology-focused studies\\xa0– for example, \\nby\\xa0introducing human rights as an element of computer science degrees \\nand\\xa0coding ‘bootcamps’;\\n — Facilitate collaboration between civil society and the software development \\ncommunity on the development and use of AI to achieve of the SDGs; and\\n — Test the implications of human rights for AI through strategic litigation.\\nFor investors:\\n — Include assessment of the implications of AI for human rights in ESG \\nor\\xa0equivalent investment metrics.217\\n217 Minkkinen, M., Niukkanen, A., and Mäntymäki, M. (2022), ‘What about investors? ESG analyses as tools \\nfor\\xa0ethics-based AI auditing’, AI & Society, https://doi.org/10.1007/s00146-022-01415-0.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 55, 'page_label': '54'}, page_content='AI governance and human rights\\nResetting the relationship\\n54 Chatham House\\nAbout the author\\nKate Jones is an associate fellow with Chatham House’s International Law \\nProgramme. She is a consultant and researcher on human rights law, public \\ninternational law, governance and diplomacy, focusing on their intersection with \\ntechnology. With more than 20 years of legal experience, Kate has published and \\nspoken widely on aspects of tech governance, with specialisms in disinformation \\nand foreign interference, artificial intelligence and human rights. Her publications \\nat Chatham House include the 2019 research paper\\xa0Online Disinformation and \\nPolitical Discourse: Applying a Human Rights Framework.\\nAcknowledgments\\nThis research paper is published as part of the Human Rights Pathways initiative \\nof\\xa0Chatham House, funded by the Swiss Federal Department of Foreign Affairs.\\nThanks are due to all those whose ideas and comments have helped shape the'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 55, 'page_label': '54'}, page_content='of\\xa0Chatham House, funded by the Swiss Federal Department of Foreign Affairs.\\nThanks are due to all those whose ideas and comments have helped shape the \\npaper. This includes those who generously agreed to be interviewed; all the \\nparticipants in a Chatham House roundtable on Artificial Intelligence and Human \\nRights, convened with kind cooperation of the Geneva Human Rights Platform \\nat\\xa0the Villa Moynier, Geneva in May 2022; and all who attended a London meeting \\non AI and human rights kindly convened by the European Center for Not-for-Profit \\nLaw (ECNL) in June 2022.\\nThe author is grateful to all at Chatham House who have contributed to the \\ncontent, editing and publication of the paper, including Harriet Moynihan, \\nChanu Peiris, Rashmin Sagoo, Elizabeth Wilmshurst KC, Marjorie Buchser, David \\nGriffiths, Rowan Wilkinson, Rachel Mullally, Sophia Rose and Chris Matthews. \\nShe would also like to thank those outside Chatham House who reviewed drafts:'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 55, 'page_label': '54'}, page_content='Griffiths, Rowan Wilkinson, Rachel Mullally, Sophia Rose and Chris Matthews. \\nShe would also like to thank those outside Chatham House who reviewed drafts: \\nVanja Škorić of ECNL, Janis Wong of the Alan Turing Institute and the anonymous \\npeer reviewers.\\nFinally, thanks go to many others for interesting conversations and debates on this \\ntopic over the last year, including Lukas Madl and the advisory board of Innovethic, \\nChristian Hunt and his Human Risk Podcast, and the Digital Society Initiative \\nat Chatham House.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 56, 'page_label': '55'}, page_content='All rights reserved. No part of this publication may be reproduced or transmitted in any\\xa0form or by any means, \\nelectronic or mechanical including photocopying, recording or any information storage or retrieval system, \\nwithout the prior written permission of the\\xa0copyright holder. Please direct all enquiries\\xa0to the publishers.\\nChatham House does not express opinions of its own. The opinions expressed in this publication \\nare\\xa0the\\xa0responsibility of the author(s).\\nCopyright © The Royal Institute of International Affairs, 2023\\nCover image: An attendee tries a virtual reality experience during the Mobile World Congress trade show \\nin\\xa0Barcelona, Spain on 3 March 2022.\\nPhoto credit: Copyright © Joan Cros/NurPhoto/Getty Images\\nISBN 978 1 78413 549 2 \\nDOI 10.55317/9781784135492\\nCite this paper: Jones, K. (2023), AI governance and human rights: Resetting the relationship, Research Paper, \\nLondon: Royal Institute of International Affairs, https:/ /doi.org/10.55317/9781784135492.'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 56, 'page_label': '55'}, page_content='London: Royal Institute of International Affairs, https:/ /doi.org/10.55317/9781784135492.\\nThis publication is printed on FSC-certified paper.\\ndesignbysoapbox.com'),\n",
       " Document(metadata={'source': './data/AI_Governance.pdf', 'page': 57, 'page_label': '56'}, page_content='The Royal Institute of International Affairs \\nChatham House\\n10 St James’s Square, London SW1Y 4LE \\nT +44 (0)20 7957 5700 \\ncontact@chathamhouse.org  |  chathamhouse.org\\nCharity Registration Number: 208223\\nIndependent thinking since 1920')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splitted_doc = text_splitter.split_documents(documents=doc)\n",
    "splitted_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Storing embeddings by using Chroma DB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dp971\\AppData\\Local\\Temp\\ipykernel_28044\\1742640573.py:3: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  db = Chroma.from_documents(documents=doc[:20], embedding=OpenAIEmbeddings())\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import OpenAIEmbeddings, OllamaEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "db = Chroma.from_documents(documents=doc[:20], embedding=OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'page': 6, 'page_label': '5', 'source': './data/AI_Governance.pdf'}, page_content='5 Chatham House\\n02 \\nWhat is AI?\\nAI has capacity to transform human life\\xa0– \\nboth\\xa0for\\xa0better and for worse.\\nAI is increasingly present in our lives, and its impact will expand significantly \\nin the\\xa0coming years. From predictive text, to social media news feeds, to virtual \\nhomes and mobile phone voice assistants, AI is already a part of everyday life. \\nAI\\xa0offers automated translation, assists shoppers buying online and recommends \\nthe fastest route on the drive home. It is also a key component of much-debated, \\nrapidly developing technologies such as facial recognition and self-driving vehicles.\\nThere is no single agreed definition of AI: it is a general term referring to \\nmachines’\\xa0evolving capacity to take on tasks requiring some form of intelligence. \\nThe tasks that AI performs can include generating predictions, making decisions \\nand providing recommendations.1 This means that AI may make decisions itself, \\nor\\xa0provide information for use in human decision-making.\\nAI systems are algorithmic\\xa0– the algorithm being the computational process or \\nset of rules that the computer follows to calculate a result. To learn, AI generally \\nrelies on synthesising and making inferences from large quantities of data. \\nIt\\xa0is\\xa0the machine’s capacity to learn by itself how to do tasks better, rather than \\nsimply following instructions, that distinguishes AI from traditional computer \\nprogrammes. Contrary to popular myth, self-improvement does not prevent \\nAI\\xa0from being constrained by rules.\\nGovernments are among the largest adopters of AI, deploying it to assist in \\nmaking\\xa0decisions that can have major consequences for the lives of individual \\ncitizens. For example, governments are using AI to assist with decisions on \\nentitlement to immigration status, welfare benefits, school entry and priority \\n1 The European Commission’s High-Level Expert Group on Artificial Intelligence offers a fuller definition: \\n‘Artificial intelligence (AI) systems are software (and possibly also hardware) systems designed by humans \\nthat, given a complex goal, act in the physical or digital dimension by perceiving their environment through \\ndata acquisition, interpreting the collected structured or unstructured data, reasoning on the knowledge, or \\nprocessing the information, derived from this data and deciding the best action(s) to take to achieve the given \\ngoal. AI systems can either use symbolic rules or learn a numeric model, and they can also adapt their behaviour \\nby analysing how the environment is affected by their previous actions.’ Independent High-Level Expert \\nGroup on Artificial Intelligence (2019), Ethics Guidelines for Trustworthy AI, Brussels: European Commission, \\nhttps://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai.'),\n",
       " Document(metadata={'page': 7, 'page_label': '6', 'source': './data/AI_Governance.pdf'}, page_content='AI governance and human rights\\nResetting the relationship\\n6 Chatham House\\nvaccinations. They are adopting it to assist with provision of justice, in both civil \\nand criminal processes. And they may be using AI to assist in delivery of critical \\ninfrastructure and national security.\\nAI is likely to pervade almost every domain of human activity, and to become \\nincreasingly important as technology evolves towards greater interoperability, \\nincluding through the development of the metaverse.2 This paper discusses general \\nfeatures of AI, but by no means diminishes the need for parallel sector-specific \\ndiscussion. The use of AI in the healthcare system, in social media or in the criminal \\njustice process, for instance, each raise specific human rights issues that need \\nto\\xa0be\\xa0addressed in context, alongside the overarching issues discussed here.\\n2.1 What potential does AI hold for human \\nrights\\xa0and the common good?\\nDue to its speed and its power of self-learning, AI has the capacity to transform \\nour\\xa0societies. It can operate faster\\xa0– and potentially better\\xa0– than any human. \\nIt\\xa0can\\xa0achieve scientific breakthroughs, calculate fair distributions and outcomes, \\nand\\xa0make more accurate predictions.\\nAI holds enormous potential to enable human development and flourishing. \\nFor\\xa0example, AI is accelerating the battle against disease3 and mitigating the \\nimpact of disability;4 it is helping to tackle climate change5 and optimize efficiency \\nin agriculture;6 it can assist distribution of humanitarian aid;7 it has enormous \\npotential for improving access to, and quality of, education globally;8 and it can \\ntransform public and private transport.9 AI could help to ensure that policing is \\nfair and respectful of human dignity. It may make workplaces more productive, \\nreduce the load of manual labour and help developed countries to manage the \\nchallenges of an ageing population. To give a specific example of the benefits, \\nthe AI programme AlphaFold is predicting the structures of both human and \\nanimal\\xa0proteins with tremendous speed and remarkable accuracy, with potentially \\ntransformative effects on medical treatments, crop science and plastic reduction.10\\n2 Moynihan, H., Buchser, M. and Wallace, J. (2022), What is the metaverse?, Explainer, London: Royal Institute \\nof\\xa0International Affairs, https://www.chathamhouse.org/2022/04/what-metaverse.\\n3 For example, as regards COVID-19: Soomro, T. A. et al. (2021), ‘Artificial intelligence (AI) for medical imaging \\nto combat coronavirus disease (COVID-19): a detailed review with direction for future research’, Artificial \\nIntelligence Review, 55(2), pp. 1409–39, https://doi.org/10.1007%2Fs10462-021-09985-z.\\n4 For example: Microsoft (undated), ‘AI for Accessibility’, https://www.microsoft.com/en-us/ai/\\nai-for-accessibility.\\n5 Rolnick, D. et al. (2019), ‘Tackling Climate Change with Machine Learning’, arXiv, 1906.05433v2 [cs.CY], \\nhttps://arxiv.org/pdf/1906.05433.pdf.\\n6 Cline, T. (2019), ‘Digital agriculture: making the most of machine learning on farm’, Spore, https://spore.cta.\\nint/en/dossiers/article/digital-agriculture-making-the-most-of-machine-learning-on-farm-sid0dbfbb123-30b2-\\n48fd-830e-71312f66af04?msclkid=d9322204a57311ecb7a36f2895e35dd1.\\n7 For example: UN Global Pulse (2022), ‘Innovating Together for our Common Future’, www.unglobalpulse.org.\\n8 For example: UNESCO (2022), ‘Artificial Intelligence and the Futures of Learning’, https://en.unesco.org/\\nthemes/ict-education/ai-futures-learning.\\n9 For example, European Parliament Briefing (2019), ‘Artificial Intelligence in Transport: Current and \\nFuture Developments, Opportunities and Challenges’, https://www.europarl.europa.eu/RegData/etudes/\\nBRIE/2019/635609/EPRS_BRI(2019)635609_EN.pdf?msclkid=cd1a70d2aaa011ec9f9ff79af4f9d88d.\\n10 Tunyasuvunakool, K. et al. (2021), ‘Highly accurate protein structure prediction for the human proteome’, \\nNature, 596, 21 July 2021, pp. 590–96, https://doi.org/10.1038/s41586-021-03828-1.'),\n",
       " Document(metadata={'page': 9, 'page_label': '8', 'source': './data/AI_Governance.pdf'}, page_content='AI governance and human rights\\nResetting the relationship\\n8 Chatham House\\nMany AI tools abuse human rights as a collateral consequence of their operation. \\nAI risks embedding and exaggerating bias and discrimination, invading privacy, \\nreducing personal autonomy and making society more, rather than less, unequal. \\nFor example, AI sentencing tools may discriminate against minorities, potentially \\nturning back decades of progress towards equality. AI in healthcare may harm \\nhuman health if algorithms are incorrect or biased,15 while AI in welfare-provision \\nor migration may make unfair decisions on eligibility. AI tools may infer sensitive \\ninformation about individuals in violation of their privacy.\\nEven an AI tool designed with the intention of implementing scrupulous standards \\nof fairness will fail if it does not replicate the complex range of factors and subtle, \\ncontext-specific decision-making processes of humans. Unchecked, AI systems \\ntend to exacerbate structural imbalances of power and to disadvantage the most \\nmarginalized in society.16\\nFurther, some AI tools may have outputs detrimental to humanity through their \\npotential to shape human experience of the world. For example, AI algorithms \\nin social media may, by distorting the availability of information, manipulate \\naudience views in violation of the rights to freedom of thought and opinion,17 or \\nprioritize content that incites hatred and violence between social groups.18 AI used \\nto detect aptitudes or to select people for jobs, while intended to broaden human \\nhorizons and ambition, risks doing the opposite. Without safeguards, AI is likely \\nto entrench and exaggerate social divides and divisions, distort our impressions \\nof\\xa0the world and thus have negative consequences on aspects of human life. These \\nrisks are amplified by the difficulty of identifying when AI fails, for example when \\nit is malfunctioning, manipulative, acting illegally or making unfair decisions. \\nAt\\xa0present, companies rarely make public their identification of mistakes or errors \\nin their AI. Consumers cannot therefore see which standards have been met.\\nFinally, AI may entrench and even exacerbate social divides between rich \\nand poor, worsening the situation of the most vulnerable. As AI development \\nand implementation is largely driven by the commercial sector, it risks being \\nharnessed for the benefit of those who can pay rather than to resolve the world’s \\nmost significant challenges, and risks being deployed in ways that further \\ndispossess\\xa0vulnerable communities around the world.19\\n15 Park, Y. et al. (2020), ‘Evaluating artificial intelligence in medicine: phases of clinical research’, \\nJAMIA\\xa0Open,\\xa03(3), October 2020, pp. 326–31, https://doi.org/10.1093/jamiaopen/ooaa033.\\n16 European Digital Rights (EDRi) et al. (2021), Civil Society Statement on an EU Artificial Intelligence Act for \\nFundamental Rights, 30 November 2021, https://edri.org/wp-content/uploads/2021/12/Political-statement-on-\\nAI-Act.pdf; Kalluri, P. (2020), ‘Don’t ask if artificial intelligence is good or fair, ask how it shifts power’, Nature, \\n7\\xa0July 2020, https://www.nature.com/articles/d41586-020-02003-2.\\n17 Jones, K. (2019), Online Disinformation and Political Discourse: Applying a Human Rights Framework, \\nResearch\\xa0Paper, London: Royal Institute of International Affairs, https://www.chathamhouse.org/2019/11/\\nonline-disinformation-and-political-discourse-applying-human-rights-framework.\\n18 Kornbluh, K. (2022), ‘Disinformation, Radicalization, and Algorithmic Amplification: What Steps Can Congress \\nTake?’, Just Security blog, 7 February 2022, https://www.justsecurity.org/79995/disinformation-radicalization-\\nand-algorithmic-amplification-what-steps-can-congress-take.\\n19 Hao, K. (2022), ‘AI Colonialism’, MIT Technology Review, 19 April 2022, \\nhttps://www.technologyreview.com/2022/04/19/1049592/artificial-intelligence-colonialism.'),\n",
       " Document(metadata={'page': 4, 'page_label': '3', 'source': './data/AI_Governance.pdf'}, page_content='3 Chatham House\\n01 \\nIntroduction\\nAI is redefining what it means to be human. As existing \\ninternational norms designed to allow every human being a life \\nof liberty and dignity, human rights ought to be the foundation \\nfor AI governance.\\nHuman rights are central to what it means to be human. They were drafted \\nand\\xa0agreed internationally, with worldwide popular support, to define freedoms \\nand\\xa0entitlements that would allow every human being to live a life of liberty and \\ndignity. Those fundamental human rights have been interpreted and developed \\nover decades to delineate the parameters of fairness, equality and liberty for \\nevery individual.\\nNow, artificial intelligence (AI) is redefining what it means to be human. \\nIts systems\\xa0and processes have the potential to alter the human experience \\nfundamentally. AI will affect not only public policy areas such as road safety and \\nhealthcare, but also human autonomy, relationships and dignity. It will affect \\nlifestyles and professions, as well as the future course of human development \\nand the nature and scale of conflicts. It will change the relationships between \\ncommunities and those between the individual, the state and corporations. \\nAI offers tremendous benefits for all societies but also presents risks. These risks \\npotentially include further division between the privileged and the unprivileged; \\nthe erosion of individual freedoms through ubiquitous surveillance; and the \\nreplacement of independent thought and judgement with\\xa0automated control.\\nThis paper aims to explain why human rights ought to be the foundation for \\nAI governance, to explore the reasons why they are not\\xa0– except in the EU and \\nsome international organizations\\xa0– and to demonstrate how human rights can \\nbe\\xa0embedded from the beginning in future AI governance initiatives.\\nWhile AI is being implemented rapidly around the world, most governance \\ninitiatives to date have emerged from developed states. This paper therefore \\nfocuses on practice and process primarily in the EU, the UK and the US. However, \\nthe paper also acknowledges the significance of AI initiatives elsewhere in the \\nworld\\xa0– China in particular is a leading developer and exporter of AI technology.')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"what is AI?\"\n",
    "result = db.similarity_search(query=query)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5 Chatham House\\n02 \\nWhat is AI?\\nAI has capacity to transform human life\\xa0– \\nboth\\xa0for\\xa0better and for worse.\\nAI is increasingly present in our lives, and its impact will expand significantly \\nin the\\xa0coming years. From predictive text, to social media news feeds, to virtual \\nhomes and mobile phone voice assistants, AI is already a part of everyday life. \\nAI\\xa0offers automated translation, assists shoppers buying online and recommends \\nthe fastest route on the drive home. It is also a key component of much-debated, \\nrapidly developing technologies such as facial recognition and self-driving vehicles.\\nThere is no single agreed definition of AI: it is a general term referring to \\nmachines’\\xa0evolving capacity to take on tasks requiring some form of intelligence. \\nThe tasks that AI performs can include generating predictions, making decisions \\nand providing recommendations.1 This means that AI may make decisions itself, \\nor\\xa0provide information for use in human decision-making.\\nAI systems are algorithmic\\xa0– the algorithm being the computational process or \\nset of rules that the computer follows to calculate a result. To learn, AI generally \\nrelies on synthesising and making inferences from large quantities of data. \\nIt\\xa0is\\xa0the machine’s capacity to learn by itself how to do tasks better, rather than \\nsimply following instructions, that distinguishes AI from traditional computer \\nprogrammes. Contrary to popular myth, self-improvement does not prevent \\nAI\\xa0from being constrained by rules.\\nGovernments are among the largest adopters of AI, deploying it to assist in \\nmaking\\xa0decisions that can have major consequences for the lives of individual \\ncitizens. For example, governments are using AI to assist with decisions on \\nentitlement to immigration status, welfare benefits, school entry and priority \\n1 The European Commission’s High-Level Expert Group on Artificial Intelligence offers a fuller definition: \\n‘Artificial intelligence (AI) systems are software (and possibly also hardware) systems designed by humans \\nthat, given a complex goal, act in the physical or digital dimension by perceiving their environment through \\ndata acquisition, interpreting the collected structured or unstructured data, reasoning on the knowledge, or \\nprocessing the information, derived from this data and deciding the best action(s) to take to achieve the given \\ngoal. AI systems can either use symbolic rules or learn a numeric model, and they can also adapt their behaviour \\nby analysing how the environment is affected by their previous actions.’ Independent High-Level Expert \\nGroup on Artificial Intelligence (2019), Ethics Guidelines for Trustworthy AI, Brussels: European Commission, \\nhttps://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Storing embeddings by using faiss vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "faiss_db = FAISS.from_documents(documents=doc[:20], embedding=OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='11e627a8-1ea8-45f8-9bb6-3d94f1b80bc2', metadata={'source': './data/AI_Governance.pdf', 'page': 6, 'page_label': '5'}, page_content='5 Chatham House\\n02 \\nWhat is AI?\\nAI has capacity to transform human life\\xa0– \\nboth\\xa0for\\xa0better and for worse.\\nAI is increasingly present in our lives, and its impact will expand significantly \\nin the\\xa0coming years. From predictive text, to social media news feeds, to virtual \\nhomes and mobile phone voice assistants, AI is already a part of everyday life. \\nAI\\xa0offers automated translation, assists shoppers buying online and recommends \\nthe fastest route on the drive home. It is also a key component of much-debated, \\nrapidly developing technologies such as facial recognition and self-driving vehicles.\\nThere is no single agreed definition of AI: it is a general term referring to \\nmachines’\\xa0evolving capacity to take on tasks requiring some form of intelligence. \\nThe tasks that AI performs can include generating predictions, making decisions \\nand providing recommendations.1 This means that AI may make decisions itself, \\nor\\xa0provide information for use in human decision-making.\\nAI systems are algorithmic\\xa0– the algorithm being the computational process or \\nset of rules that the computer follows to calculate a result. To learn, AI generally \\nrelies on synthesising and making inferences from large quantities of data. \\nIt\\xa0is\\xa0the machine’s capacity to learn by itself how to do tasks better, rather than \\nsimply following instructions, that distinguishes AI from traditional computer \\nprogrammes. Contrary to popular myth, self-improvement does not prevent \\nAI\\xa0from being constrained by rules.\\nGovernments are among the largest adopters of AI, deploying it to assist in \\nmaking\\xa0decisions that can have major consequences for the lives of individual \\ncitizens. For example, governments are using AI to assist with decisions on \\nentitlement to immigration status, welfare benefits, school entry and priority \\n1 The European Commission’s High-Level Expert Group on Artificial Intelligence offers a fuller definition: \\n‘Artificial intelligence (AI) systems are software (and possibly also hardware) systems designed by humans \\nthat, given a complex goal, act in the physical or digital dimension by perceiving their environment through \\ndata acquisition, interpreting the collected structured or unstructured data, reasoning on the knowledge, or \\nprocessing the information, derived from this data and deciding the best action(s) to take to achieve the given \\ngoal. AI systems can either use symbolic rules or learn a numeric model, and they can also adapt their behaviour \\nby analysing how the environment is affected by their previous actions.’ Independent High-Level Expert \\nGroup on Artificial Intelligence (2019), Ethics Guidelines for Trustworthy AI, Brussels: European Commission, \\nhttps://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai.'),\n",
       " Document(id='ccd4c122-70b9-4d13-8892-a40ed4cb5733', metadata={'source': './data/AI_Governance.pdf', 'page': 7, 'page_label': '6'}, page_content='AI governance and human rights\\nResetting the relationship\\n6 Chatham House\\nvaccinations. They are adopting it to assist with provision of justice, in both civil \\nand criminal processes. And they may be using AI to assist in delivery of critical \\ninfrastructure and national security.\\nAI is likely to pervade almost every domain of human activity, and to become \\nincreasingly important as technology evolves towards greater interoperability, \\nincluding through the development of the metaverse.2 This paper discusses general \\nfeatures of AI, but by no means diminishes the need for parallel sector-specific \\ndiscussion. The use of AI in the healthcare system, in social media or in the criminal \\njustice process, for instance, each raise specific human rights issues that need \\nto\\xa0be\\xa0addressed in context, alongside the overarching issues discussed here.\\n2.1 What potential does AI hold for human \\nrights\\xa0and the common good?\\nDue to its speed and its power of self-learning, AI has the capacity to transform \\nour\\xa0societies. It can operate faster\\xa0– and potentially better\\xa0– than any human. \\nIt\\xa0can\\xa0achieve scientific breakthroughs, calculate fair distributions and outcomes, \\nand\\xa0make more accurate predictions.\\nAI holds enormous potential to enable human development and flourishing. \\nFor\\xa0example, AI is accelerating the battle against disease3 and mitigating the \\nimpact of disability;4 it is helping to tackle climate change5 and optimize efficiency \\nin agriculture;6 it can assist distribution of humanitarian aid;7 it has enormous \\npotential for improving access to, and quality of, education globally;8 and it can \\ntransform public and private transport.9 AI could help to ensure that policing is \\nfair and respectful of human dignity. It may make workplaces more productive, \\nreduce the load of manual labour and help developed countries to manage the \\nchallenges of an ageing population. To give a specific example of the benefits, \\nthe AI programme AlphaFold is predicting the structures of both human and \\nanimal\\xa0proteins with tremendous speed and remarkable accuracy, with potentially \\ntransformative effects on medical treatments, crop science and plastic reduction.10\\n2 Moynihan, H., Buchser, M. and Wallace, J. (2022), What is the metaverse?, Explainer, London: Royal Institute \\nof\\xa0International Affairs, https://www.chathamhouse.org/2022/04/what-metaverse.\\n3 For example, as regards COVID-19: Soomro, T. A. et al. (2021), ‘Artificial intelligence (AI) for medical imaging \\nto combat coronavirus disease (COVID-19): a detailed review with direction for future research’, Artificial \\nIntelligence Review, 55(2), pp. 1409–39, https://doi.org/10.1007%2Fs10462-021-09985-z.\\n4 For example: Microsoft (undated), ‘AI for Accessibility’, https://www.microsoft.com/en-us/ai/\\nai-for-accessibility.\\n5 Rolnick, D. et al. (2019), ‘Tackling Climate Change with Machine Learning’, arXiv, 1906.05433v2 [cs.CY], \\nhttps://arxiv.org/pdf/1906.05433.pdf.\\n6 Cline, T. (2019), ‘Digital agriculture: making the most of machine learning on farm’, Spore, https://spore.cta.\\nint/en/dossiers/article/digital-agriculture-making-the-most-of-machine-learning-on-farm-sid0dbfbb123-30b2-\\n48fd-830e-71312f66af04?msclkid=d9322204a57311ecb7a36f2895e35dd1.\\n7 For example: UN Global Pulse (2022), ‘Innovating Together for our Common Future’, www.unglobalpulse.org.\\n8 For example: UNESCO (2022), ‘Artificial Intelligence and the Futures of Learning’, https://en.unesco.org/\\nthemes/ict-education/ai-futures-learning.\\n9 For example, European Parliament Briefing (2019), ‘Artificial Intelligence in Transport: Current and \\nFuture Developments, Opportunities and Challenges’, https://www.europarl.europa.eu/RegData/etudes/\\nBRIE/2019/635609/EPRS_BRI(2019)635609_EN.pdf?msclkid=cd1a70d2aaa011ec9f9ff79af4f9d88d.\\n10 Tunyasuvunakool, K. et al. (2021), ‘Highly accurate protein structure prediction for the human proteome’, \\nNature, 596, 21 July 2021, pp. 590–96, https://doi.org/10.1038/s41586-021-03828-1.'),\n",
       " Document(id='e6887283-449c-481a-9dd4-d1b2be1379d9', metadata={'source': './data/AI_Governance.pdf', 'page': 9, 'page_label': '8'}, page_content='AI governance and human rights\\nResetting the relationship\\n8 Chatham House\\nMany AI tools abuse human rights as a collateral consequence of their operation. \\nAI risks embedding and exaggerating bias and discrimination, invading privacy, \\nreducing personal autonomy and making society more, rather than less, unequal. \\nFor example, AI sentencing tools may discriminate against minorities, potentially \\nturning back decades of progress towards equality. AI in healthcare may harm \\nhuman health if algorithms are incorrect or biased,15 while AI in welfare-provision \\nor migration may make unfair decisions on eligibility. AI tools may infer sensitive \\ninformation about individuals in violation of their privacy.\\nEven an AI tool designed with the intention of implementing scrupulous standards \\nof fairness will fail if it does not replicate the complex range of factors and subtle, \\ncontext-specific decision-making processes of humans. Unchecked, AI systems \\ntend to exacerbate structural imbalances of power and to disadvantage the most \\nmarginalized in society.16\\nFurther, some AI tools may have outputs detrimental to humanity through their \\npotential to shape human experience of the world. For example, AI algorithms \\nin social media may, by distorting the availability of information, manipulate \\naudience views in violation of the rights to freedom of thought and opinion,17 or \\nprioritize content that incites hatred and violence between social groups.18 AI used \\nto detect aptitudes or to select people for jobs, while intended to broaden human \\nhorizons and ambition, risks doing the opposite. Without safeguards, AI is likely \\nto entrench and exaggerate social divides and divisions, distort our impressions \\nof\\xa0the world and thus have negative consequences on aspects of human life. These \\nrisks are amplified by the difficulty of identifying when AI fails, for example when \\nit is malfunctioning, manipulative, acting illegally or making unfair decisions. \\nAt\\xa0present, companies rarely make public their identification of mistakes or errors \\nin their AI. Consumers cannot therefore see which standards have been met.\\nFinally, AI may entrench and even exacerbate social divides between rich \\nand poor, worsening the situation of the most vulnerable. As AI development \\nand implementation is largely driven by the commercial sector, it risks being \\nharnessed for the benefit of those who can pay rather than to resolve the world’s \\nmost significant challenges, and risks being deployed in ways that further \\ndispossess\\xa0vulnerable communities around the world.19\\n15 Park, Y. et al. (2020), ‘Evaluating artificial intelligence in medicine: phases of clinical research’, \\nJAMIA\\xa0Open,\\xa03(3), October 2020, pp. 326–31, https://doi.org/10.1093/jamiaopen/ooaa033.\\n16 European Digital Rights (EDRi) et al. (2021), Civil Society Statement on an EU Artificial Intelligence Act for \\nFundamental Rights, 30 November 2021, https://edri.org/wp-content/uploads/2021/12/Political-statement-on-\\nAI-Act.pdf; Kalluri, P. (2020), ‘Don’t ask if artificial intelligence is good or fair, ask how it shifts power’, Nature, \\n7\\xa0July 2020, https://www.nature.com/articles/d41586-020-02003-2.\\n17 Jones, K. (2019), Online Disinformation and Political Discourse: Applying a Human Rights Framework, \\nResearch\\xa0Paper, London: Royal Institute of International Affairs, https://www.chathamhouse.org/2019/11/\\nonline-disinformation-and-political-discourse-applying-human-rights-framework.\\n18 Kornbluh, K. (2022), ‘Disinformation, Radicalization, and Algorithmic Amplification: What Steps Can Congress \\nTake?’, Just Security blog, 7 February 2022, https://www.justsecurity.org/79995/disinformation-radicalization-\\nand-algorithmic-amplification-what-steps-can-congress-take.\\n19 Hao, K. (2022), ‘AI Colonialism’, MIT Technology Review, 19 April 2022, \\nhttps://www.technologyreview.com/2022/04/19/1049592/artificial-intelligence-colonialism.'),\n",
       " Document(id='1ea1d9c1-3b99-408b-99ef-403bf1784cb3', metadata={'source': './data/AI_Governance.pdf', 'page': 4, 'page_label': '3'}, page_content='3 Chatham House\\n01 \\nIntroduction\\nAI is redefining what it means to be human. As existing \\ninternational norms designed to allow every human being a life \\nof liberty and dignity, human rights ought to be the foundation \\nfor AI governance.\\nHuman rights are central to what it means to be human. They were drafted \\nand\\xa0agreed internationally, with worldwide popular support, to define freedoms \\nand\\xa0entitlements that would allow every human being to live a life of liberty and \\ndignity. Those fundamental human rights have been interpreted and developed \\nover decades to delineate the parameters of fairness, equality and liberty for \\nevery individual.\\nNow, artificial intelligence (AI) is redefining what it means to be human. \\nIts systems\\xa0and processes have the potential to alter the human experience \\nfundamentally. AI will affect not only public policy areas such as road safety and \\nhealthcare, but also human autonomy, relationships and dignity. It will affect \\nlifestyles and professions, as well as the future course of human development \\nand the nature and scale of conflicts. It will change the relationships between \\ncommunities and those between the individual, the state and corporations. \\nAI offers tremendous benefits for all societies but also presents risks. These risks \\npotentially include further division between the privileged and the unprivileged; \\nthe erosion of individual freedoms through ubiquitous surveillance; and the \\nreplacement of independent thought and judgement with\\xa0automated control.\\nThis paper aims to explain why human rights ought to be the foundation for \\nAI governance, to explore the reasons why they are not\\xa0– except in the EU and \\nsome international organizations\\xa0– and to demonstrate how human rights can \\nbe\\xa0embedded from the beginning in future AI governance initiatives.\\nWhile AI is being implemented rapidly around the world, most governance \\ninitiatives to date have emerged from developed states. This paper therefore \\nfocuses on practice and process primarily in the EU, the UK and the US. However, \\nthe paper also acknowledges the significance of AI initiatives elsewhere in the \\nworld\\xa0– China in particular is a leading developer and exporter of AI technology.')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"what is AI?\"\n",
    "result = faiss_db.similarity_search(query=query)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5 Chatham House\\n02 \\nWhat is AI?\\nAI has capacity to transform human life\\xa0– \\nboth\\xa0for\\xa0better and for worse.\\nAI is increasingly present in our lives, and its impact will expand significantly \\nin the\\xa0coming years. From predictive text, to social media news feeds, to virtual \\nhomes and mobile phone voice assistants, AI is already a part of everyday life. \\nAI\\xa0offers automated translation, assists shoppers buying online and recommends \\nthe fastest route on the drive home. It is also a key component of much-debated, \\nrapidly developing technologies such as facial recognition and self-driving vehicles.\\nThere is no single agreed definition of AI: it is a general term referring to \\nmachines’\\xa0evolving capacity to take on tasks requiring some form of intelligence. \\nThe tasks that AI performs can include generating predictions, making decisions \\nand providing recommendations.1 This means that AI may make decisions itself, \\nor\\xa0provide information for use in human decision-making.\\nAI systems are algorithmic\\xa0– the algorithm being the computational process or \\nset of rules that the computer follows to calculate a result. To learn, AI generally \\nrelies on synthesising and making inferences from large quantities of data. \\nIt\\xa0is\\xa0the machine’s capacity to learn by itself how to do tasks better, rather than \\nsimply following instructions, that distinguishes AI from traditional computer \\nprogrammes. Contrary to popular myth, self-improvement does not prevent \\nAI\\xa0from being constrained by rules.\\nGovernments are among the largest adopters of AI, deploying it to assist in \\nmaking\\xa0decisions that can have major consequences for the lives of individual \\ncitizens. For example, governments are using AI to assist with decisions on \\nentitlement to immigration status, welfare benefits, school entry and priority \\n1 The European Commission’s High-Level Expert Group on Artificial Intelligence offers a fuller definition: \\n‘Artificial intelligence (AI) systems are software (and possibly also hardware) systems designed by humans \\nthat, given a complex goal, act in the physical or digital dimension by perceiving their environment through \\ndata acquisition, interpreting the collected structured or unstructured data, reasoning on the knowledge, or \\nprocessing the information, derived from this data and deciding the best action(s) to take to achieve the given \\ngoal. AI systems can either use symbolic rules or learn a numeric model, and they can also adapt their behaviour \\nby analysing how the environment is affected by their previous actions.’ Independent High-Level Expert \\nGroup on Artificial Intelligence (2019), Ethics Guidelines for Trustworthy AI, Brussels: European Commission, \\nhttps://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Storing embeddings by using Lance vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dp971\\Coading\\GEN_AI\\Prerequisites\\Projects\\RAG\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import LanceDB\n",
    "lance_db = LanceDB.from_documents(documents=doc[:20], embedding=OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='11e627a8-1ea8-45f8-9bb6-3d94f1b80bc2', metadata={'source': './data/AI_Governance.pdf', 'page': 6, 'page_label': '5'}, page_content='5 Chatham House\\n02 \\nWhat is AI?\\nAI has capacity to transform human life\\xa0– \\nboth\\xa0for\\xa0better and for worse.\\nAI is increasingly present in our lives, and its impact will expand significantly \\nin the\\xa0coming years. From predictive text, to social media news feeds, to virtual \\nhomes and mobile phone voice assistants, AI is already a part of everyday life. \\nAI\\xa0offers automated translation, assists shoppers buying online and recommends \\nthe fastest route on the drive home. It is also a key component of much-debated, \\nrapidly developing technologies such as facial recognition and self-driving vehicles.\\nThere is no single agreed definition of AI: it is a general term referring to \\nmachines’\\xa0evolving capacity to take on tasks requiring some form of intelligence. \\nThe tasks that AI performs can include generating predictions, making decisions \\nand providing recommendations.1 This means that AI may make decisions itself, \\nor\\xa0provide information for use in human decision-making.\\nAI systems are algorithmic\\xa0– the algorithm being the computational process or \\nset of rules that the computer follows to calculate a result. To learn, AI generally \\nrelies on synthesising and making inferences from large quantities of data. \\nIt\\xa0is\\xa0the machine’s capacity to learn by itself how to do tasks better, rather than \\nsimply following instructions, that distinguishes AI from traditional computer \\nprogrammes. Contrary to popular myth, self-improvement does not prevent \\nAI\\xa0from being constrained by rules.\\nGovernments are among the largest adopters of AI, deploying it to assist in \\nmaking\\xa0decisions that can have major consequences for the lives of individual \\ncitizens. For example, governments are using AI to assist with decisions on \\nentitlement to immigration status, welfare benefits, school entry and priority \\n1 The European Commission’s High-Level Expert Group on Artificial Intelligence offers a fuller definition: \\n‘Artificial intelligence (AI) systems are software (and possibly also hardware) systems designed by humans \\nthat, given a complex goal, act in the physical or digital dimension by perceiving their environment through \\ndata acquisition, interpreting the collected structured or unstructured data, reasoning on the knowledge, or \\nprocessing the information, derived from this data and deciding the best action(s) to take to achieve the given \\ngoal. AI systems can either use symbolic rules or learn a numeric model, and they can also adapt their behaviour \\nby analysing how the environment is affected by their previous actions.’ Independent High-Level Expert \\nGroup on Artificial Intelligence (2019), Ethics Guidelines for Trustworthy AI, Brussels: European Commission, \\nhttps://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai.'),\n",
       " Document(id='ccd4c122-70b9-4d13-8892-a40ed4cb5733', metadata={'source': './data/AI_Governance.pdf', 'page': 7, 'page_label': '6'}, page_content='AI governance and human rights\\nResetting the relationship\\n6 Chatham House\\nvaccinations. They are adopting it to assist with provision of justice, in both civil \\nand criminal processes. And they may be using AI to assist in delivery of critical \\ninfrastructure and national security.\\nAI is likely to pervade almost every domain of human activity, and to become \\nincreasingly important as technology evolves towards greater interoperability, \\nincluding through the development of the metaverse.2 This paper discusses general \\nfeatures of AI, but by no means diminishes the need for parallel sector-specific \\ndiscussion. The use of AI in the healthcare system, in social media or in the criminal \\njustice process, for instance, each raise specific human rights issues that need \\nto\\xa0be\\xa0addressed in context, alongside the overarching issues discussed here.\\n2.1 What potential does AI hold for human \\nrights\\xa0and the common good?\\nDue to its speed and its power of self-learning, AI has the capacity to transform \\nour\\xa0societies. It can operate faster\\xa0– and potentially better\\xa0– than any human. \\nIt\\xa0can\\xa0achieve scientific breakthroughs, calculate fair distributions and outcomes, \\nand\\xa0make more accurate predictions.\\nAI holds enormous potential to enable human development and flourishing. \\nFor\\xa0example, AI is accelerating the battle against disease3 and mitigating the \\nimpact of disability;4 it is helping to tackle climate change5 and optimize efficiency \\nin agriculture;6 it can assist distribution of humanitarian aid;7 it has enormous \\npotential for improving access to, and quality of, education globally;8 and it can \\ntransform public and private transport.9 AI could help to ensure that policing is \\nfair and respectful of human dignity. It may make workplaces more productive, \\nreduce the load of manual labour and help developed countries to manage the \\nchallenges of an ageing population. To give a specific example of the benefits, \\nthe AI programme AlphaFold is predicting the structures of both human and \\nanimal\\xa0proteins with tremendous speed and remarkable accuracy, with potentially \\ntransformative effects on medical treatments, crop science and plastic reduction.10\\n2 Moynihan, H., Buchser, M. and Wallace, J. (2022), What is the metaverse?, Explainer, London: Royal Institute \\nof\\xa0International Affairs, https://www.chathamhouse.org/2022/04/what-metaverse.\\n3 For example, as regards COVID-19: Soomro, T. A. et al. (2021), ‘Artificial intelligence (AI) for medical imaging \\nto combat coronavirus disease (COVID-19): a detailed review with direction for future research’, Artificial \\nIntelligence Review, 55(2), pp. 1409–39, https://doi.org/10.1007%2Fs10462-021-09985-z.\\n4 For example: Microsoft (undated), ‘AI for Accessibility’, https://www.microsoft.com/en-us/ai/\\nai-for-accessibility.\\n5 Rolnick, D. et al. (2019), ‘Tackling Climate Change with Machine Learning’, arXiv, 1906.05433v2 [cs.CY], \\nhttps://arxiv.org/pdf/1906.05433.pdf.\\n6 Cline, T. (2019), ‘Digital agriculture: making the most of machine learning on farm’, Spore, https://spore.cta.\\nint/en/dossiers/article/digital-agriculture-making-the-most-of-machine-learning-on-farm-sid0dbfbb123-30b2-\\n48fd-830e-71312f66af04?msclkid=d9322204a57311ecb7a36f2895e35dd1.\\n7 For example: UN Global Pulse (2022), ‘Innovating Together for our Common Future’, www.unglobalpulse.org.\\n8 For example: UNESCO (2022), ‘Artificial Intelligence and the Futures of Learning’, https://en.unesco.org/\\nthemes/ict-education/ai-futures-learning.\\n9 For example, European Parliament Briefing (2019), ‘Artificial Intelligence in Transport: Current and \\nFuture Developments, Opportunities and Challenges’, https://www.europarl.europa.eu/RegData/etudes/\\nBRIE/2019/635609/EPRS_BRI(2019)635609_EN.pdf?msclkid=cd1a70d2aaa011ec9f9ff79af4f9d88d.\\n10 Tunyasuvunakool, K. et al. (2021), ‘Highly accurate protein structure prediction for the human proteome’, \\nNature, 596, 21 July 2021, pp. 590–96, https://doi.org/10.1038/s41586-021-03828-1.'),\n",
       " Document(id='e6887283-449c-481a-9dd4-d1b2be1379d9', metadata={'source': './data/AI_Governance.pdf', 'page': 9, 'page_label': '8'}, page_content='AI governance and human rights\\nResetting the relationship\\n8 Chatham House\\nMany AI tools abuse human rights as a collateral consequence of their operation. \\nAI risks embedding and exaggerating bias and discrimination, invading privacy, \\nreducing personal autonomy and making society more, rather than less, unequal. \\nFor example, AI sentencing tools may discriminate against minorities, potentially \\nturning back decades of progress towards equality. AI in healthcare may harm \\nhuman health if algorithms are incorrect or biased,15 while AI in welfare-provision \\nor migration may make unfair decisions on eligibility. AI tools may infer sensitive \\ninformation about individuals in violation of their privacy.\\nEven an AI tool designed with the intention of implementing scrupulous standards \\nof fairness will fail if it does not replicate the complex range of factors and subtle, \\ncontext-specific decision-making processes of humans. Unchecked, AI systems \\ntend to exacerbate structural imbalances of power and to disadvantage the most \\nmarginalized in society.16\\nFurther, some AI tools may have outputs detrimental to humanity through their \\npotential to shape human experience of the world. For example, AI algorithms \\nin social media may, by distorting the availability of information, manipulate \\naudience views in violation of the rights to freedom of thought and opinion,17 or \\nprioritize content that incites hatred and violence between social groups.18 AI used \\nto detect aptitudes or to select people for jobs, while intended to broaden human \\nhorizons and ambition, risks doing the opposite. Without safeguards, AI is likely \\nto entrench and exaggerate social divides and divisions, distort our impressions \\nof\\xa0the world and thus have negative consequences on aspects of human life. These \\nrisks are amplified by the difficulty of identifying when AI fails, for example when \\nit is malfunctioning, manipulative, acting illegally or making unfair decisions. \\nAt\\xa0present, companies rarely make public their identification of mistakes or errors \\nin their AI. Consumers cannot therefore see which standards have been met.\\nFinally, AI may entrench and even exacerbate social divides between rich \\nand poor, worsening the situation of the most vulnerable. As AI development \\nand implementation is largely driven by the commercial sector, it risks being \\nharnessed for the benefit of those who can pay rather than to resolve the world’s \\nmost significant challenges, and risks being deployed in ways that further \\ndispossess\\xa0vulnerable communities around the world.19\\n15 Park, Y. et al. (2020), ‘Evaluating artificial intelligence in medicine: phases of clinical research’, \\nJAMIA\\xa0Open,\\xa03(3), October 2020, pp. 326–31, https://doi.org/10.1093/jamiaopen/ooaa033.\\n16 European Digital Rights (EDRi) et al. (2021), Civil Society Statement on an EU Artificial Intelligence Act for \\nFundamental Rights, 30 November 2021, https://edri.org/wp-content/uploads/2021/12/Political-statement-on-\\nAI-Act.pdf; Kalluri, P. (2020), ‘Don’t ask if artificial intelligence is good or fair, ask how it shifts power’, Nature, \\n7\\xa0July 2020, https://www.nature.com/articles/d41586-020-02003-2.\\n17 Jones, K. (2019), Online Disinformation and Political Discourse: Applying a Human Rights Framework, \\nResearch\\xa0Paper, London: Royal Institute of International Affairs, https://www.chathamhouse.org/2019/11/\\nonline-disinformation-and-political-discourse-applying-human-rights-framework.\\n18 Kornbluh, K. (2022), ‘Disinformation, Radicalization, and Algorithmic Amplification: What Steps Can Congress \\nTake?’, Just Security blog, 7 February 2022, https://www.justsecurity.org/79995/disinformation-radicalization-\\nand-algorithmic-amplification-what-steps-can-congress-take.\\n19 Hao, K. (2022), ‘AI Colonialism’, MIT Technology Review, 19 April 2022, \\nhttps://www.technologyreview.com/2022/04/19/1049592/artificial-intelligence-colonialism.'),\n",
       " Document(id='1ea1d9c1-3b99-408b-99ef-403bf1784cb3', metadata={'source': './data/AI_Governance.pdf', 'page': 4, 'page_label': '3'}, page_content='3 Chatham House\\n01 \\nIntroduction\\nAI is redefining what it means to be human. As existing \\ninternational norms designed to allow every human being a life \\nof liberty and dignity, human rights ought to be the foundation \\nfor AI governance.\\nHuman rights are central to what it means to be human. They were drafted \\nand\\xa0agreed internationally, with worldwide popular support, to define freedoms \\nand\\xa0entitlements that would allow every human being to live a life of liberty and \\ndignity. Those fundamental human rights have been interpreted and developed \\nover decades to delineate the parameters of fairness, equality and liberty for \\nevery individual.\\nNow, artificial intelligence (AI) is redefining what it means to be human. \\nIts systems\\xa0and processes have the potential to alter the human experience \\nfundamentally. AI will affect not only public policy areas such as road safety and \\nhealthcare, but also human autonomy, relationships and dignity. It will affect \\nlifestyles and professions, as well as the future course of human development \\nand the nature and scale of conflicts. It will change the relationships between \\ncommunities and those between the individual, the state and corporations. \\nAI offers tremendous benefits for all societies but also presents risks. These risks \\npotentially include further division between the privileged and the unprivileged; \\nthe erosion of individual freedoms through ubiquitous surveillance; and the \\nreplacement of independent thought and judgement with\\xa0automated control.\\nThis paper aims to explain why human rights ought to be the foundation for \\nAI governance, to explore the reasons why they are not\\xa0– except in the EU and \\nsome international organizations\\xa0– and to demonstrate how human rights can \\nbe\\xa0embedded from the beginning in future AI governance initiatives.\\nWhile AI is being implemented rapidly around the world, most governance \\ninitiatives to date have emerged from developed states. This paper therefore \\nfocuses on practice and process primarily in the EU, the UK and the US. However, \\nthe paper also acknowledges the significance of AI initiatives elsewhere in the \\nworld\\xa0– China in particular is a leading developer and exporter of AI technology.')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"what is AI?\"\n",
    "result = faiss_db.similarity_search(query=query)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5 Chatham House\\n02 \\nWhat is AI?\\nAI has capacity to transform human life\\xa0– \\nboth\\xa0for\\xa0better and for worse.\\nAI is increasingly present in our lives, and its impact will expand significantly \\nin the\\xa0coming years. From predictive text, to social media news feeds, to virtual \\nhomes and mobile phone voice assistants, AI is already a part of everyday life. \\nAI\\xa0offers automated translation, assists shoppers buying online and recommends \\nthe fastest route on the drive home. It is also a key component of much-debated, \\nrapidly developing technologies such as facial recognition and self-driving vehicles.\\nThere is no single agreed definition of AI: it is a general term referring to \\nmachines’\\xa0evolving capacity to take on tasks requiring some form of intelligence. \\nThe tasks that AI performs can include generating predictions, making decisions \\nand providing recommendations.1 This means that AI may make decisions itself, \\nor\\xa0provide information for use in human decision-making.\\nAI systems are algorithmic\\xa0– the algorithm being the computational process or \\nset of rules that the computer follows to calculate a result. To learn, AI generally \\nrelies on synthesising and making inferences from large quantities of data. \\nIt\\xa0is\\xa0the machine’s capacity to learn by itself how to do tasks better, rather than \\nsimply following instructions, that distinguishes AI from traditional computer \\nprogrammes. Contrary to popular myth, self-improvement does not prevent \\nAI\\xa0from being constrained by rules.\\nGovernments are among the largest adopters of AI, deploying it to assist in \\nmaking\\xa0decisions that can have major consequences for the lives of individual \\ncitizens. For example, governments are using AI to assist with decisions on \\nentitlement to immigration status, welfare benefits, school entry and priority \\n1 The European Commission’s High-Level Expert Group on Artificial Intelligence offers a fuller definition: \\n‘Artificial intelligence (AI) systems are software (and possibly also hardware) systems designed by humans \\nthat, given a complex goal, act in the physical or digital dimension by perceiving their environment through \\ndata acquisition, interpreting the collected structured or unstructured data, reasoning on the knowledge, or \\nprocessing the information, derived from this data and deciding the best action(s) to take to achieve the given \\ngoal. AI systems can either use symbolic rules or learn a numeric model, and they can also adapt their behaviour \\nby analysing how the environment is affected by their previous actions.’ Independent High-Level Expert \\nGroup on Artificial Intelligence (2019), Ethics Guidelines for Trustworthy AI, Brussels: European Commission, \\nhttps://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0].page_content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
